{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è½‰py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook rag_system_llama.ipynb to script\n",
      "[NbConvertApp] Writing 56052 bytes to rag_system_llama.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script rag_system_llama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zirong/miniforge3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Display and Image handling\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage  # ä½¿ç”¨ PILImage ä½œä¸º PIL.Image çš„åˆ«å\n",
    "from IPython.display import Image as IPyImage  # ä½¿ç”¨ IPyImage ä½œä¸º IPython çš„ Image\n",
    "\n",
    "# Vector DB\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# LLM\n",
    "import ollama\n",
    "\n",
    "# PDFå¤„ç†\n",
    "import PyPDF2\n",
    "\n",
    "# è®¾ç½®æ—¥å¿—\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# æ£€æŸ¥å¹¶åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "Path('chroma_db').mkdir(exist_ok=True)\n",
    "Path('image').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:55:20) \n",
      "[Clang 16.0.6 ]\n",
      "PyTorch version: 2.6.0\n",
      "Transformers version: 4.51.3\n",
      "Accelerate version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "# print(f\"Python version: {sys.version}\")\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"Transformers version: {transformers.__version__}\")\n",
    "# print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¾åœ¨æª”æ¡ˆæœ€ä¸Šæ–¹ (import ä¹‹å¾Œ)\n",
    "TYPE_MAP = {\n",
    "    \"acupoint\"    : [\"é‡ç¸\", \"acupuncture\"],\n",
    "    \"herb\"        : [\"herbology\", \"herbal\", \"æ–¹åŠ‘\"],\n",
    "    \"ccd\"         : [\"ccd\", \"èªçŸ¥\", \"cognition\"],\n",
    "    \"social\"      : [],                   # csv ç›´æ¥æŒ‡å®š\n",
    "    \"professional\": [],\n",
    "    \"image\":[]                                       # å…¶ä»–æœªåˆ†é¡\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voice to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper /Users/zirong/Desktop/test.mp4 --language Chinese --model tiny\n",
    "import whisper\n",
    "def transcribe_file(file_path, model_size=\"base\"):\n",
    "    model = whisper.load_model(model_size)\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# def main():\n",
    "#     audio_file = \"no_upload/test_mp3/01.mp3\"  # ä¿®æ”¹ç‚ºä½ çš„éŸ³æª”è·¯å¾‘\n",
    "#     transcription = transcribe_file(audio_file)\n",
    "#     print(\"Transcription:\", transcription)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åœ–ç‰‡è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union  # æ·»åŠ  Union å¯¼å…¥\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, image_dir: str = \"image\"):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def process_and_save(\n",
    "        self,\n",
    "        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |\n",
    "        target_size: Tuple[int, int],\n",
    "        prefix: str = \"resized_\",\n",
    "        quality: int = 95\n",
    "    ) -> Optional[Path]:\n",
    "        \"\"\"ç»Ÿä¸€çš„å›¾ç‰‡å¤„ç†æ–¹æ³•ï¼Œå¤„ç†å¹¶ä¿å­˜å›¾ç‰‡\"\"\"\n",
    "        try:\n",
    "            # ç¡®ä¿ image_path æ˜¯ Path å¯¹è±¡\n",
    "            image_path = Path(image_path)\n",
    "            if not str(image_path).startswith(str(self.image_dir)):\n",
    "                image_path = self.image_dir / image_path\n",
    "                \n",
    "            # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨\n",
    "            if not image_path.exists():\n",
    "                logger.error(f\"Image not found: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            # è¯»å–å¹¶å¤„ç†å›¾ç‰‡\n",
    "            image = PILImage.open(image_path)\n",
    "            \n",
    "            # è½¬æ¢ä¸º RGB æ¨¡å¼\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "                \n",
    "            # è®¡ç®—ç­‰æ¯”ä¾‹ç¼©æ”¾çš„å¤§å°\n",
    "            width, height = image.size\n",
    "            ratio = min(target_size[0]/width, target_size[1]/height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            \n",
    "            # ç¼©æ”¾å›¾ç‰‡\n",
    "            image = image.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            \n",
    "            # åˆ›å»ºæ–°çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡\n",
    "            new_image = PILImage.new('RGB', target_size, (255, 255, 255))\n",
    "            \n",
    "            # è®¡ç®—å±…ä¸­ä½ç½®\n",
    "            x = (target_size[0] - new_size[0]) // 2\n",
    "            y = (target_size[1] - new_size[1]) // 2\n",
    "            \n",
    "            # è´´ä¸Šç¼©æ”¾åçš„å›¾ç‰‡\n",
    "            new_image.paste(image, (x, y))\n",
    "            \n",
    "            # ç”Ÿæˆè¾“å‡ºè·¯å¾„\n",
    "            output_path = self.image_dir / f\"{image_path.name}\" #output_path = self.image_dir / f\"{prefix}{image_path.name}\"\n",
    "            # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡\n",
    "            new_image.save(output_path, quality=quality)\n",
    "            logger.info(f\"Saved processed image to: {output_path}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    def load_for_display(self, \n",
    "                        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |\n",
    "                        display_size: Tuple[int, int]) -> Optional[PILImage.Image]:\n",
    "        \"\"\"è½½å…¥å›¾ç‰‡ç”¨äºæ˜¾ç¤º\"\"\"\n",
    "        try:\n",
    "            processed_path = self.process_and_save(image_path, display_size, prefix=\"display_\")\n",
    "            if processed_path:\n",
    "                return PILImage.open(processed_path)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image for display {image_path}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding è™•ç†æ¨¡çµ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import sentencepiece as spm \n",
    "\n",
    "class EmbeddingProcessor:\n",
    "\n",
    "    MAX_TOKEN = 56          # 56 + BOS + EOS = 58 < 64\n",
    "    OVERLAP   = 16\n",
    "    DEFAULT_COLLECTION = \"ccd_docs_siglip\"\n",
    "\n",
    "    # åˆå§‹åŒ– embedding processor\n",
    "    def __init__(self, \n",
    "                persist_directory: str = \"chroma_db\",\n",
    "                image_dir: str = \"image\",\n",
    "                image_size: tuple = (224, 224),\n",
    "                collection_name:str = DEFAULT_COLLECTION,\n",
    "                reset: bool = False \n",
    "                ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–: å»ºç«‹clip_collection,ä½¿ç”¨CLIP(or OpenCLIP)åšembedding\n",
    "        \"\"\"\n",
    "        # ---------- è·¯å¾‘ & åŸºæœ¬è¨­å®š ----------\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_size = image_size\n",
    "        self.collection_name = collection_name\n",
    "        self.image_processor = ImageProcessor(image_dir)\n",
    "\n",
    "        # ---------- 1) å»ºç«‹ Chroma client ----------\n",
    "        logger.info(f\"Initializing Chroma with directory: {persist_directory}\")\n",
    "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "        # ---------- 2) reset (= åˆªæ‰èˆŠåº«) ----------\n",
    "        if reset:\n",
    "            try:\n",
    "                self.chroma_client.delete_collection(self.collection_name)\n",
    "                logger.info(f\"Deleted collection: {self.collection_name}\")\n",
    "            except (chromadb.errors.NotFoundError, ValueError):\n",
    "                logger.info(\"No old collection to delete\")\n",
    "\n",
    "        \n",
    "        # ---------- 3) åˆå§‹åŒ– SigLIP ----------\n",
    "        SIGLIP_NAME = \"google/siglip-base-patch16-224\"\n",
    "        self.processor = AutoProcessor.from_pretrained(SIGLIP_NAME)\n",
    "        self.siglip    = AutoModel.from_pretrained(SIGLIP_NAME)\n",
    "\n",
    "        # å–è¼¸å‡ºå‘é‡é•·åº¦ (base = 768)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros((1, 3, 224, 224))\n",
    "            self.clip_dim = self.siglip.get_image_features(dummy).shape[1]\n",
    "\n",
    "        # ---------- 4) å–å¾—æˆ–å»ºç«‹ collection ----------\n",
    "        self.clip_collection = self.chroma_client.get_or_create_collection(\n",
    "            name     = self.collection_name,\n",
    "            metadata = {\"dimension\": self.clip_dim}\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Using collection '{self.collection_name}' \"\n",
    "            f\"(dimension={self.clip_dim}, reset={reset})\"\n",
    "        )\n",
    "\n",
    "    def to_2d(self,x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().cpu().numpy()\n",
    "        elif isinstance(x, list):\n",
    "            x = np.asarray(x, dtype=np.float32)\n",
    "        if x.ndim == 1:        # (512,) â†’ (1,512)\n",
    "            x = x[None, :]\n",
    "        elif x.ndim != 2:\n",
    "            raise ValueError(f\"embedding ndim should be 1 or 2, got {x.shape}\")\n",
    "        return x.tolist()      # List[List[float]]\n",
    "\n",
    "    def chunk_text_by_token(\n",
    "            self,\n",
    "            text: str,\n",
    "            max_tokens: Optional[int] = None,\n",
    "            overlap: Optional[int] = None\n",
    "        ) -> List[str]:\n",
    "        CH_SENT_SPLIT = re.compile(r'([ã€‚ï¼ï¼Ÿï¼›\\n])')\n",
    "        \"\"\"å¥è™Ÿå„ªå…ˆæ–·å¥ï¼›ä»»ä½•å­å¥æœ€çµ‚éƒ½ â‰¤ 56 token\"\"\"\n",
    "        max_tokens = max_tokens or self.MAX_TOKEN      # 56\n",
    "        overlap    = overlap    or self.OVERLAP        # 16\n",
    "\n",
    "        # --- 1) ä»¥ä¸­æ–‡æ¨™é»åˆ‡æˆå­å¥ ---\n",
    "        sentences, buf, parts = [], \"\", CH_SENT_SPLIT.split(text)\n",
    "        for frag in parts:\n",
    "            if CH_SENT_SPLIT.match(frag):\n",
    "                buf += frag          # æŠŠæ¨™é»åŠ å›ä¾†\n",
    "                sentences.append(buf.strip())\n",
    "                buf = \"\"\n",
    "            else:\n",
    "                buf += frag\n",
    "        if buf: sentences.append(buf.strip())\n",
    "\n",
    "        # --- 2) ä»»ä½• >56 token çš„å¥å­å†æ»‘çª—åˆ‡ ---\n",
    "        chunks = []\n",
    "        for s in sentences:\n",
    "            ids = self.processor.tokenizer(s).input_ids\n",
    "            if len(ids) <= max_tokens:\n",
    "                chunks.append(s)\n",
    "            else:\n",
    "                step = max_tokens - overlap\n",
    "                for i in range(0, len(ids), step):\n",
    "                    seg_ids = ids[i:i + max_tokens]\n",
    "                    seg = self.processor.tokenizer.decode(seg_ids,\n",
    "                                                        skip_special_tokens=True)\n",
    "                    chunks.append(seg)\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def encode_text_to_vec(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        ç”¨ CLIP çš„ text encoder å°‡æ–‡å­—è½‰ç‚º512ç¶­å‘é‡\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunks = self.chunk_text_by_token(text)\n",
    "            if not chunks:\n",
    "                logger.error(\"No valid chunks generated for the text.\")\n",
    "                return None\n",
    "            all_vecs = []\n",
    "            for ch in chunks:\n",
    "                inp = self.processor(text=[ch], return_tensors=\"pt\").to(self.siglip.device)\n",
    "                with torch.no_grad():\n",
    "                    vec = self.siglip.get_text_features(**inp)\n",
    "                all_vecs.append(vec)\n",
    "            # é€™è£¡å¯å–å¹³å‡æˆ–ç›´æ¥å›å‚³å¤šæ¢å‘é‡\n",
    "            embs = torch.stack(all_vecs).mean(dim=0)\n",
    "            emb = embs / embs.norm(dim=-1, keepdim=True)\n",
    "            return emb.squeeze(0).cpu().tolist()   \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in encode_text_to_vec: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def add_qa_pairs(self,\n",
    "                questions: List[str],\n",
    "                answers: List[str],\n",
    "                question_metadatas: List[Dict],\n",
    "                answer_metadatas: List[Dict],\n",
    "                images: Optional[List[str]] = None):\n",
    "        \"\"\"æ·»åŠ é—®ç­”å¯¹åˆ°ä¸åŒçš„é›†åˆ\"\"\"\n",
    "        try:\n",
    "            # æ·»åŠ é—®é¢˜\n",
    "            if questions and question_metadatas:\n",
    "                logger.info(f\"Adding {len(questions)} questions\")\n",
    "                self.question_collection.add(\n",
    "                    documents=questions,\n",
    "                    metadatas=question_metadatas,\n",
    "                    ids=[f\"q_{i}\" for i in range(len(questions))]\n",
    "                )\n",
    "            \n",
    "            # æ·»åŠ å›ç­”\n",
    "            if answers and answer_metadatas:\n",
    "                logger.info(f\"Adding {len(answers)} answers\")\n",
    "                self.answer_collection.add(\n",
    "                    documents=answers,\n",
    "                    metadatas=answer_metadatas,\n",
    "                    ids=[f\"a_{i}\" for i in range(len(answers))]\n",
    "                )\n",
    "            \n",
    "            # å¤„ç†å›¾ç‰‡\n",
    "            if images:\n",
    "                logger.info(f\"Processing {len(images)} images\")\n",
    "                all_ids=[]\n",
    "                all_embeddings=[]\n",
    "                all_metadatas = []\n",
    "\n",
    "                \n",
    "                for i, (img_path,question_text) in enumerate(zip(images, questions)):\n",
    "                    img_emb = self.process_image(str(self.image_dir / img_path))\n",
    "                    txt_emb = self.encode_text_to_vec(question_text)\n",
    "\n",
    "                    if img_emb is not None:\n",
    "                        all_embeddings.append(img_emb.tolist())\n",
    "                        all_metadatas.append({\n",
    "                            \"type\": \"image\", \n",
    "                            \"path\": img_path,\n",
    "                            \"associated_question\": question_text\n",
    "                        })\n",
    "                        all_ids.append(f\"img_{i}\")\n",
    "                    if txt_emb is not None:\n",
    "                        all_embeddings.append(txt_emb.tolist())  \n",
    "                        all_metadatas.append({\n",
    "                            \"type\": \"clip_text\", \n",
    "                            \"text\": question_text,\n",
    "                            \"related_image\": img_path\n",
    "                        })\n",
    "                        all_ids.append(f\"txt_{i}\")\n",
    "\n",
    "                if len(all_embeddings)>0:\n",
    "                    logger.info(f\"Adding {len(all_embeddings)} total embeddings to collection\")\n",
    "                    self.image_collection.add(\n",
    "                        embeddings=all_embeddings,\n",
    "                        metadatas=all_metadatas,\n",
    "                        ids=all_ids\n",
    "                    )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding QA pairs: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def encode_image_to_vec(self, image_path: str) -> Optional[np.ndarray]:\n",
    "            \"\"\"\n",
    "            ç”¨ CLIP image encoder å°‡åœ–ç‰‡è½‰ç‚º512ç¶­å‘é‡\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # å…ˆåšåŸºç¤è™•ç†,ç¸®æ”¾æˆ–å¦å­˜\n",
    "                processed_path = self.image_processor.process_and_save(\n",
    "                    image_path, self.image_size\n",
    "                )\n",
    "                if not processed_path:\n",
    "                    return None\n",
    "\n",
    "                image = PILImage.open(processed_path)\n",
    "                inputs = self.processor(images=[image], return_tensors=\"pt\").to(self.siglip.device)\n",
    "                with torch.no_grad():\n",
    "                    embs = self.siglip.get_image_features(**inputs)\n",
    "                return (embs / embs.norm(dim=-1, keepdim=True)).cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in encode_image_to_vec: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    def add_vectors(\n",
    "        self,\n",
    "        texts: Optional[List[str]] = None,\n",
    "        metadatas: Optional[List[Dict]] = None,\n",
    "        images: Optional[List[str]] = None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        çµ±ä¸€æŠŠæ–‡å­— / åœ–ç‰‡å¯«é€² clip_collection\n",
    "        \"\"\"\n",
    "        texts     = texts or []\n",
    "        images    = images or []\n",
    "        metadatas = metadatas or []\n",
    "\n",
    "        all_embs, all_metas, docs, all_ids = [], [], [], []\n",
    "        idx = 0\n",
    "\n",
    "        # -------------------- æ–‡å­— --------------------\n",
    "        for i, txt in enumerate(texts):\n",
    "            emb = self.encode_text_to_vec(txt)\n",
    "            if emb is None:\n",
    "                continue\n",
    "\n",
    "            # â‘  å– metadata ä¸”ä¿è­‰æ˜¯ dict\n",
    "            src_meta = metadatas[i] if i < len(metadatas) else {}\n",
    "            if not isinstance(src_meta, dict):\n",
    "                src_meta = {\"note\": str(src_meta)}\n",
    "\n",
    "            # â‘¡ domain â†’ type æ˜ å°„ï¼ˆåªåšä¸€æ¬¡ï¼‰\n",
    "            domain = src_meta.pop(\"domain\", \"\").lower()\n",
    "            if domain in {\"é‡ç¸å­¸\", \"acupuncture\"}:\n",
    "                src_meta[\"type\"] = \"acupoint\"\n",
    "            elif domain in {\"ä¸­é†«æ–¹åŠ‘\", \"herb\"}:\n",
    "                src_meta[\"type\"] = \"herb\"\n",
    "            elif domain in {\"ccd\",\"canine\"}:\n",
    "                src_meta[\"type\"] = \"ccd\"\n",
    "\n",
    "            for vec in self.to_2d(emb):\n",
    "                md = {\n",
    "                    \"type\": src_meta.get(\"type\", \"professional\"),\n",
    "                    \"content\": txt,\n",
    "                    **src_meta,            # å…¶é¤˜æ¬„ä½ä¿ç•™\n",
    "                }\n",
    "                all_embs.append(vec)\n",
    "                all_metas.append(md)\n",
    "                docs.append(txt)\n",
    "                all_ids.append(f\"text_{idx}\")\n",
    "                idx += 1\n",
    "\n",
    "        # -------------------- åœ–ç‰‡ --------------------\n",
    "        for j, img_name in enumerate(images):\n",
    "            full_path = str(self.image_dir / img_name)\n",
    "            emb = self.encode_image_to_vec(full_path)\n",
    "            if emb is None:\n",
    "                continue\n",
    "\n",
    "            src_meta = metadatas[j] if j < len(metadatas) else {}\n",
    "            if not isinstance(src_meta, dict):\n",
    "                src_meta = {\"note\": str(src_meta)}\n",
    "\n",
    "            md = {\n",
    "                \"type\": \"image\",\n",
    "                \"path\": img_name,\n",
    "                **src_meta,\n",
    "            }\n",
    "            for vec in self.to_2d(emb):\n",
    "                all_embs.append(vec)\n",
    "                all_metas.append(md)\n",
    "                docs.append(\"\")          # å ä½\n",
    "                all_ids.append(f\"img_{idx}\")\n",
    "                idx += 1\n",
    "\n",
    "        # -------------------- å¯«å…¥ Chroma --------------------\n",
    "        if all_embs:\n",
    "            self.clip_collection.add(\n",
    "                embeddings = all_embs,\n",
    "                metadatas  = all_metas,\n",
    "                documents  = docs,\n",
    "                ids        = all_ids,\n",
    "            )\n",
    "            logger.info(f\"Added {len(all_embs)} items to '{self.collection_name}'\")\n",
    "\n",
    "\n",
    "    def similarity_search(self, query: str, k=25) -> Dict:\n",
    "        \"\"\"\n",
    "        å°queryåšCLIP text embeddingå¾Œ,åœ¨clip_collectionè£¡æ‰¾æœ€ç›¸ä¼¼çš„kç­†\n",
    "        \"\"\"\n",
    "        try:\n",
    "            emb = self.encode_text_to_vec(query)\n",
    "            if emb is None:\n",
    "                return {\"metadatas\":[],\"documents\":[],\"distances\":[]}\n",
    "        \n",
    "            results = self.clip_collection.query(\n",
    "                    query_embeddings=[emb],\n",
    "                    n_results=k,\n",
    "                    include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "            ) \n",
    "            # ---------- â–Œå‹•æ…‹é™æ¬Š + re-rank ----------------\n",
    "            q = query.lower()\n",
    "            if re.search(r\"(st|cv|gv|bl|pc)-\\d{1,2}|ç©´ä½\", q):\n",
    "                weight = {\"herb\": 0.3, \"ccd\": 0.3}     # acupoint = 1.0\n",
    "            elif any(w in q for w in [\"æŸ´èƒ¡\", \"é»ƒèŠ©\", \"æ¸…ç†±\"]):\n",
    "                weight = {\"acupoint\": 0.3, \"ccd\": 0.3}\n",
    "            elif any(w in q for w in [\"èªçŸ¥\", \"nlrp3\", \"ç™¼ç‚\"]):\n",
    "                weight = {\"herb\": 0.3, \"acupoint\": 0.3}\n",
    "            else:\n",
    "                weight = {}\n",
    "\n",
    "            metas = results[\"metadatas\"][0]\n",
    "            dists = results[\"distances\"][0]\n",
    "            docs  = results[\"documents\"][0]\n",
    "\n",
    "            scored = []\n",
    "            for i, (m, d) in enumerate(zip(metas, dists)):\n",
    "                w = weight.get(m.get(\"type\", \"\"), 1.0)\n",
    "                scored.append((d * w, i))          # è·é›¢æ„ˆå°æ„ˆå¥½\n",
    "            scored.sort(key=lambda x: x[0])\n",
    "\n",
    "            # idxs = [i for _, i in scored][:k]       # å–å‰ k\n",
    "            idxs = list(range(len(metas)))[:k]\n",
    "            \n",
    "            for key in [\"metadatas\", \"distances\", \"documents\"]:\n",
    "                results[key][0] = [results[key][0][i] for i in idxs]\n",
    "\n",
    "                return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error in search: {str(e)}\")\n",
    "            return {\"metadatas\":[],\"documents\":[],\"distances\":[]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è³‡æ–™è™•ç†æ¨¡çµ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, embedding_processor: 'EmbeddingProcessor'):\n",
    "        self.embedding_processor = embedding_processor\n",
    "        \n",
    "    def extract_social_posts(self, csv_path: str) -> Tuple[List[Dict], List[str]]:\n",
    "        \"\"\"å¤„ç† CSV å¹¶æå–é—®ç­”å¯¹å’Œå›¾ç‰‡\"\"\"\n",
    "        logger.info(f\"Processing CSV: {csv_path}\")\n",
    "        qa_pairs = []\n",
    "        images = []\n",
    "        \n",
    "        df = pd.read_excel(csv_path)\n",
    "        current_post = None\n",
    "        current_responses = []\n",
    "        current_images = []\n",
    "        current_link = None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # å¤„ç†æ–°çš„å¸–å­\n",
    "            if pd.notna(row['post']):\n",
    "                # ä¿å­˜å‰ä¸€ä¸ªé—®ç­”å¯¹\n",
    "                if current_post is not None:\n",
    "                    qa_pair = {\n",
    "                        'question': current_post,\n",
    "                        'answers': current_responses.copy(),\n",
    "                        'images': current_images.copy(),\n",
    "                        'metadata': {\n",
    "                            'type': 'social',\n",
    "                            'source': 'facebook',\n",
    "                            'images': ','.join(current_images) if current_images else '',\n",
    "                            'answer_count': len(current_responses),\n",
    "                            'link': current_link if current_link else ''\n",
    "                        }\n",
    "                    }\n",
    "                    if pd.notna(row.get('image_description')):\n",
    "                        qa_pair['metadata']['image_desc'] = row['image_description']\n",
    "\n",
    "                    qa_pairs.append(qa_pair)\n",
    "                    if current_images:\n",
    "                        images.extend(current_images)\n",
    "                \n",
    "                # åˆå§‹åŒ–æ–°çš„é—®ç­”å¯¹\n",
    "                current_post = row['post']\n",
    "                current_responses = []\n",
    "                current_images = []\n",
    "                current_link = row.get('link', '')\n",
    "            \n",
    "            # æ·»åŠ å›å¤\n",
    "            if pd.notna(row.get('responses')):\n",
    "                current_responses.append(row['responses'])\n",
    "            \n",
    "            # å¤„ç†å›¾ç‰‡\n",
    "            if pd.notna(row.get('images')):\n",
    "                img_path = row['images']\n",
    "                current_images.append(img_path)\n",
    "                logger.info(f\"Found image: {img_path} for current post\")\n",
    "  \n",
    "        \n",
    "        # ä¿å­˜æœ€åä¸€ä¸ªé—®ç­”å¯¹\n",
    "        if current_post is not None and len(current_responses) >= 3:\n",
    "            qa_pair = {\n",
    "                'question': current_post,\n",
    "                'answers': current_responses,\n",
    "                'images': current_images,\n",
    "                'metadata': {\n",
    "                    'type': 'social_qa',\n",
    "                    'source': 'facebook',\n",
    "                    'images': ','.join(current_images) if current_images else '',\n",
    "                    'answer_count': len(current_responses),\n",
    "                    'link': current_link if current_link else ''\n",
    "                }\n",
    "            }\n",
    "            if pd.notna(row.get('image_description')):\n",
    "                    qa_pair['metadata']['image_desc'] = row['image_description']\n",
    "\n",
    "            qa_pairs.append(qa_pair)\n",
    "            if current_images:\n",
    "                images.extend(current_images)\n",
    "        \n",
    "        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯\n",
    "        for i, qa in enumerate(qa_pairs):\n",
    "            logger.info(f\"\\nQA Pair {i+1}:\")\n",
    "            logger.info(f\"Question: {qa['question'][:100]}...\")\n",
    "            logger.info(f\"Number of answers: {len(qa['answers'])}\")\n",
    "            logger.info(f\"Images: {qa['images']}\")\n",
    "            logger.info(f\"Link: {qa.get('link', 'No link')}\")\n",
    "        \n",
    "        return qa_pairs, images\n",
    "\n",
    "    \n",
    "    def chunk_text(self,paragraph: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°‡çµ¦å®šæ®µè½ï¼Œä»¥ chunk_size å­—ç¬¦ç‚ºä¸Šé™é€²è¡Œåˆ‡åˆ†ï¼Œä¸¦ä¸”åœ¨ chunk ä¹‹é–“ä¿ç•™ overlap å€‹å­—çš„é‡ç–Šï¼Œ\n",
    "        ä»¥å…ä¸Šä¸‹æ–‡æ–·è£‚ã€‚\n",
    "        å‚™è¨»: \n",
    "        - é€™è£¡ä»¥ã€Œå­—ç¬¦ã€ç‚ºå–®ä½ï¼Œé©åˆä¸­æ–‡ï¼›è‹±æ–‡ä¹Ÿå¯ç”¨ï¼Œä½†è‹¥æƒ³ç²¾ç¢ºå°è‹±æ–‡ tokens å¯æ”¹æ›´å…ˆé€²æ–¹æ³•ã€‚\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        length = len(paragraph)\n",
    "\n",
    "        # å»æ‰å‰å¾Œå¤šé¤˜ç©ºç™½\n",
    "        paragraph = paragraph.strip()\n",
    "\n",
    "        while start < length:\n",
    "            end = start + chunk_size\n",
    "            # å– substring\n",
    "            chunk = paragraph[start:end]\n",
    "            chunks.append(chunk)\n",
    "            # ç§»å‹•æŒ‡æ¨™(ä¸‹ä¸€å€‹ chunk)\n",
    "            # overlap é é˜²æ–·å¥å¤±å»ä¸Šä¸‹æ–‡\n",
    "            start += (chunk_size - overlap)\n",
    "\n",
    "        return chunks\n",
    "  \n",
    "\n",
    "    def process_pdf(self, pdf_path: str,row_type: str) -> List[Dict]:\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "        professional_qa_pairs = []\n",
    "        pdf_name = Path(pdf_path).name  # è·å–æ–‡ä»¶å\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                is_formula = self.detect_domain(pdf_name) == \"ä¸­é†«æ–¹åŠ‘\"\n",
    "                is_acu = self.detect_domain(pdf_name) == \"é‡ç¸å­¸\"\n",
    "\n",
    "\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    text = page.extract_text()\n",
    "                    print(f\"Page {page_num+1} raw text:\", repr(text))\n",
    "                    if is_formula:\n",
    "                        paragraphs = self.split_formula_blocks(text)\n",
    "                    elif is_acu:\n",
    "                        paragraphs = self.split_acu_blocks(text)\n",
    "                    else:\n",
    "                        paragraphs = text.split('\\n\\n')\n",
    "                    \n",
    "                    \n",
    "                    # è™•ç†æ¯å€‹æ®µè½\n",
    "                    for para in paragraphs:\n",
    "                        # logger.info(f\"Para type: {type(para)}\")\n",
    "\n",
    "                        para_chunks = self.chunk_text(para)\n",
    "                        # logger.info(f\"Got {len(para_chunks)} chunks from chunk_text()\")\n",
    "\n",
    "                        for c in para_chunks:\n",
    "                            qa_pair = {\n",
    "                                'question': c[:50] + \"...\",  \n",
    "                                'answers': [c],\n",
    "                                'metadata': {\n",
    "                                    'type': row_type,\n",
    "                                    'source_file': pdf_name,  # æ·»åŠ æ–‡ä»¶å\n",
    "                                    'page': str(page_num + 1),\n",
    "                                    'content_length': str(len(c))\n",
    "                                } #'domain':self.detect_domain(pdf_name),\n",
    "                            }\n",
    "                            professional_qa_pairs.append(qa_pair)\n",
    "                \n",
    "                logger.info(f\"Extracted {len(professional_qa_pairs)} paragraphs from {pdf_name}\")\n",
    "                return professional_qa_pairs\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {pdf_name}: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def detect_domain(self, pdf_name: str) -> str:\n",
    "        lower = pdf_name.lower()\n",
    "\n",
    "        if \"é‡ç¸\" in pdf_name or \"acupuncture\" in lower:\n",
    "            return \"é‡ç¸å­¸\"\n",
    "        if \"herbal\" in lower or \"herbology\" in lower or \"æ–¹åŠ‘\" in pdf_name:\n",
    "            return \"ä¸­é†«æ–¹åŠ‘\"\n",
    "        return \"å…¶ä»–\"\n",
    "    \n",
    "    def split_formula_blocks(self,text: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        ç”¨æ­£å‰‡æŠ“å‡ºã€â— å…­å‘³åœ°é»ƒä¸¸ã€æˆ–ã€Liu Wei Di Huang Wanã€é–‹é ­ï¼Œ\n",
    "        æ¯é‡ä¸‹ä¸€å€‹æ–¹åå°±çµæŸä¸Šä¸€å¡Š\n",
    "        \"\"\"\n",
    "        pattern = re.compile(r\"(?:â—|\\s)([\\u4e00-\\u9fffA-Za-z\\- ]{3,40}(?:æ¹¯|ä¸¸|é£²|æ•£|è†))\")\n",
    "        blocks = []\n",
    "        cur_block = []\n",
    "        for line in text.splitlines():\n",
    "            if pattern.search(line):\n",
    "                # é‡åˆ°ä¸‹ä¸€å¸–è—¥ â†’ å…ˆæ”¶å‰ä¸€å¸–\n",
    "                if cur_block:\n",
    "                    blocks.append(\"\\n\".join(cur_block).strip())\n",
    "                    cur_block = []\n",
    "            cur_block.append(line)\n",
    "        if cur_block:\n",
    "            blocks.append(\"\\n\".join(cur_block).strip())\n",
    "        return [b for b in blocks if len(b) > 60]    \n",
    "\n",
    "    def split_acu_blocks(self,text: str) -> list[str]:\n",
    "        # ç¯„ä¾‹ä»£ç¢¼ï¼šLIâ€‘11ã€HT-7ã€SI 3\n",
    "        pattern = re.compile(r\"\\b([A-Z]{1,2}[ -â€‘]\\d{1,3})\\b\")\n",
    "        blocks, cur = [], []\n",
    "        for line in text.splitlines():\n",
    "            if pattern.search(line):\n",
    "                if cur: blocks.append(\"\\n\".join(cur).strip()); cur = []\n",
    "            cur.append(line)\n",
    "        if cur: blocks.append(\"\\n\".join(cur).strip())\n",
    "        return [b for b in blocks if len(b) > 40]\n",
    "\n",
    "    def process_all(self, csv_path: str, pdf_paths: List[str]):\n",
    "        \"\"\"ç¶œåˆè™•ç†ç¤¾ç¾¤ CSV + PDFs\"\"\"\n",
    "        try:\n",
    "            social_qa_pairs, images = [], []  \n",
    "            # 1. å¤„ç†ç¤¾ç¾¤æ•°æ®\n",
    "            if csv_path: \n",
    "                social_qa_pairs, images = self.extract_social_posts(csv_path)\n",
    "                logger.info(f\"\\nProcessed social data:\")\n",
    "                logger.info(f\"- Social QA pairs: {len(social_qa_pairs)}\")\n",
    "                logger.info(f\"- Images found: {len(images)}\")\n",
    "            else:\n",
    "                logger.info(\"Skip social CSV, onlyè™•ç† PDFs\")\n",
    "            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶\n",
    "            valid_images = []\n",
    "            for img in images:\n",
    "                img_path = Path(self.embedding_processor.image_dir) / img\n",
    "                if img_path.exists():\n",
    "                    valid_images.append(img)\n",
    "            \n",
    "            # 2. å¤„ç†æ‰€æœ‰ PDF\n",
    "            all_professional_pairs = []\n",
    "            for pdf_path in pdf_paths:\n",
    "                pdf_name = pdf_path.name.lower()\n",
    "                for t, keys in TYPE_MAP.items():\n",
    "                    if any(k in pdf_name for k in keys):\n",
    "                        row_type = t; break\n",
    "                else:\n",
    "                    row_type = \"professional\"\n",
    "                pdf_qa_pairs = self.process_pdf(pdf_path, row_type=row_type)\n",
    "                #pdf_qa_pairs = self.process_pdf(pdf_path)\n",
    "                all_professional_pairs.extend(pdf_qa_pairs)\n",
    "                logger.info(f\"\\nProcessed {Path(pdf_path).name}:\")\n",
    "                logger.info(f\"- Extracted paragraphs: {len(pdf_qa_pairs)}\")\n",
    "            \n",
    "            # 3. åˆå¹¶ => all_qa_pairs\n",
    "            all_qa_pairs = social_qa_pairs + all_professional_pairs\n",
    "            \n",
    "            # 4. æº–å‚™ texts + metadatas => ä½ å°±èƒ½ä¸€æ¬¡æˆ–å¤šæ¬¡å‘¼å« add_vectors\n",
    "            questions = []\n",
    "            answers = []\n",
    "            question_metas = []\n",
    "            answer_metas = []\n",
    "            \n",
    "            # å¤„ç†æ‰€æœ‰é—®ç­”å¯¹\n",
    "            for qa_pair in all_qa_pairs:\n",
    "                # question \n",
    "                questions.append(qa_pair['question'])\n",
    "                question_metas.append(qa_pair['metadata'])\n",
    "                \n",
    "                # answers\n",
    "                for ans_text in qa_pair['answers']:\n",
    "                    answers.append(ans_text)\n",
    "                    am = qa_pair['metadata'].copy()\n",
    "                    am['parent_question'] = qa_pair['question']\n",
    "                    answer_metas.append(am)\n",
    "\n",
    "            # ------------- é€™è£¡æ‰é–‹å§‹çµ„ professional texts / metas -------------\n",
    "            prof_texts = [qa[\"answers\"][0] for qa in all_professional_pairs]\n",
    "            prof_metas = [qa[\"metadata\"]   for qa in all_professional_pairs]\n",
    "\n",
    "            \n",
    "            # è¾“å‡ºå¤„ç†ç»“æœ\n",
    "            logger.info(f\"\\nFinal processing summary:\")\n",
    "            logger.info(f\"- Total questions: {len(questions)}\")\n",
    "            logger.info(f\"- Total answers: {len(answers)}\")\n",
    "            logger.info(f\"- Valid images: {len(valid_images)}\")\n",
    "            logger.info(f\"- Social content: {len(social_qa_pairs)} QA pairs\")\n",
    "            logger.info(f\"- Professional content: {len(all_professional_pairs)} paragraphs\")\n",
    "            \n",
    "\n",
    "\n",
    "            # --------- ğŸ”§ æŠŠ 3 çµ„ metadata éƒ½ä¿è­‰æ˜¯ dict (æ”¾åœ¨æ­¤è™•) ---------\n",
    "            question_metas = [m if isinstance(m, dict) else {\"note\": str(m)}\n",
    "                            for m in question_metas]\n",
    "            prof_metas     = [m if isinstance(m, dict) else {\"note\": str(m)}\n",
    "                            for m in prof_metas]\n",
    "            # è‹¥è¦ç”¨ answer_metas ä¹Ÿä¸€ä½µè™•ç†\n",
    "            answer_metas   = [m if isinstance(m, dict) else {\"note\": str(m)}\n",
    "                            for m in answer_metas]\n",
    "\n",
    "            # 5. å…¨éƒ¨å¯«é€²clip_collection\n",
    "            \n",
    "            \n",
    "            # (C) professional paragraphs\n",
    "            # prof_texts  = [qa[\"answers\"][0] for qa in all_professional_pairs]\n",
    "            # prof_metas  = [qa[\"metadata\"]   for qa in all_professional_pairs]\n",
    "\n",
    "            self.embedding_processor.add_vectors(texts=prof_texts,\n",
    "                                            metadatas=prof_metas)\n",
    "            \n",
    "            # (A) å…ˆåŠ æ‰€æœ‰ question\n",
    "            self.embedding_processor.add_vectors(\n",
    "                texts = questions,\n",
    "                metadatas = question_metas\n",
    "            )\n",
    "\n",
    "            # (B) å†åŠ æ‰€æœ‰ answers\n",
    "            # self.embedding_processor.add_vectors(\n",
    "            #     texts = answers,\n",
    "            #     metadatas = answer_metas\n",
    "            # )\n",
    "            \n",
    "            # (D) å†åŠ  images\n",
    "            # æ²’æœ‰å°æ‡‰metadataï¼Ÿå¯ä»¥ç°¡å–®åš\n",
    "            # [{\"type\":\"image\",\"source\":\"facebook\"} ...] æˆ–\n",
    "            # æƒ³çŸ¥é“å®ƒå±¬æ–¼å“ªå€‹QApair? å°±è¦è‡ªå·±å°æ‡‰\n",
    "            if valid_images:\n",
    "                meta_for_imgs = []\n",
    "                for img_name in valid_images:\n",
    "                    meta_for_imgs.append({\n",
    "                        \"type\":\"image\",\n",
    "                        \"source\":\"facebook\",\n",
    "                        \"filename\": img_name\n",
    "                    })\n",
    "\n",
    "                self.embedding_processor.add_vectors(\n",
    "                    images=valid_images,\n",
    "                    metadatas=meta_for_imgs\n",
    "                )\n",
    "\n",
    "            logger.info(\"All data added to clip_collection.\")\n",
    "            return len(questions), len(valid_images)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing documents: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAç³»çµ±æ¨¡çµ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "class QASystem:\n",
    "    def __init__(self, embedding_processor: 'EmbeddingProcessor',\n",
    "                 model_name: str = 'llama3.2-vision'):\n",
    "        self.embedding_processor = embedding_processor\n",
    "        self.model_name = model_name\n",
    "        logger.info(f\"Initialized QA System with Ollama model: {model_name}\")\n",
    "\n",
    "    def _classify_collection_results(self, raw_result) -> Dict:\n",
    "        \"\"\"\n",
    "        å°‡ clip_collection çš„æª¢ç´¢çµæœ (metadatas/documents...) \n",
    "        è½‰æ›æˆ { 'social': {...}, 'professional': {...}, 'images': {...} } \n",
    "        çš„çµæ§‹ï¼Œä¾¿æ–¼å¾ŒçºŒ gather_references / format_contextã€‚\n",
    "        \"\"\"\n",
    "        # é è¨­ç©ºçµæ§‹\n",
    "        structured = {\n",
    "            \"social\": {\n",
    "                \"metadata\": [],\n",
    "                \"link\": [],\n",
    "                \"content\": [],\n",
    "                \"documents\":[]\n",
    "            },\n",
    "            \"professional\": {\n",
    "                \"metadata\": [],\n",
    "                \"content\": [],\n",
    "                \"documents\":[]\n",
    "                # ...\n",
    "            },\n",
    "            \"images\": {\n",
    "                \"metadata\": [],\n",
    "                \"paths\": [],\n",
    "                \"relevance\":[]\n",
    "            },\n",
    "            \n",
    "        }\n",
    "\n",
    "        # raw_result[\"metadatas\"] æ˜¯å€‹ 2D list => [ [meta0, meta1, ...] ]\n",
    "        if raw_result.get(\"metadatas\"):\n",
    "            meta_list = raw_result[\"metadatas\"][0]  # å› ç‚ºåªæœ‰1å€‹ query\n",
    "            dist_list = raw_result[\"distances\"][0] if raw_result.get(\"distances\") else []\n",
    "            doc_list = raw_result[\"documents\"][0]\n",
    "            ids_list  = raw_result[\"ids\"][0] if raw_result.get(\"ids\") else []\n",
    "            # documents_list = raw_result[\"documents\"][0]\n",
    "\n",
    "            for i, meta in enumerate(meta_list):\n",
    "                dist = dist_list[i] if i < len(dist_list) else None\n",
    "                doc_id = ids_list[i] if i < len(ids_list) else \"\"\n",
    "                doc_text = doc_list[i] if i < len(doc_list) else \"\"\n",
    "\n",
    "                # åˆ¤æ–· metadata æ˜¯å±¬æ–¼å“ªå€‹ä¾†æº\n",
    "                # ä¾‹å¦‚ meta.get(\"type\") == \"social_qa\" => æ”¾åˆ° social\n",
    "                #     meta.get(\"type\") == \"professional\" => æ”¾åˆ° professional\n",
    "                #     meta.get(\"type\") == \"image\" => æ”¾åˆ° images\n",
    "                src_type = meta.get(\"type\",\"\")\n",
    "\n",
    "                if src_type == \"social\":\n",
    "                    structured[\"social\"][\"metadata\"].append(meta)\n",
    "                    structured[\"social\"][\"documents\"].append(doc_text)\n",
    "                elif src_type in (\"acupoint\", \"herb\", \"ccd\", \"professional\"):\n",
    "                    structured[\"professional\"][\"metadata\"].append(meta)\n",
    "                    structured[\"social\"][\"documents\"].append(doc_text)\n",
    "                \n",
    "                elif src_type == \"image\":\n",
    "                    structured[\"images\"][\"metadata\"].append(meta)\n",
    "                    # æ”¾ path\n",
    "                    path = meta.get(\"path\",\"\")\n",
    "                    structured[\"images\"][\"paths\"].append(path)\n",
    "                    structured[\"images\"][\"relevance\"].append(dist)\n",
    "\n",
    "                else:\n",
    "                    # å°‡æœªçŸ¥ type å…¨ä¸Ÿ professionalï¼Œæˆ–ä¾éœ€æ±‚æ”¹ social\n",
    "                    meta.setdefault(\"type\", \"professional\")\n",
    "                    structured[\"professional\"][\"metadata\"].append(meta)\n",
    "                    structured[\"professional\"][\"documents\"].append(doc_text)\n",
    "\n",
    "\n",
    "        return structured\n",
    "\n",
    "\n",
    "    def determine_question_type(self,query: str) -> str:\n",
    "        \"\"\"\n",
    "        å›å‚³: \"multiple_choice\" | \"true_false\" | \"qa\"\n",
    "        æ”¯æ´ä¸­è‹±æ–‡ & å„ç¨®æ¨™é»\n",
    "        \"\"\"\n",
    "        q = query.strip().lower()\n",
    "\n",
    "        # --- Multipleâ€‘choice --------------------------------------------------\n",
    "        # 1) è¡Œé¦–æˆ–æ›è¡Œå¾Œå‡ºç¾  Aï½D / å…¨å½¢ï¼¡ï½ï¼¤ / ã€Œç­”ã€ï¼Œ\n",
    "        #    å¾Œé¢æ¥ã€€. ï¼ : ï¼š ã€)\n",
    "        mc_pattern = re.compile(r'(?:^|\\n)\\s*(?:[a-dï½-ï½„]|ç­”)[:\\.ï¼:ï¼šã€\\)]', re.I)\n",
    "        # 2) or å¥å­å¸¶ \"which of the following\"\n",
    "        mc_keywords_en = [\"which of the following\", \"which one of the following\",\n",
    "                        \"which option\", \"choose one of\"]\n",
    "\n",
    "        if mc_pattern.search(query) or any(kw in q for kw in mc_keywords_en):\n",
    "            return \"multiple_choice\"\n",
    "\n",
    "        # --- True / False -----------------------------------------------------\n",
    "        tf_keywords_zh = [\"æ˜¯å¦\", \"æ˜¯å—\", \"å°å—\", \"æ­£ç¢ºå—\"]\n",
    "        tf_keywords_en = ['true or false', 'is it', 'is this', 'is that', \n",
    "             'is it possible', 'correct or not']\n",
    "\n",
    "        if any(k in q for k in tf_keywords_zh + tf_keywords_en):\n",
    "            return \"true_false\"\n",
    "\n",
    "        # --- Default ----------------------------------------------------------\n",
    "        return \"qa\"\n",
    "\n",
    "    def gather_references(self, search_results: Dict) -> str:\n",
    "        \"\"\"\n",
    "        å¾ search_results ä¸­æ“·å– PDF æª”å/ç¤¾ç¾¤é€£çµï¼Œä¸¦çµ„æˆä¸€å€‹å­—ä¸²\n",
    "        \"\"\"\n",
    "        if not isinstance(search_results, dict):\n",
    "            logger.error(\"search_results æ ¼å¼éŒ¯èª¤: %s\", type(search_results))\n",
    "            return \"\"\n",
    "\n",
    "        references = []\n",
    "\n",
    "        # è™•ç† social\n",
    "        for meta in search_results[\"social\"].get(\"metadata\", []):\n",
    "            if meta.get(\"type\") == \"social_qa\" and \"link\" in meta:\n",
    "                references.append(f\"(ç¶“é©—) {meta['link']}\")\n",
    "\n",
    "        # è™•ç† professional\n",
    "        for meta in search_results[\"professional\"].get(\"metadata\", []):\n",
    "            if meta.get(\"type\") in [\"pdf\", \"professional\"]:\n",
    "                pdf_name = meta.get(\"source_file\", \"unknown.pdf\")\n",
    "                references.append(f\"(æ–‡ç») {pdf_name}\")\n",
    "\n",
    "        # å»é‡\n",
    "        unique_refs = list(set(references))\n",
    "        return \"\\n\".join(unique_refs)\n",
    "\n",
    "\n",
    "    def build_user_prompt(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: str,\n",
    "        references_str: str = \"\"\n",
    "        ) -> str:\n",
    "        # ä¸å«ä»»ä½•æ ¼å¼è¦ç¯„ï¼åªçµ¦é¡Œç›®èˆ‡è³‡æ–™\n",
    "        return (\n",
    "            f\"{query}\\n\\n\"\n",
    "            \"åƒè€ƒè³‡æ–™ï¼š\\n\" + context +\n",
    "            \"\\nä¾†æºï¼š\\n\" + references_str\n",
    "        )\n",
    "\n",
    "       \n",
    "    def translate_en_to_zh(self,chinese_text: str) -> str:\n",
    "        try:\n",
    "            # æŒ‡å®šåŸæ–‡èªè¨€ç‚º 'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œç›®æ¨™èªè¨€ç‚º 'en'ï¼ˆè‹±æ–‡ï¼‰\n",
    "            translator = GoogleTranslator(source='en', target='zh-TW')\n",
    "            result = translator.translate(chinese_text)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"ç¿»è­¯éŒ¯èª¤ï¼š{e} - å°æ‡‰ä¸­æ–‡å•é¡Œï¼š{chinese_text}\")\n",
    "            return chinese_text  # è‹¥ç¿»è­¯å¤±æ•—ï¼Œè¿”å›åŸæ–‡\n",
    "\n",
    "\n",
    "    def merge_adjacent(self, metas, docs, k_keep: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        å°‡åŒæª”åŒé ä¸” _id é€£è™Ÿçš„ç‰‡æ®µåˆä½µï¼Œå›å‚³å‰ k_keep æ®µæ–‡å­—ã€‚\n",
    "        åƒæ•¸\n",
    "        ----\n",
    "        metas : list[dict]    # raw_result[\"metadatas\"][0]\n",
    "        docs  : list[str]     # raw_result[\"documents\"][0]\n",
    "        \"\"\"\n",
    "        ID_NUM_RE = re.compile(r\"_(\\d+)$\")   # å°¾ç¢¼å–æ•¸å­—ï¼štext_123 â†’ 123\n",
    "        merged, buf = [], \"\"\n",
    "        last_src, last_idx = (\"\", \"\"), -999\n",
    "\n",
    "        for md, doc in zip(metas, docs):\n",
    "            src_key = (md.get(\"source_file\", \"\"), md.get(\"page\", \"\"))\n",
    "\n",
    "            # å– _id å°¾ç¢¼ï¼›è‹¥ä¸å­˜åœ¨å‰‡è¨­ -1\n",
    "            _id = md.get(\"_id\", \"\")\n",
    "            m = ID_NUM_RE.search(_id)\n",
    "            cur_idx = int(m.group(1)) if m else -1\n",
    "\n",
    "            # åŒæª”åŒé ä¸”é€£è™Ÿ â†’ è¦–ç‚ºç›¸é„°\n",
    "            if src_key == last_src and cur_idx == last_idx + 1:\n",
    "                buf += doc\n",
    "            else:\n",
    "                if buf:\n",
    "                    merged.append(buf)\n",
    "                buf = doc\n",
    "            last_src, last_idx = src_key, cur_idx\n",
    "\n",
    "        if buf:\n",
    "            merged.append(buf)\n",
    "\n",
    "        return \"\\n\\n\".join(merged[:k_keep])\n",
    "\n",
    "\n",
    "    def generate_response(self, query: str,question_type: Optional[str] = None) -> Tuple[str, List[str]]:\n",
    "        try:\n",
    "            TARGET_DOMAIN = \"\" \n",
    "            raw_result = self.embedding_processor.similarity_search(\n",
    "                query,\n",
    "                k=25,\n",
    "                domain=TARGET_DOMAIN)  \n",
    "            print(raw_result[\"metadatas\"])\n",
    "            # â†“â†“â†“ ç›´æ¥åŠ é€™ 2 è¡Œï¼ˆåªç•™åœ¨é™¤éŒ¯æœŸé–“ï¼‰\n",
    "            hit_types = [m[\"type\"] for m in raw_result[\"metadatas\"][0][:10]]\n",
    "            print(\"[DEBUG] top-10 types:\", hit_types)\n",
    "            # â†‘â†‘â†‘\n",
    "\n",
    "            # â”€â”€ å‹•æ…‹ä¾ type èª¿åˆ†ï¼Œè£œå›å€™é¸åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            # metas = raw_result[\"metadatas\"][0]\n",
    "            # docs  = raw_result[\"documents\"][0]\n",
    "            # sims  = raw_result[\"distances\"][0]\n",
    "\n",
    "            # # ä¾é¡Œç›®é—œéµå­—æ±ºå®š penalty\n",
    "            # q = query.lower()\n",
    "            # if re.search(r\"(st-|pc-|cv-|gv-|bl-)\\d{1,2}|ç©´ä½\", q):\n",
    "            #     penalty = {\"herb\": 0.6, \"ccd\": 0.7}\n",
    "            # elif any(w in q for w in [\"æŸ´èƒ¡\",\"é»ƒèŠ©\",\"æ¸…ç†±\",\"è§£æ¯’\"]):\n",
    "            #     penalty = {\"acupoint\": 0.7, \"ccd\": 0.7}\n",
    "            # elif any(w in q for w in [\"èªçŸ¥\",\"å¤±æ™º\",\"nlrp3\",\"ç™¼ç‚\"]):\n",
    "            #     penalty = {\"herb\": 0.6, \"acupoint\": 0.7}\n",
    "            # else:\n",
    "            #     penalty = {}\n",
    "\n",
    "            # scored = [\n",
    "            #     (score * penalty.get(md[\"type\"], 1.0), md, doc)\n",
    "            #     for score, md, doc in zip(sims, metas, docs)\n",
    "            # ]\n",
    "            # # é‡æ–°æ’åºä¸¦å–å‰ k_keep=8\n",
    "            # scored.sort(key=lambda x: x[0], reverse=True)\n",
    "            # metas, docs = zip(*[(m, d) for _, m, d in scored[:8]])\n",
    "\n",
    "            # # æŠŠè™•ç†å¾Œçš„ metas / docs å›å‚³çµ¦ä¸‹æ¸¸\n",
    "            # raw_result[\"metadatas\"][0] = list(metas)\n",
    "            # raw_result[\"documents\"][0] = list(docs)\n",
    "            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "            \n",
    "            # â† æ–°å¢ä¿éšª\n",
    "            if not raw_result[\"documents\"] or len(raw_result[\"documents\"][0]) == 0:\n",
    "                logger.warning(\"No hits for query â†’ æ”¹ç”¨ k=50 å†è©¦ä¸€æ¬¡\")\n",
    "                raw_result = self.embedding_processor.similarity_search(query, k=50)\n",
    "\n",
    "            if not raw_result[\"documents\"] or len(raw_result[\"documents\"][0]) == 0:\n",
    "                # é‚„æ˜¯ç©ºï¼Œç›´æ¥å›è¦† [NoRef]\n",
    "                return \"[NoRef] ç„¡è¶³å¤ è­‰æ“šåˆ¤æ–·\", []\n",
    "            \n",
    "            # ç”¨å¾Œè™•ç†\n",
    "            search_results = self._classify_collection_results(raw_result)\n",
    "            logger.info(\"SEARCH RESULT(structured): %s\",search_results)\n",
    "\n",
    "\n",
    "            metas = raw_result[\"metadatas\"][0]\n",
    "            docs  = raw_result[\"documents\"][0]\n",
    "\n",
    "            # context = self.format_context(search_results)\n",
    "            # raw_ctx = self.merge_adjacent(metas, docs, k_keep=5)[:1500]\n",
    "            context = self.merge_adjacent(raw_result[\"metadatas\"][0],\n",
    "                              raw_result[\"documents\"][0])[:1500]\n",
    "\n",
    "            context = context[:1500]          # æœ€å¤š 1500 å­—\n",
    "\n",
    "            references_str = self.gather_references(search_results)\n",
    "            # linkæ‡‰è©²ç”¨å‚³åƒæ•¸çš„æœƒæˆåŠŸ å¯èƒ½ç”¨context.linkä¹‹é¡çš„æŠ“é¡Œç›®çš„reference\n",
    "            \n",
    "            zh_query = self.translate_en_to_zh(query)\n",
    "            \n",
    "\n",
    "            # --- â‘  é¡Œå‹ --------------------------------------------------------\n",
    "            q_type = question_type or self.determine_question_type(query)\n",
    "\n",
    "            user_prompt = self.build_user_prompt(\n",
    "                query=query,\n",
    "                context=context[:1500],\n",
    "                references_str=references_str\n",
    "            )\n",
    "            # ---------- â‘¡ æ ¹æ“šé¡Œå‹å‹•æ…‹çµ„ system æŒ‡ä»¤ ----------\n",
    "            if q_type == \"multiple_choice\":\n",
    "                format_rules = (\n",
    "                    \"é€™æ˜¯ä¸€é¡Œé¸æ“‡é¡Œï¼Œå›ç­”æ ¼å¼å¦‚ä¸‹ï¼š\\n\"\n",
    "                    \"å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\\n\"\n",
    "                    \"ç”¨ 2-3 å¥è©±èªªæ˜ç†ç”±ã€‚\\n\"\n",
    "                    \"è«‹åœ¨ç­”æ¡ˆæœ€å¾Œé¡¯ç¤ºä½ åƒè€ƒçš„ä¾†æºé€£çµæˆ–è«–æ–‡åç¨±ï¼Œå¦‚æœä¾†æºä¸­åŒ…å«ã€Œ(ç¶“é©—) some_linkã€ï¼Œè«‹åœ¨å›ç­”ä¸­ä»¥ [Experience: some_link] å½¢å¼æ¨™ç¤ºï¼›è‹¥åŒ…å«ã€Œ(æ–‡ç») some.pdfã€ï¼Œå°± [reference: some.pdf]\\n\"\n",
    "                    \"å¦‚æª¢ç´¢çµæœä»ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹ä»¥[NoRef]æ¨™ç¤ºä¸¦æ ¹æ“šä½ çš„å¸¸è­˜å›ç­”ã€‚\"\n",
    "                    \"æœ€å¾Œå†å›ç­”ç­”æ¡ˆï¼Œåªèƒ½å›ç­” A/B/C/D (è«‹å‹¿å¸¶ä»»ä½•æ¨™é»ã€æ–‡å­—ã€ä¹Ÿä¸è¦åªå›ç­”é¸é …)\\n\"\n",
    "                    \"è‹¥åŒæ™‚å‡ºç¾å¤šå€‹é¸é …ï¼Œè«‹åªé¸ä¸€å€‹æœ€é©åˆçš„\\n\"\n",
    "                    \"å•é¡Œå¦‚ä¸‹ï¼š\\n\"\n",
    "                )\n",
    "            elif q_type == \"true_false\":\n",
    "                format_rules = (\n",
    "                    \"é€™æ˜¯ä¸€é¡Œæ˜¯éé¡Œï¼Œè«‹æŒ‰ç…§ä¸‹åˆ—æ ¼å¼å›ç­”ï¼š\\n\"\n",
    "                    \"å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\\n\"\n",
    "                    \"è«‹åœ¨ç­”æ¡ˆæœ€å¾Œé¡¯ç¤ºä½ åƒè€ƒçš„ä¾†æºé€£çµæˆ–è«–æ–‡åç¨±ï¼Œå¦‚æœä¾†æºä¸­åŒ…å«ã€Œ(ç¶“é©—) some_linkã€ï¼Œè«‹åœ¨å›ç­”ä¸­ä»¥ [Experience: some_link] å½¢å¼æ¨™ç¤ºï¼›è‹¥åŒ…å«ã€Œ(æ–‡ç») some.pdfã€ï¼Œå°± [reference: some.pdf]\\n\"\n",
    "                    \"å¦‚æª¢ç´¢çµæœä»ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹ä»¥[NoRef]æ¨™ç¤ºä¸¦æ ¹æ“šä½ çš„å¸¸è­˜å›ç­”ã€‚\\n\"\n",
    "                    \"æœ€å¾Œå†çµ¦å‡ºçµè«–ï¼Œåªèƒ½å¯«ã€ŒTrueã€æˆ–ã€ŒFalseã€\\n\"\n",
    "                    \"å•é¡Œå¦‚ä¸‹ï¼š\\n\"\n",
    "                )\n",
    "            else:   # qa\n",
    "                format_rules = (\n",
    "                    \"è«‹ä¾ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\\n\"\n",
    "                    \"é‡å°å•é¡Œæä¾›å…·é«”ç­”æ¡ˆ \\n\"\n",
    "                    \"è«‹åœ¨ç­”æ¡ˆæœ€å¾Œé¡¯ç¤ºä½ åƒè€ƒçš„ä¾†æºé€£çµæˆ–è«–æ–‡åç¨±ï¼Œå¦‚æœä¾†æºä¸­åŒ…å«ã€Œ(ç¶“é©—) some_linkã€ï¼Œè«‹åœ¨å›ç­”ä¸­ä»¥ [Experience: some_link] å½¢å¼æ¨™ç¤ºï¼›è‹¥åŒ…å«ã€Œ(æ–‡ç») some.pdfã€ï¼Œå°± [reference: some.pdf]\\n\"\n",
    "                    \"è‹¥é‡åˆ°ç„¡æ³•ç¢ºå®šæˆ–è­‰æ“šä¸è¶³çš„æƒ…æ³å¯ä»¥è£œå……èªªæ˜ç ”ç©¶ä¸è¶³ï¼Œè«‹ä»¥[NoRef]æ¨™ç¤ºä¸¦æ ¹æ“šä½ çš„å¸¸è­˜å›ç­”ã€‚\\n\"\n",
    "                    \"å•é¡Œå¦‚ä¸‹ï¼š\\n\"\n",
    "                )\n",
    "\n",
    "            system_prompt = (\n",
    "                \"æ‚¨æ˜¯ä¸€åå°ˆæ¥­ç¸é†«ï¼Œ1.æ“…é•·çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ç¶œåˆç—‡ï¼ˆCCDï¼‰çš„è¨ºæ–·å’Œè­·ç† 2.æ“æœ‰è±å¯Œçš„å¯µç‰©ä¸­é†«çŸ¥è­˜ 3.å¸¸è¦‹å•é¡Œè¨ºæ–·åŠæ”¹å–„å»ºè­°\\n\"\n",
    "                + format_rules\n",
    "            )\n",
    "        \n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt}\n",
    "            ]\n",
    "            print(\"=======sys prompt =======\",system_prompt)\n",
    "            print(\"=======user prompt =======\",user_prompt)\n",
    "            # è™•ç†åœ–ç‰‡\n",
    "            image_paths = []\n",
    "            # 2) å¾ social metadata æŠŠåœ–ç‰‡æ’ˆå‡º\n",
    "            for md in search_results[\"social\"][\"metadata\"]:\n",
    "                if md.get(\"images\"):  # e.g. \"image12.jpg,image02.jpg\"\n",
    "                    for img_name in md[\"images\"].split(\",\"):\n",
    "                        img_name = img_name.strip()\n",
    "                        if img_name:\n",
    "                            full_path = self.embedding_processor.image_dir / img_name\n",
    "                            if full_path.exists():\n",
    "                                image_paths.append(str(full_path.resolve()))\n",
    "            # 3) OLlama åªå…è¨±ä¸€å¼µ, ä½ å¯å– image_paths[:1] => message[\"images\"] = ...\n",
    "            if image_paths:\n",
    "                print(\"We found images: \", image_paths)\n",
    "                # ä½ å¯ä»¥å…ˆéš¨ä¾¿å–ä¸€å¼µ\n",
    "                # or å…¨éƒ¨ inject to prompt\n",
    "            else:\n",
    "                logger.info(\"No images to display\")\n",
    "\n",
    "            # ç”Ÿæˆå“åº”\n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=message\n",
    "            )\n",
    "            return response['message']['content'], image_paths\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\")\n",
    "            # return f\"å‡ºç¾å•é¡Œï¼Œæª¢æŸ¥ollamaé€£ç·šæˆ–æ˜¯generate_response\", []\n",
    "            raise\n",
    "\n",
    "    def format_context(self, search_results: Dict) -> str:\n",
    "        \"\"\"Format context from search results\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "\n",
    "            # 1) è™•ç†ç¤¾ç¾¤è¨è«–\n",
    "            social_metas = search_results[\"social\"].get(\"metadata\", [])\n",
    "            social_links = search_results[\"social\"].get(\"link\", [])\n",
    "            social_docs = search_results[\"social\"].get(\"documents\", [])\n",
    "            social_content = search_results[\"social\"].get(\"content\", [])\n",
    "\n",
    "            if social_metas or social_links or social_docs:\n",
    "                context += \"\\n[ç¤¾ç¾¤è¨è«–]\\n\"\n",
    "                # é€™è£¡ç¤ºç¯„æŠŠ linkã€documents éƒ½è¼¸å‡º\n",
    "                for i, meta in enumerate(social_metas):\n",
    "                    link_str = meta.get(\"link\", \"\")\n",
    "                    doc_text = social_docs[i] if i < len(social_docs) else \"\"\n",
    "                    context += f\"ã€Linkã€‘{link_str}\\n\" if link_str else \"\"\n",
    "                    # doc_text å°±æ˜¯æª¢ç´¢å›ä¾†çš„ chunk\n",
    "                    context += f\"ã€è¨è«–ç‰‡æ®µã€‘{doc_text}\\n\\n\"\n",
    "\n",
    "            # 2) è™•ç†å°ˆæ¥­æ–‡ç»\n",
    "            prof_metas = search_results[\"professional\"].get(\"metadata\", [])\n",
    "            prof_docs = search_results[\"professional\"].get(\"documents\", [])\n",
    "\n",
    "            if prof_metas or prof_docs:\n",
    "                context += \"\\n[å°ˆæ¥­æ–‡ç»]\\n\"\n",
    "                for j, meta in enumerate(prof_metas):\n",
    "                    source_file = meta.get(\"source_file\", \"\")\n",
    "                    doc_text = prof_docs[j] if j < len(prof_docs) else \"\"\n",
    "                    # å¦‚æœæ‚¨æœ‰å¦å¤–å­˜æ”¾é ç¢¼ page = meta.get(\"page\"), ä¹Ÿå¯åˆ—å‡º\n",
    "                    page_num = meta.get(\"page\", \"\")\n",
    "                    context += f\"ã€æ–‡ä»¶ç‰‡æ®µã€‘{doc_text}\\n\"\n",
    "                    if source_file:\n",
    "                        context += f\"(æª”æ¡ˆ: {source_file}\"\n",
    "                        context += f\", é : {page_num})\" if page_num else \")\"\n",
    "                    context += \"\\n\\n\"\n",
    "\n",
    "            # åµéŒ¯ç”¨ (å¯ä¿ç•™ä¹Ÿå¯ç§»é™¤)\n",
    "            print(\"social metadata:\", social_metas)\n",
    "            print(\"social links:\", social_links)\n",
    "            print(\"professional metadata:\", prof_metas)\n",
    "\n",
    "            return context if context.strip() else \"åƒè€ƒè³‡æ–™ç„¡æ³•å–å¾—\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting context: {str(e)}\")\n",
    "            return \"Unable to retrieve reference materials\"\n",
    "\n",
    "\n",
    "    def display_response(self, query: str,question_type: Optional[str] = None):\n",
    "            \"\"\"Display response with text and images\"\"\"\n",
    "            try:\n",
    "                logger.info(\"Starting to generate response...\")\n",
    "                try:\n",
    "                    response_text, image_paths = self.generate_response(query,question_type)\n",
    "                except Exception as e:\n",
    "                    response_text = f\"[ERROR] {e}\"\n",
    "                    image_paths = []\n",
    "                \n",
    "                print(\"Question:\", query)\n",
    "                print(\"\\nSystem Response:\")\n",
    "                print(response_text)\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "                if image_paths:\n",
    "                    print(\"\\nRelated Image:\")\n",
    "                    img_path = image_paths[0]  # We now only have one image\n",
    "                    try:\n",
    "                        img = PILImage.open(img_path)\n",
    "                        display(IPyImage(filename=img_path))\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error displaying image {img_path}: {str(e)}\")\n",
    "                else:\n",
    "                    logger.info(\"No images to display\")\n",
    "\n",
    "\n",
    "                return response_text, image_paths # add for response 0406\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in display_response: {str(e)}\", exc_info=True)  \n",
    "                return \"\", [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¡Œç›®æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å–®ä¸€é¡Œç›®æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦æŸ¥è©¢\n",
    "test_queries = [\n",
    "# \"CCD æ˜¯å¦èˆ‡ç¥ç¶“ç™¼ç‚ç›¸é—œï¼Ÿæœ‰ç„¡ç‰¹å®šç´°èƒå› å­ï¼ˆcytokinesï¼‰æˆ–ç™¼ç‚è·¯å¾‘ï¼ˆä¾‹å¦‚NLRP3 inflammasomeï¼‰åƒèˆ‡ï¼Ÿ\",\n",
    "# \"CCD æ˜¯å¦èˆ‡è…¸é“å¾®ç”Ÿç‰©ç¾¤è®ŠåŒ–æœ‰é—œï¼Ÿæ˜¯å¦æœ‰ç‰¹å®šç´°èŒç¾¤è½æœƒå½±éŸ¿å¤§è…¦å¥åº·ï¼Ÿ\",\n",
    "# \" å¤±æ™ºçŠ¬çš„æ¾æœé«”æ˜¯å¦é€€åŒ–\",\n",
    "# \" æœ‰åˆ»æ¿å½¢ç‚ºçš„çŠ¬éš»æ˜¯å¦æœƒå¢åŠ CCDé¢¨éšªï¼Ÿ\",\n",
    "# \" å¤±æ™ºçŠ¬åˆ†æ³Œè¤ªé»‘æ¿€ç´ çš„èƒ½åŠ›æ˜¯å¦é€€åŒ–ï¼Ÿ\",\n",
    "# \" çš®è³ªé¡å›ºé†‡cortisolæˆ–æ‡‰æ¿€è·çˆ¾è’™stress hormonesæ˜¯å¦å¯ä½œç‚º CCD çš„æ½›åœ¨è¨ºæ–·æŒ‡æ¨™ï¼Ÿ\",\n",
    "# \" å¦‚ä½•å€åˆ†æ­£å¸¸è€åŒ–èˆ‡CCDçš„æ—©æœŸå¾µå…†ï¼Ÿ \",\n",
    "# \" B ç¾¤ç¶­ç”Ÿç´ æ˜¯å¦èƒ½é™ä½ CCD é€²å±•é¢¨éšªï¼Ÿ\",\n",
    "# \" é£Ÿç”¨GABAæ˜¯å¦å°æ–¼é é˜²CCDæœ‰æ•ˆï¼Ÿ\",\n",
    "# \" è­¦çŠ¬ã€æ•‘é›£çŠ¬ç­‰å·¥ä½œçŠ¬åœ¨ç½¹æ‚£CCDçš„æ©Ÿç‡æ¯”è¼ƒå®¶åº­é™ªä¼´çŠ¬\",\n",
    "# \" ç›®å‰æ˜¯å¦æœ‰å½±åƒå­¸æª¢æ¸¬å¯ä»¥æº–ç¢ºå€åˆ† CCD èˆ‡å…¶ä»–ç¥ç¶“é€€è¡Œæ€§ç–¾ç—…ï¼Ÿ\",\n",
    "# \" å¦‚æœCCDé€²å±•åˆ°æœ€å¾Œéšæ®µï¼Œå“ªäº›ç—‡ç‹€æœ€éœ€è¦é—œæ³¨ï¼Ÿå¦‚ä½•å¹³è¡¡ç‹—ç‹—çš„ç”Ÿæ´»è³ªé‡èˆ‡ç–¼ç—›ç®¡ç†ï¼Œä¸¦ä¸”æ±ºå®šç‹—ç‹—æœªä¾†çš„æ–¹å‘\",\n",
    "\n",
    "# \"æ ¹æ“šè³‡æ–™ä¸­å°çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ï¼ˆCCDï¼‰ç¥ç¶“ç™¼ç‚æ©Ÿåˆ¶çš„æ¢è¨ï¼ŒNLRP3ç‚ç—‡å°é«”åœ¨åˆ†å­å±¤é¢ä¸Šå¦‚ä½•åƒèˆ‡CCDé€²ç¨‹ï¼Ÿè©²éç¨‹æ¶‰åŠå“ªäº›é—œéµç´°èƒå› å­èˆ‡èª¿æ§æ©Ÿåˆ¶ï¼Ÿ\",\n",
    "# \"è³‡æ–™æåˆ°è…¸é“å¾®ç”Ÿç‰©ç¾¤èˆ‡CCDä¹‹é–“å¯èƒ½å­˜åœ¨è¯ç¹«ï¼Œè«‹å•æ–‡ä¸­å¦‚ä½•é—¡è¿°è…¸é“èŒç¾¤å¤±è¡¡å½±éŸ¿ç¥ç¶“å‚³å°èˆ‡å…ç–«åæ‡‰çš„åˆ†å­æ©Ÿåˆ¶ï¼Ÿå“ªäº›ç‰¹å®šç´°èŒç¾¤è½çš„è®ŠåŒ–è¢«èªç‚ºèˆ‡CCDé€²å±•ç›¸é—œï¼Ÿ\",\n",
    "# \"åœ¨æ¢è¨CCDçš„è¨ºæ–·ç­–ç•¥ä¸­ï¼Œè©²è³‡æ–™å°æ–¼åˆ©ç”¨å½±åƒå­¸æŠ€è¡“ï¼ˆå¦‚MRIèˆ‡CTï¼‰å€åˆ†CCDèˆ‡å…¶ä»–ç¥ç¶“é€€è¡Œæ€§ç–¾ç—…çš„æ‡‰ç”¨æå‡ºäº†å“ªäº›è¦‹è§£ï¼Ÿé€™äº›æŠ€è¡“çš„å„ªå‹¢èˆ‡å±€é™æ€§åˆ†åˆ¥æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "# \"è³‡æ–™ä¸­å°å¤±æ™ºçŠ¬æ¾æœé«”é€€åŒ–èˆ‡è¤ªé»‘æ¿€ç´ åˆ†æ³Œæ¸›å°‘ä¹‹é–“çš„é—œè¯æœ‰è©³ç´°è«–è¿°ï¼Œè«‹å•è©²ç ”ç©¶å¦‚ä½•æè¿°é€™ä¸€ç”Ÿç†è®ŠåŒ–çš„åˆ†å­æ©Ÿåˆ¶ä»¥åŠå…¶å°çŠ¬éš»ç¡çœ -è¦ºé†’é€±æœŸçš„å½±éŸ¿ï¼Ÿ\",\n",
    "# \"é‡å°CCDçš„æ²»ç™‚ç­–ç•¥ï¼Œè³‡æ–™ä¸­æå‡ºäº†å“ªäº›åŸºæ–¼åˆ†å­æ©Ÿåˆ¶çš„æ²»ç™‚æ–¹æ³•ï¼Ÿè«‹åˆ†æé€™äº›æ–¹æ³•åœ¨è‡¨åºŠæ‡‰ç”¨ä¸Šçš„ç¾ç‹€ã€æ½›åœ¨å„ªå‹¢åŠæœªä¾†ç ”ç©¶ä¸­äºŸå¾…è§£æ±ºçš„æŒ‘æˆ°ã€‚\",\n",
    "\n",
    "# \"å“ªç¨®çŠ¬å®¹æ˜“å¤±æ™ºï¼Ÿ\",\n",
    "# \"å¤§ä¸­å°å‹ç‹—çš„å¤±æ™ºç…§é¡§æ–¹å¼æœ‰ä»€éº¼ä¸åŒï¼Ÿ\"\n",
    "# \"æˆ‘çš„ç‹—ç‹—æœ‰å¤±æ™ºç—‡ï¼Œæ™šä¸Šç¸½æ˜¯ç¹åœˆåœˆè€Œä¸”å«å€‹ä¸åœï¼Œæœ‰ä»€éº¼æ–¹æ³•èƒ½å¹«åŠ©ç‰ å®‰éœä¸‹ä¾†ç¡è¦ºå—ï¼Ÿæœ‰äººæ¨è–¦éè¤ªé»‘æ¿€ç´ ï¼Œé€™çœŸçš„æœ‰æ•ˆå—ï¼Ÿ\",\n",
    "# \"æˆ‘çš„è€ç‹—æœ‰èªçŸ¥éšœç¤™ï¼Œç¶“å¸¸å¡åœ¨è§’è½æˆ–å®¶å…·é–“ä¸çŸ¥é“å¦‚ä½•è„«å›°ï¼Œæœ‰ä»€éº¼ç’°å¢ƒå®‰æ’å’Œå±…å®¶ç…§è­·æªæ–½å¯ä»¥å¹«åŠ©ç‰ æ›´èˆ’é©åœ°ç”Ÿæ´»ï¼Ÿå…¶ä»–é£¼ä¸»éƒ½æ˜¯æ€éº¼è™•ç†é€™ç¨®æƒ…æ³çš„ï¼Ÿæœ‰ç›¸é—œç…§ç‰‡å—ï¼Ÿ\",\n",
    "# \"çµ¦æˆ‘ä¸€äº›ç…§è­·ç’°å¢ƒçš„åœ–ç‰‡\",\n",
    "# \"é‡å°å¹´é•·çŠ¬éš»å¯èƒ½å‡ºç¾çš„ç¥ç¶“ç—…ç†è®ŠåŒ–ï¼Œå“ªäº›é—œéµæŒ‡æ¨™å¸¸è¢«ç”¨ä¾†å°æ¯”é˜¿èŒ²æµ·é»˜é¡å‹çš„é€€åŒ–ç—‡ç‹€ï¼Œä¸¦ä¸”èˆ‡è‡¨åºŠè§€å¯Ÿåˆ°çš„è¡Œç‚ºè¡°é€€æœ‰ä½•é—œè¯ï¼Ÿ\",\n",
    "# \"é™¤äº†è—¥ç‰©ä»‹å…¥ä¹‹å¤–ï¼Œå¹³æ™‚é£¼é¤Šç®¡ç†èˆ‡ç’°å¢ƒèª¿æ•´æ–¹é¢æœ‰å“ªäº›å…·é«”ä½œæ³•ï¼Œèƒ½åŒæ™‚æœ‰åŠ©æ–¼å¤±æ™ºçŠ¬èˆ‡å¤±æ™ºè²“ç¶­æŒè¼ƒä½³çš„ç”Ÿæ´»å“è³ªï¼Œä¸¦ç‚ºä½•å¤šç¨®æ–¹å¼ä¸¦ç”¨çš„ç…§è­·ç­–ç•¥å¾€å¾€æ›´èƒ½å»¶ç·©èªçŸ¥é€€åŒ–ï¼Ÿ\",\n",
    "# \"è‹¥ä»¥è€çŠ¬ä½œç‚ºæ¨¡æ“¬äººé¡è€åŒ–èˆ‡å¤±æ™ºçš„å¯¦é©—æ¨¡å‹ï¼Œé€²è¡ŒèªçŸ¥å¢ç›Šæˆ–æ²»ç™‚æ€§è—¥ç‰©çš„è©•ä¼°æ™‚ï¼Œæœ€å¸¸æ¡ç”¨å“ªäº›è©•é‡æ–¹æ³•ä¾†ç¢ºèªè—¥ç‰©å°è¡Œç‚ºå’Œç¥ç¶“åŠŸèƒ½çš„å½±éŸ¿ï¼Œä¸¦ä¸”åœ¨å“ªäº›ç¥ç¶“å‚³å°è·¯å¾‘ä¸Šé€šå¸¸æœƒçœ‹åˆ°è¼ƒæ˜é¡¯çš„æŒ‡æ¨™æ€§è®ŠåŒ–ï¼Ÿ\",\n",
    "# \"In older dogs, which key indicators are commonly used to compare with Alzheimer-type degeneration, and how do these indicators relate to clinically observed behavioral decline?\",\n",
    "# \"Beyond pharmacological intervention, which specific management and environmental adjustments help senior dogs and cats with cognitive impairment maintain a higher quality of life, and why does combining multiple caregiving strategies often slow cognitive decline more effectively?\",\n",
    "# \"When using senior dogs as a model for human aging and dementia to evaluate cognitive-enhancing or therapeutic drugs, what assessment methods are most commonly employed to gauge the drugâ€™s effects on behavior and neurological function, and in which neurotransmission pathways are the most prominent changes typically observed?\"\n",
    "\"åœ¨è©•ä¼°çŠ¬éš» CCD çš„è‡¨åºŠç—‡ç‹€æ™‚ï¼Œä¸‹åˆ—å“ªä¸€é …è¡Œç‚ºé¢å‘æœ€å¸¸è¢«åˆ—ç‚ºä¸»è¦è§€å¯ŸæŒ‡æ¨™ä¹‹ä¸€? A. æ¯›è‰²æ˜¯å¦è®Šç™½ B. é£²æ°´é‡çš„å¢åŠ  C. å®šå‘èƒ½åŠ› (Orientation) èˆ‡ç©ºé–“è¾¨è­˜åº¦ D. å¿ƒè·³èˆ‡å‘¼å¸é€Ÿç‡\"\n",
    "\n",
    "                    ]\n",
    "\n",
    "for query in test_queries:\n",
    "    qa_system.display_response(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Chroma with directory: chroma_db\n",
      "INFO:__main__:Using collection 'clip_collection' (dimension=768, reset=False)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "TEST_MODE = False                           # â† åˆ‡æ›é–‹é—œ\n",
    "COLLECTION_NAME = \"clip_collection\"   # æ¸¬è©¦ç”¨å‘é‡åº«\n",
    "\n",
    "# 1) åˆå§‹åŒ– embedding_processorï¼Œå‚³å…¥æ–°çš„ collection_name\n",
    "embedding_processor = EmbeddingProcessor(\n",
    "    image_size=(224, 224) ,\n",
    "    collection_name=COLLECTION_NAME,    # â˜…è‹¥ __init__ æ²’é€™åƒæ•¸ï¼Œæ”¹ä¸‹æ–¹è¨»è§£æ–¹æ³•\n",
    "    reset=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é‡å»ºDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨\n",
    "data_processor = DataProcessor(embedding_processor)\n",
    "\n",
    "# 3) æŒ‡å®šæ¸¬è©¦æˆ–æ­£å¼è³‡æ–™å¤¾\n",
    "rag_data_dir = Path(\"RAG_data_test\" if TEST_MODE else \"RAG_data\")\n",
    "pdf_paths = list(rag_data_dir.glob(\"*.pdf\"))\n",
    "\n",
    "\n",
    "print(\"æ‰¾åˆ°ä»¥ä¸‹ PDFï¼š\")\n",
    "for p in pdf_paths: print(\" -\", p.name)\n",
    "\n",
    "# 4) è™•ç†è³‡æ–™ ï¼ˆCSV ä½ å¯ä»¥å‚³ None ä»£è¡¨ä¸è™•ç†ç¤¾ç¾¤è³‡æ–™ï¼‰\n",
    "_ = data_processor.process_all(\n",
    "    csv_path=\"post_response_filtered.xlsx\",           # åªæ¸¬ PDFï¼Œå¯å…ˆä¸ç®¡ç¤¾ç¾¤\n",
    "    pdf_paths=pdf_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized QA System with Ollama model: llama3.2-vision\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹ QA ç³»çµ±ï¼Œæ²¿ç”¨åŒä¸€å€‹ embedding_processor\n",
    "qa_system = QASystem(\n",
    "    embedding_processor=embedding_processor,\n",
    "    model_name='llama3.2-vision'\n",
    ")\n",
    "# TARGET_DOMAIN = \"\"   # æƒ³æ¸¬å“ªå€‹å°±å¡«å“ªå€‹\n",
    "# qa_system.TARGET_DOMAIN = TARGET_DOMAIN   # è‹¥ä½ å¯«æˆå±¬æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_zh_to_en(chinese_text: str) -> str:\n",
    "    try:\n",
    "        # æŒ‡å®šåŸæ–‡èªè¨€ç‚º 'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œç›®æ¨™èªè¨€ç‚º 'en'ï¼ˆè‹±æ–‡ï¼‰\n",
    "        translator = GoogleTranslator(source='zh-TW', target='en')\n",
    "        result = translator.translate(chinese_text)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"ç¿»è­¯éŒ¯èª¤ï¼š{e} - å°æ‡‰ä¸­æ–‡å•é¡Œï¼š{chinese_text}\")\n",
    "        return chinese_text  # è‹¥ç¿»è­¯å¤±æ•—ï¼Œè¿”å›åŸæ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_answer(llm_response: str, q_type: str) -> str:\n",
    "    \"\"\"\n",
    "    æ ¹æ“šé¡Œå‹ (é¸æ“‡ or æ˜¯é)ï¼Œå¾ LLM çš„å›è¦†å­—ä¸²ä¸­è§£æå‡ºå¯èƒ½çš„æœ€çµ‚ç­”æ¡ˆã€‚\n",
    "    \"\"\"\n",
    "    if not llm_response or not llm_response.strip():\n",
    "        return \"\" \n",
    "\n",
    "    # æŠŠå›è¦†éƒ½è½‰å°å¯«ï¼Œä»¥ä¾¿æœå°‹\n",
    "    q_type = q_type.strip().lower()        # ä¿éšªèµ·è¦‹\n",
    "    cleaned = llm_response.strip()\n",
    "    lines = [ln.strip() for ln in llm_response.splitlines() if ln.strip()]\n",
    "    \n",
    "    \n",
    "    if q_type == \"multiple_choice\":\n",
    "       \n",
    "         # å…ˆæŠ“æœ€å¾Œä¸€è¡Œ\n",
    "        last = lines[-1]\n",
    "        if re.fullmatch(r\"[ABCDabcd]\", last):\n",
    "            return last.upper()\n",
    "        # fallbackï¼šæ‰¾ 'ç­”æ¡ˆï¼šB'\n",
    "        m = re.search(r\"ç­”æ¡ˆ[:ï¼š\\s]*([ABCDabcd])\", llm_response)\n",
    "        return m.group(1).upper() if m else \"\"\n",
    "    \n",
    "    elif q_type == \"true_false\":\n",
    "\n",
    "        for line in reversed(llm_response.splitlines()):\n",
    "            line = line.strip().lower()\n",
    "            if line.startswith((\"çµè«–\", \"ç­”æ¡ˆ\")):\n",
    "                if \"true\" in line or \"æ˜¯\" in line:\n",
    "                    return \"True\"\n",
    "                if \"false\" in line or \"å¦\" in line or \"ä¸\" in line:\n",
    "                    return \"False\"\n",
    "                \n",
    "        negative_phrases = [\n",
    "            \"ä¸æ˜¯\", \"å¦\", \"ä¸å°\", \"false\", \"no\", \"ä¸å¯ä»¥\",\n",
    "            \"ä¸èƒ½\", \"ä¸è¡Œ\", \"never\", \"cannot\"\n",
    "        ]\n",
    "        positive_phrases = [\n",
    "            \"æ˜¯\", \"å°\", \"true\", \"yes\", \"å¯ä»¥\",\n",
    "            \"èƒ½\", \"è¡Œ\", \"å¯ä»¥çš„\", \"æ²’å•é¡Œ\"\n",
    "        ]\n",
    "       # å»æ‰æ¨™é»\n",
    "        text_nopunct = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", cleaned)\n",
    "\n",
    "        for phrase in negative_phrases:\n",
    "            if phrase in text_nopunct:\n",
    "                return \"False\"\n",
    "        for phrase in positive_phrases:\n",
    "            if phrase in text_nopunct:\n",
    "                return \"True\"\n",
    "        return \"UNKNOWN\"\n",
    "    \n",
    "    else:\n",
    "        return \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è®€æª” + é¡Œå‹ç¯©é¸\n",
    "df = pd.read_excel(\"test_questions.xlsx\")\n",
    "# test_df = df[df[\"type\"].isin([\"multiple_choice\", \"true_false\"])].copy()\n",
    "test_df = df[df[\"type\"].isin([\"multiple_choice\"])].copy()\n",
    "\n",
    "# 2. â˜… å»ºç«‹æ¬„ä½ï¼ˆä¸€å®šè¦åœ¨å¾Œé¢çš„ç¯©é¸å‰å…ˆåŠ ï¼‰\n",
    "test_df[\"llm_response\"] = \"\"\n",
    "test_df[\"predicted\"]    = \"\"\n",
    "test_df[\"is_correct\"]   = 0\n",
    "\n",
    "# 3. å†ä¾ domain ç¯©å­é›†åˆ\n",
    "# test_df = test_df[test_df[\"domain\"] == \"ä¸­é†«\"].copy()\n",
    "test_df=test_df.head(3)\n",
    "\n",
    "# 4. è¿´åœˆè¨ˆåˆ†\n",
    "for idx, row in test_df.iterrows():\n",
    "    q  = row[\"question\"]\n",
    "    q_type = row[\"type\"]\n",
    "    gt = str(row[\"answers\"]).strip()\n",
    "\n",
    "    resp, _ = qa_system.display_response(q, q_type)\n",
    "\n",
    "    if not resp.strip():\n",
    "        print(f\"[WARN] id={row['id']}  LLM å›å‚³ç©ºç™½\")\n",
    "\n",
    "    pred = parse_llm_answer(resp, q_type)\n",
    "\n",
    "    test_df.at[idx, \"llm_response\"] = resp\n",
    "    test_df.at[idx, \"predicted\"]    = pred\n",
    "    test_df.at[idx, \"is_correct\"]   = int(pred.upper() == gt.upper())\n",
    "\n",
    "# 5. è¨ˆç®— Accuracy\n",
    "overall_acc = test_df[\"is_correct\"].mean()\n",
    "\n",
    "print(\"\\n=== æ¯å€‹ domain çš„ Accuracy ===\")\n",
    "domain_stats = (\n",
    "    test_df.groupby(\"domain\")[\"is_correct\"]\n",
    "           .agg([\"count\", \"sum\"])\n",
    "           .reset_index()\n",
    "           .rename(columns={\"sum\": \"correct\"})\n",
    ")\n",
    "domain_stats[\"accuracy\"] = domain_stats[\"correct\"] / domain_stats[\"count\"]\n",
    "print(domain_stats.to_string(index=False, \n",
    "      formatters={\"accuracy\": \"{:.2%}\".format}))\n",
    "\n",
    "print(f\"\\nOVERALL Accuracy = {overall_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== æ¸¬è©¦çµæœ ===\")\n",
    "wq = test_df[test_df[\"is_correct\"] == 0].copy()\n",
    "# total = len(wq)\n",
    "# correct_count = wq[\"is_correct\"].sum()\n",
    "print(wq[[\"id\",\"type\",\"answers\",\"llm_response\",\"predicted\"]])\n",
    "# print(f\"\\nå…± {total} é¡Œï¼Œæ­£ç¢º {correct_count} é¡Œï¼ŒAccuracy = {accuracy:.2f}\")\n",
    "    \n",
    "# ======= è‹¥éœ€è¦å°‡å›è¦†çµæœè¼¸å‡º CSV \n",
    "wq.to_excel(\"wq.xlsx\", index=False)\n",
    "print(\"çµæœå·²å„²å­˜ test_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
