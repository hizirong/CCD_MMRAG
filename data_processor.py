from embedding import EmbeddingProcessor
from logger import logger
from typing import List, Dict, Tuple
import PyPDF2
from pathlib import Path
import pandas as pd
import re

TYPE_MAP = {
    "acupoint"    : ["é‡ç¸", "acupuncture"],
    "herb"        : ["herbology", "herbal", "æ–¹åŠ‘"],
    "ccd"         : ["ccd", "èªçŸ¥", "cognition"],
    "social"      : [],                   # csv ç›´æ¥æŒ‡å®š
    "professional": [],
    "image":[]                                       # å…¶ä»–æœªåˆ†é¡
}


class DataProcessor:
    def __init__(self, embedding_processor: 'EmbeddingProcessor'):
        self.embedding_processor = embedding_processor
        
    def extract_social_posts(self, csv_path: str) -> Tuple[List[Dict], List[str]]:
        """å¤„ç† CSV å¹¶æå–é—®ç­”å¯¹å’Œå›¾ç‰‡"""
        logger.info(f"Processing CSV: {csv_path}")
        qa_pairs = []
        images = []
        
        df = pd.read_excel(csv_path)
        current_post = None
        current_responses = []
        current_images = []
        current_link = None
        
        for _, row in df.iterrows():
            # å¤„ç†æ–°çš„å¸–å­
            if pd.notna(row['post']):
                # ä¿å­˜å‰ä¸€ä¸ªé—®ç­”å¯¹
                if current_post is not None:
                    qa_pair = {
                        'question': current_post,
                        'answers': current_responses.copy(),
                        'images': current_images.copy(),
                        'metadata': {
                            'type': 'social',
                            'source': 'facebook',
                            'images': ','.join(current_images) if current_images else '',
                            'answer_count': len(current_responses),
                            'link': current_link if current_link else ''
                        }
                    }
                    if pd.notna(row.get('image_description')):
                        qa_pair['metadata']['image_desc'] = row['image_description']

                    qa_pairs.append(qa_pair)
                    if current_images:
                        images.extend(current_images)
                
                # åˆå§‹åŒ–æ–°çš„é—®ç­”å¯¹
                current_post = row['post']
                current_responses = []
                current_images = []
                current_link = row.get('link', '')
            
            # æ·»åŠ å›å¤
            if pd.notna(row.get('responses')):
                current_responses.append(row['responses'])
            
            # å¤„ç†å›¾ç‰‡
            if pd.notna(row.get('images')):
                img_path = row['images']
                current_images.append(img_path)
                logger.info(f"Found image: {img_path} for current post")
  
        
        # ä¿å­˜æœ€åä¸€ä¸ªé—®ç­”å¯¹
        if current_post is not None and len(current_responses) >= 3:
            qa_pair = {
                'question': current_post,
                'answers': current_responses,
                'images': current_images,
                'metadata': {
                    'type': 'social_qa',
                    'source': 'facebook',
                    'images': ','.join(current_images) if current_images else '',
                    'answer_count': len(current_responses),
                    'link': current_link if current_link else ''
                }
            }
            if pd.notna(row.get('image_description')):
                    qa_pair['metadata']['image_desc'] = row['image_description']

            qa_pairs.append(qa_pair)
            if current_images:
                images.extend(current_images)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
        for i, qa in enumerate(qa_pairs):
            logger.info(f"\nQA Pair {i+1}:")
            logger.info(f"Question: {qa['question'][:100]}...")
            logger.info(f"Number of answers: {len(qa['answers'])}")
            logger.info(f"Images: {qa['images']}")
            logger.info(f"Link: {qa.get('link', 'No link')}")
        
        return qa_pairs, images

    
    def chunk_text(self,paragraph: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
        """
        å°‡çµ¦å®šæ®µè½ï¼Œä»¥ chunk_size å­—ç¬¦ç‚ºä¸Šé™é€²è¡Œåˆ‡åˆ†ï¼Œä¸¦ä¸”åœ¨ chunk ä¹‹é–“ä¿ç•™ overlap å€‹å­—çš„é‡ç–Šï¼Œ
        ä»¥å…ä¸Šä¸‹æ–‡æ–·è£‚ã€‚
        å‚™è¨»: 
        - é€™è£¡ä»¥ã€Œå­—ç¬¦ã€ç‚ºå–®ä½ï¼Œé©åˆä¸­æ–‡ï¼›è‹±æ–‡ä¹Ÿå¯ç”¨ï¼Œä½†è‹¥æƒ³ç²¾ç¢ºå°è‹±æ–‡ tokens å¯æ”¹æ›´å…ˆé€²æ–¹æ³•ã€‚
        """
        chunks = []
        start = 0
        length = len(paragraph)

        # å»æ‰å‰å¾Œå¤šé¤˜ç©ºç™½
        paragraph = paragraph.strip()

        while start < length:
            end = start + chunk_size
            # å– substring
            chunk = paragraph[start:end]
            chunks.append(chunk)
            # ç§»å‹•æŒ‡æ¨™(ä¸‹ä¸€å€‹ chunk)
            # overlap é é˜²æ–·å¥å¤±å»ä¸Šä¸‹æ–‡
            start += (chunk_size - overlap)

        return chunks
  

    def process_pdf(self, pdf_path: str,row_type: str) -> List[Dict]:
        logger.info(f"Processing PDF: {pdf_path}")
        professional_qa_pairs = []
        pdf_name = Path(pdf_path).name  # è·å–æ–‡ä»¶å
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                is_formula = self.detect_domain(pdf_name) == "ä¸­é†«æ–¹åŠ‘"
                is_acu = self.detect_domain(pdf_name) == "é‡ç¸å­¸"


                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    print(f"Page {page_num+1} raw text:", repr(text))
                    if is_formula:
                        paragraphs = self.split_formula_blocks(text)
                    elif is_acu:
                        paragraphs = self.split_acu_blocks(text)
                    else:
                        paragraphs = text.split('\n\n')
                    
                    
                    # è™•ç†æ¯å€‹æ®µè½
                    for para in paragraphs:
                        # logger.info(f"Para type: {type(para)}")

                        para_chunks = self.chunk_text(para)
                        # logger.info(f"Got {len(para_chunks)} chunks from chunk_text()")

                        for c in para_chunks:
                            qa_pair = {
                                'question': c[:50] + "...",  
                                'answers': [c],
                                'metadata': {
                                    'type': row_type,
                                    'source_file': pdf_name,  # æ·»åŠ æ–‡ä»¶å
                                    'page': str(page_num + 1),
                                    'content_length': str(len(c))
                                } #'domain':self.detect_domain(pdf_name),
                            }
                            professional_qa_pairs.append(qa_pair)
                
                logger.info(f"Extracted {len(professional_qa_pairs)} paragraphs from {pdf_name}")
                return professional_qa_pairs
                
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_name}: {str(e)}")
            return []
        
    def detect_domain(self, pdf_name: str) -> str:
        lower = pdf_name.lower()

        if "é‡ç¸" in pdf_name or "acupuncture" in lower:
            return "é‡ç¸å­¸"
        if "herbal" in lower or "herbology" in lower or "æ–¹åŠ‘" in pdf_name:
            return "ä¸­é†«æ–¹åŠ‘"
        return "å…¶ä»–"
    
    def split_formula_blocks(self,text: str) -> list[str]:
        """
        ç”¨æ­£å‰‡æŠ“å‡ºã€â— å…­å‘³åœ°é»ƒä¸¸ã€æˆ–ã€Liu Wei Di Huang Wanã€é–‹é ­ï¼Œ
        æ¯é‡ä¸‹ä¸€å€‹æ–¹åå°±çµæŸä¸Šä¸€å¡Š
        """
        pattern = re.compile(r"(?:â—|\s)([\u4e00-\u9fffA-Za-z\- ]{3,40}(?:æ¹¯|ä¸¸|é£²|æ•£|è†))")
        blocks = []
        cur_block = []
        for line in text.splitlines():
            if pattern.search(line):
                # é‡åˆ°ä¸‹ä¸€å¸–è—¥ â†’ å…ˆæ”¶å‰ä¸€å¸–
                if cur_block:
                    blocks.append("\n".join(cur_block).strip())
                    cur_block = []
            cur_block.append(line)
        if cur_block:
            blocks.append("\n".join(cur_block).strip())
        return [b for b in blocks if len(b) > 60]    

    def split_acu_blocks(self,text: str) -> list[str]:
        # ç¯„ä¾‹ä»£ç¢¼ï¼šLIâ€‘11ã€HT-7ã€SI 3
        pattern = re.compile(r"\b([A-Z]{1,2}[ -â€‘]\d{1,3})\b")
        blocks, cur = [], []
        for line in text.splitlines():
            if pattern.search(line):
                if cur: blocks.append("\n".join(cur).strip()); cur = []
            cur.append(line)
        if cur: blocks.append("\n".join(cur).strip())
        return [b for b in blocks if len(b) > 40]

    def process_all(self, csv_path: str, pdf_paths: List[str]):
        """ç¶œåˆè™•ç†ç¤¾ç¾¤ CSV + PDFs"""
        try:
            social_qa_pairs, images = [], []  
            # 1. å¤„ç†ç¤¾ç¾¤æ•°æ®
            if csv_path: 
                social_qa_pairs, images = self.extract_social_posts(csv_path)
                logger.info(f"\nProcessed social data:")
                logger.info(f"- Social QA pairs: {len(social_qa_pairs)}")
                logger.info(f"- Images found: {len(images)}")
            else:
                logger.info("Skip social CSV, onlyè™•ç† PDFs")
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
            valid_images = []
            for img in images:
                img_path = Path(self.embedding_processor.image_dir) / img
                if img_path.exists():
                    valid_images.append(img)
            
            # 2. å¤„ç†æ‰€æœ‰ PDF
            all_professional_pairs = []
            for pdf_path in pdf_paths:
                pdf_name = pdf_path.name.lower()
                for t, keys in TYPE_MAP.items():
                    if any(k in pdf_name for k in keys):
                        row_type = t; break
                else:
                    row_type = "professional"
                pdf_qa_pairs = self.process_pdf(pdf_path, row_type=row_type)
                #pdf_qa_pairs = self.process_pdf(pdf_path)
                all_professional_pairs.extend(pdf_qa_pairs)
                logger.info(f"\nProcessed {Path(pdf_path).name}:")
                logger.info(f"- Extracted paragraphs: {len(pdf_qa_pairs)}")
            
            # 3. åˆå¹¶ => all_qa_pairs
            all_qa_pairs = social_qa_pairs + all_professional_pairs
            
            # 4. æº–å‚™ texts + metadatas => ä½ å°±èƒ½ä¸€æ¬¡æˆ–å¤šæ¬¡å‘¼å« add_vectors
            questions = []
            answers = []
            question_metas = []
            answer_metas = []
            
            # å¤„ç†æ‰€æœ‰é—®ç­”å¯¹
            for qa_pair in all_qa_pairs:
                # question 
                questions.append(qa_pair['question'])
                question_metas.append(qa_pair['metadata'])
                
                # answers
                for ans_text in qa_pair['answers']:
                    answers.append(ans_text)
                    am = qa_pair['metadata'].copy()
                    am['parent_question'] = qa_pair['question']
                    answer_metas.append(am)

            # ------------- é€™è£¡æ‰é–‹å§‹çµ„ professional texts / metas -------------
            prof_texts = [qa["answers"][0] for qa in all_professional_pairs]
            prof_metas = [qa["metadata"]   for qa in all_professional_pairs]

            
            # è¾“å‡ºå¤„ç†ç»“æœ
            logger.info(f"\nFinal processing summary:")
            logger.info(f"- Total questions: {len(questions)}")
            logger.info(f"- Total answers: {len(answers)}")
            logger.info(f"- Valid images: {len(valid_images)}")
            logger.info(f"- Social content: {len(social_qa_pairs)} QA pairs")
            logger.info(f"- Professional content: {len(all_professional_pairs)} paragraphs")
            


            # --------- ğŸ”§ æŠŠ 3 çµ„ metadata éƒ½ä¿è­‰æ˜¯ dict (æ”¾åœ¨æ­¤è™•) ---------
            question_metas = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in question_metas]
            prof_metas     = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in prof_metas]
            # è‹¥è¦ç”¨ answer_metas ä¹Ÿä¸€ä½µè™•ç†
            answer_metas   = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in answer_metas]


            self.embedding_processor.add_vectors(texts=prof_texts,
                                            metadatas=prof_metas)
            
            # (A) å…ˆåŠ æ‰€æœ‰ question
            self.embedding_processor.add_vectors(
                texts = questions,
                metadatas = question_metas
            )

            if valid_images:
                meta_for_imgs = []
                for img_name in valid_images:
                    meta_for_imgs.append({
                        "type":"image",
                        "source":"facebook",
                        "filename": img_name
                    })

                self.embedding_processor.add_vectors(
                    images=valid_images,
                    metadatas=meta_for_imgs
                )

            logger.info("All data added to clip_collection.")
            return len(questions), len(valid_images) #return questions, question_metas, all_professional_pairs, valid_images
                
        except Exception as e:
            logger.error(f"Error processing documents: {str(e)}")
            raise
