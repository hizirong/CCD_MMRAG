#!/usr/bin/env python
# coding: utf-8

# #### è½‰py

# In[307]:


get_ipython().system('jupyter nbconvert --to script rag_system_llama.ipynb')


# ### Code

# ##### import

# In[161]:


import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import pandas as pd
import numpy as np
import re

# Display and Image handling
from IPython.display import display
from PIL import Image as PILImage  # ä½¿ç”¨ PILImage ä½œä¸º PIL.Image çš„åˆ«å
from IPython.display import Image as IPyImage  # ä½¿ç”¨ IPyImage ä½œä¸º IPython çš„ Image

# Vector DB
import chromadb


# LLM
import ollama

# PDFå¤„ç†
import PyPDF2

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ£€æŸ¥å¹¶åˆ›å»ºå¿…è¦çš„ç›®å½•
Path('chroma_db').mkdir(exist_ok=True)
Path('image').mkdir(exist_ok=True)


# In[22]:


import sys
import torch
import transformers
import accelerate
# print(f"Python version: {sys.version}")
# print(f"PyTorch version: {torch.__version__}")
# print(f"Transformers version: {transformers.__version__}")
# print(f"Accelerate version: {accelerate.__version__}")


# In[162]:


# æ”¾åœ¨æª”æ¡ˆæœ€ä¸Šæ–¹ (import ä¹‹å¾Œ)
TYPE_MAP = {
    "acupoint"    : ["é‡ç¸", "acupuncture"],
    "herb"        : ["herbology", "herbal", "æ–¹åŠ‘"],
    "ccd"         : ["ccd", "èªçŸ¥", "cognition"],
    "social"      : [],                   # csv ç›´æ¥æŒ‡å®š
    "professional": [],
    "image":[]                                       # å…¶ä»–æœªåˆ†é¡
}


# ##### voice to text
# 

# In[ ]:


# whisper /Users/zirong/Desktop/test.mp4 --language Chinese --model tiny
import whisper
def transcribe_file(file_path, model_size="base"):
    model = whisper.load_model(model_size)
    result = model.transcribe(file_path)
    return result["text"]

# def main():
#     audio_file = "no_upload/test_mp3/01.mp3"  # ä¿®æ”¹ç‚ºä½ çš„éŸ³æª”è·¯å¾‘
#     transcription = transcribe_file(audio_file)
#     print("Transcription:", transcription)

# if __name__ == "__main__":
#     main()


# #### ImageProcessor

# In[163]:


from typing import Union  # æ·»åŠ  Union å¯¼å…¥
from pathlib import Path

class ImageProcessor:
    def __init__(self, image_dir: str = "image"):
        self.image_dir = Path(image_dir)
        self.image_dir.mkdir(exist_ok=True)
        
    def process_and_save(
        self,
        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |
        target_size: Tuple[int, int],
        prefix: str = "resized_",
        quality: int = 95
    ) -> Optional[Path]:
        """ç»Ÿä¸€çš„å›¾ç‰‡å¤„ç†æ–¹æ³•ï¼Œå¤„ç†å¹¶ä¿å­˜å›¾ç‰‡"""
        try:
            # ç¡®ä¿ image_path æ˜¯ Path å¯¹è±¡
            image_path = Path(image_path)
            if not str(image_path).startswith(str(self.image_dir)):
                image_path = self.image_dir / image_path
                
            # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
            if not image_path.exists():
                logger.error(f"Image not found: {image_path}")
                return None
                
            # è¯»å–å¹¶å¤„ç†å›¾ç‰‡
            image = PILImage.open(image_path)
            
            # è½¬æ¢ä¸º RGB æ¨¡å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')
                
            # è®¡ç®—ç­‰æ¯”ä¾‹ç¼©æ”¾çš„å¤§å°
            width, height = image.size
            ratio = min(target_size[0]/width, target_size[1]/height)
            new_size = (int(width * ratio), int(height * ratio))
            
            # ç¼©æ”¾å›¾ç‰‡
            image = image.resize(new_size, PILImage.Resampling.LANCZOS)
            
            # åˆ›å»ºæ–°çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
            new_image = PILImage.new('RGB', target_size, (255, 255, 255))
            
            # è®¡ç®—å±…ä¸­ä½ç½®
            x = (target_size[0] - new_size[0]) // 2
            y = (target_size[1] - new_size[1]) // 2
            
            # è´´ä¸Šç¼©æ”¾åçš„å›¾ç‰‡
            new_image.paste(image, (x, y))
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_path = self.image_dir / f"{image_path.name}" #output_path = self.image_dir / f"{prefix}{image_path.name}"
            # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
            new_image.save(output_path, quality=quality)
            logger.info(f"Saved processed image to: {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error processing image {image_path}: {str(e)}")
            return None
            
    def load_for_display(self, 
                        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |
                        display_size: Tuple[int, int]) -> Optional[PILImage.Image]:
        """è½½å…¥å›¾ç‰‡ç”¨äºæ˜¾ç¤º"""
        try:
            processed_path = self.process_and_save(image_path, display_size, prefix="display_")
            if processed_path:
                return PILImage.open(processed_path)
            return None
        except Exception as e:
            logger.error(f"Error loading image for display {image_path}: {str(e)}")
            return None


# #### EmbeddingProcessor

# In[328]:


get_ipython().run_line_magic('matplotlib', 'inline')
from transformers import AutoProcessor, AutoModel
import torch
import sentencepiece as spm 
import uuid

class EmbeddingProcessor:

    MAX_TOKEN = 56          # 56 + BOS + EOS = 58 < 64
    OVERLAP   = 16
    DEFAULT_COLLECTION = "ccd_docs_siglip"

    # åˆå§‹åŒ– embedding processor
    def __init__(self, 
                persist_directory: str = "chroma_db",
                image_dir: str = "image",
                image_size: tuple = (224, 224),
                collection_name:str = DEFAULT_COLLECTION,
                reset: bool = False 
                ):
        """
        åˆå§‹åŒ–: å»ºç«‹clip_collection,ä½¿ç”¨CLIP(or OpenCLIP)åšembedding
        """
        # ---------- è·¯å¾‘ & åŸºæœ¬è¨­å®š ----------
        self.image_dir = Path(image_dir)
        self.image_size = image_size
        self.collection_name = collection_name
        self.image_processor = ImageProcessor(image_dir)

        # ---------- 1) å»ºç«‹ Chroma client ----------
        logger.info(f"Initializing Chroma with directory: {persist_directory}")
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)

        # ---------- 2) reset (= åˆªæ‰èˆŠåº«) ----------
        if reset:
            try:
                self.chroma_client.delete_collection(self.collection_name)
                logger.info(f"Deleted collection: {self.collection_name}")
            except (chromadb.errors.NotFoundError, ValueError):
                logger.info("No old collection to delete")

        
        # ---------- 3) åˆå§‹åŒ– SigLIP ----------
        SIGLIP_NAME = "google/siglip-base-patch16-224"
        self.processor = AutoProcessor.from_pretrained(SIGLIP_NAME)
        self.siglip    = AutoModel.from_pretrained(SIGLIP_NAME)

        # å–è¼¸å‡ºå‘é‡é•·åº¦ (base = 768)
        with torch.no_grad():
            dummy = torch.zeros((1, 3, 224, 224))
            self.clip_dim = self.siglip.get_image_features(dummy).shape[1]

        # ---------- 4) å–å¾—æˆ–å»ºç«‹ collection ----------
        self.clip_collection = self.chroma_client.get_or_create_collection(
            name     = self.collection_name,
            metadata = {"dimension": self.clip_dim}
        )
        logger.info(
            f"Using collection '{self.collection_name}' "
            f"(dimension={self.clip_dim}, reset={reset})"
        )

    def to_2d(self,x):
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()
        elif isinstance(x, list):
            x = np.asarray(x, dtype=np.float32)
        if x.ndim == 1:        # (512,) â†’ (1,512)
            x = x[None, :]
        elif x.ndim != 2:
            raise ValueError(f"embedding ndim should be 1 or 2, got {x.shape}")
        return x.tolist()      # List[List[float]]

    def chunk_text_by_token(
            self,
            text: str,
            max_tokens: Optional[int] = None,
            overlap: Optional[int] = None
        ) -> List[str]:
        CH_SENT_SPLIT = re.compile(r'([ã€‚ï¼ï¼Ÿï¼›\n])')
        """å¥è™Ÿå„ªå…ˆæ–·å¥ï¼›ä»»ä½•å­å¥æœ€çµ‚éƒ½ â‰¤ 56 token"""
        max_tokens = max_tokens or self.MAX_TOKEN      # 56
        overlap    = overlap    or self.OVERLAP        # 16

        # --- 1) ä»¥ä¸­æ–‡æ¨™é»åˆ‡æˆå­å¥ ---
        sentences, buf, parts = [], "", CH_SENT_SPLIT.split(text)
        for frag in parts:
            if CH_SENT_SPLIT.match(frag):
                buf += frag          # æŠŠæ¨™é»åŠ å›ä¾†
                sentences.append(buf.strip())
                buf = ""
            else:
                buf += frag
        if buf: sentences.append(buf.strip())

        # --- 2) ä»»ä½• >56 token çš„å¥å­å†æ»‘çª—åˆ‡ ---
        chunks = []
        for s in sentences:
            ids = self.processor.tokenizer(s).input_ids
            if len(ids) <= max_tokens:
                chunks.append(s)
            else:
                step = max_tokens - overlap
                for i in range(0, len(ids), step):
                    seg_ids = ids[i:i + max_tokens]
                    seg = self.processor.tokenizer.decode(seg_ids,
                                                        skip_special_tokens=True)
                    chunks.append(seg)
        return chunks


    def encode_text_to_vec(self, text: str) -> Optional[np.ndarray]:
        """
        ç”¨ CLIP çš„ text encoder å°‡æ–‡å­—è½‰ç‚º512ç¶­å‘é‡
        """
        try:
            chunks = self.chunk_text_by_token(text)
            if not chunks:
                logger.error("No valid chunks generated for the text.")
                return None
            all_vecs = []
            for ch in chunks:
                inp = self.processor(text=[ch], return_tensors="pt").to(self.siglip.device)
                with torch.no_grad():
                    vec = self.siglip.get_text_features(**inp)
                all_vecs.append(vec)
            # é€™è£¡å¯å–å¹³å‡æˆ–ç›´æ¥å›å‚³å¤šæ¢å‘é‡
            embs = torch.stack(all_vecs).mean(dim=0)
            emb = embs / embs.norm(dim=-1, keepdim=True)
            return emb.squeeze(0).cpu().tolist()   
        except Exception as e:
            logger.error(f"Error in encode_text_to_vec: {e}")
            return None
        
    def add_qa_pairs(self,
                questions: List[str],
                answers: List[str],
                question_metadatas: List[Dict],
                answer_metadatas: List[Dict],
                images: Optional[List[str]] = None):
        """æ·»åŠ é—®ç­”å¯¹åˆ°ä¸åŒçš„é›†åˆ"""
        try:
            # æ·»åŠ é—®é¢˜
            if questions and question_metadatas:
                logger.info(f"Adding {len(questions)} questions")
                self.question_collection.add(
                    documents=questions,
                    metadatas=question_metadatas,
                    ids=[f"q_{i}" for i in range(len(questions))]
                )
            
            # æ·»åŠ å›ç­”
            if answers and answer_metadatas:
                logger.info(f"Adding {len(answers)} answers")
                self.answer_collection.add(
                    documents=answers,
                    metadatas=answer_metadatas,
                    ids=[f"a_{i}" for i in range(len(answers))]
                )
            
            # å¤„ç†å›¾ç‰‡
            if images:
                logger.info(f"Processing {len(images)} images")
                all_ids=[]
                all_embeddings=[]
                all_metadatas = []

                
                for i, (img_path,question_text) in enumerate(zip(images, questions)):
                    img_emb = self.process_image(str(self.image_dir / img_path))
                    txt_emb = self.encode_text_to_vec(question_text)

                    if img_emb is not None:
                        all_embeddings.append(img_emb.tolist())
                        all_metadatas.append({
                            "type": "image", 
                            "path": img_path,
                            "associated_question": question_text
                        })
                        all_ids.append(f"img_{i}")
                    if txt_emb is not None:
                        all_embeddings.append(txt_emb.tolist())  
                        all_metadatas.append({
                            "type": "clip_text", 
                            "text": question_text,
                            "related_image": img_path
                        })
                        all_ids.append(f"txt_{i}")

                if len(all_embeddings)>0:
                    logger.info(f"Adding {len(all_embeddings)} total embeddings to collection")
                    self.image_collection.add(
                        embeddings=all_embeddings,
                        metadatas=all_metadatas,
                        ids=all_ids
                    )
            
        except Exception as e:
            logger.error(f"Error adding QA pairs: {str(e)}")
            raise

    def encode_image_to_vec(self, image_path: str) -> Optional[np.ndarray]:
            """
            ç”¨ CLIP image encoder å°‡åœ–ç‰‡è½‰ç‚º512ç¶­å‘é‡
            """
            try:
                # å…ˆåšåŸºç¤è™•ç†,ç¸®æ”¾æˆ–å¦å­˜
                processed_path = self.image_processor.process_and_save(
                    image_path, self.image_size
                )
                if not processed_path:
                    return None

                image = PILImage.open(processed_path)
                inputs = self.processor(images=[image], return_tensors="pt").to(self.siglip.device)
                with torch.no_grad():
                    embs = self.siglip.get_image_features(**inputs)
                return (embs / embs.norm(dim=-1, keepdim=True)).cpu().numpy()
            except Exception as e:
                print(f"Error in encode_image_to_vec: {str(e)}")
                return None

    def add_vectors(
        self,
        texts: Optional[List[str]] = None,
        metadatas: Optional[List[Dict]] = None,
        images: Optional[List[str]] = None,
        ):
        """
        çµ±ä¸€æŠŠæ–‡å­— / åœ–ç‰‡å¯«é€² clip_collection
        """
        texts     = texts or []
        images    = images or []
        metadatas = metadatas or []

        all_embs, all_metas, docs, all_ids = [], [], [], []
        idx = 0

        # -------------------- æ–‡å­— --------------------
        for i, txt in enumerate(texts):
            emb = self.encode_text_to_vec(txt)
            if emb is None:
                continue

            # â‘  å– metadata ä¸”ä¿è­‰æ˜¯ dict
            src_meta = metadatas[i] if i < len(metadatas) else {}
            if not isinstance(src_meta, dict):
                src_meta = {"note": str(src_meta)}

            # â‘¡ domain â†’ type æ˜ å°„ï¼ˆåªåšä¸€æ¬¡ï¼‰
            domain = src_meta.pop("domain", "").lower()
            if domain in {"é‡ç¸å­¸", "acupuncture"}:
                src_meta["type"] = "acupoint"
            elif domain in {"herb","herbology"}:
                src_meta["type"] = "herb"
            elif domain in {"ccd","canine"}:
                src_meta["type"] = "ccd"

            for vec in self.to_2d(emb):
                md = {
                    "type": src_meta.get("type", "professional"),
                    "content": txt,
                    **src_meta,            # å…¶é¤˜æ¬„ä½ä¿ç•™
                }
                all_embs.append(vec)
                all_metas.append(md)
                docs.append(txt)
                all_ids.append(f"text_{idx}")
                idx += 1

        # -------------------- åœ–ç‰‡ --------------------
        for j, img_name in enumerate(images):
            full_path = str(self.image_dir / img_name)
            emb = self.encode_image_to_vec(full_path)
            if emb is None:
                continue

            src_meta = metadatas[j] if j < len(metadatas) else {}
            if not isinstance(src_meta, dict):
                src_meta = {"note": str(src_meta)}

            # --- 2-1 åœ–ç‰‡å‘é‡ ---
            img_meta = {
                "type": "image",
                "path": img_name,
                **src_meta,
            }
            all_embs.append(self.to_2d(emb)[0])
            all_metas.append(img_meta)
            docs.append("")                         # åœ–ç‰‡æ²’æœ‰ document
            img_id = f"img_{uuid.uuid4().hex}"
            all_ids.append(img_id)

            # --- 2-2 caption å‘é‡ï¼ˆè‹¥æœ‰ï¼‰---
            cap = src_meta.get("caption") or src_meta.get("image_description")
            if cap:
                cap_emb = self.encode_text_to_vec(cap)
                if cap_emb is not None:
                    all_embs.append(self.to_2d(cap_emb)[0])
                    all_metas.append({
                        "type": "caption",          # æ–¹ä¾¿å‰ç«¯è¾¨è­˜
                        "ref_image": img_name,      # æ—¥å¾Œå¯èšåˆ
                        "content": cap,
                        **src_meta,
                    })
                    docs.append(cap)
                    all_ids.append(f"{img_id}_cap")

            # md = {
            #     "type": "image",
            #     "path": img_name,
            #     **src_meta,
            # }
            # for vec in self.to_2d(emb):
            #     all_embs.append(vec)
            #     all_metas.append(md)
            #     docs.append("")          # å ä½
            #     all_ids.append(f"img_{uuid.uuid4().hex}")#(f"img_{idx}")
            #     idx += 1

        # -------------------- å¯«å…¥ Chroma --------------------
        if all_embs:
            self.clip_collection.add(
                embeddings = all_embs,
                metadatas  = all_metas,
                documents  = docs,
                ids        = all_ids,
            )
            logger.info(f"Added {len(all_embs)} items to '{self.collection_name}'")


    def similarity_search(self, query: str, k=25) -> Dict:
        """
        å°queryåšCLIP text embeddingå¾Œ,åœ¨clip_collectionè£¡æ‰¾æœ€ç›¸ä¼¼çš„kç­†
        """
        try:
            emb = self.encode_text_to_vec(query)
            if emb is None:
                return {"metadatas":[],"documents":[],"distances":[]}
        
            results = self.clip_collection.query(
                    query_embeddings=[emb],
                    n_results=k,
                    include=["documents","metadatas","distances","embeddings"] #include=["distances", "metadatas", "documents"]
            ) 
            # ---------- â–Œå‹•æ…‹é™æ¬Š + re-rank ----------------
            q = query.lower()
            if re.search(r"(st|cv|gv|bl|pc)-\d{1,2}|ç©´ä½", q):
                weight = {"herb": 0.3, "ccd": 0.3}     # acupoint = 1.0
            elif any(w in q for w in ["æŸ´èƒ¡", "é»ƒèŠ©", "æ¸…ç†±"]):
                weight = {"acupoint": 0.3, "ccd": 0.3}
            elif any(w in q for w in ["èªçŸ¥", "nlrp3", "ç™¼ç‚"]):
                weight = {"herb": 0.3, "acupoint": 0.3}
            else:
                weight = {}

            metas = results["metadatas"][0]
            dists = results["distances"][0]
            docs  = results["documents"][0]

            scored = []
            for i, (m, d) in enumerate(zip(metas, dists)):
                w = weight.get(m.get("type", ""), 1.0)
                scored.append((d * w, i))          # è·é›¢æ„ˆå°æ„ˆå¥½
            scored.sort(key=lambda x: x[0])

            idxs = [i for _, i in scored][:k]       # å–å‰ k
            # idxs = list(range(len(metas)))[:k]
            
            for key in ["metadatas", "distances", "documents"]:
                results[key][0] = [results[key][0][i] for i in idxs]

                return results
        except Exception as e:
            print(f"Error in search: {str(e)}")
            return {"metadatas":[],"documents":[],"distances":[]}


# #### DataProcessor

# In[165]:


class DataProcessor:
    def __init__(self, embedding_processor: 'EmbeddingProcessor'):
        self.embedding_processor = embedding_processor
        
    def extract_social_posts(self, csv_path: str) -> Tuple[List[Dict], List[str]]:
        """å¤„ç† CSV å¹¶æå–é—®ç­”å¯¹å’Œå›¾ç‰‡"""
        logger.info(f"Processing CSV: {csv_path}")
        qa_pairs = []
        images = []
        
        df = pd.read_excel(csv_path)
        current_post = None
        current_responses = []
        current_images = []
        current_link = None
        
        for _, row in df.iterrows():
            # å¤„ç†æ–°çš„å¸–å­
            if pd.notna(row['post']):
                # ä¿å­˜å‰ä¸€ä¸ªé—®ç­”å¯¹
                if current_post is not None:
                    qa_pair = {
                        'question': current_post,
                        'answers': current_responses.copy(),
                        'images': current_images.copy(),
                        'metadata': {
                            'type': 'social',
                            'source': 'facebook',
                            'images': ','.join(current_images) if current_images else '',
                            'answer_count': len(current_responses),
                            'link': current_link if current_link else ''
                        }
                    }
                    if pd.notna(row.get('image_description')):
                        qa_pair['metadata']['image_desc'] = row['image_description']

                    qa_pairs.append(qa_pair)
                    if current_images:
                        images.extend(current_images)
                
                # åˆå§‹åŒ–æ–°çš„é—®ç­”å¯¹
                current_post = row['post']
                current_responses = []
                current_images = []
                current_link = row.get('link', '')
            
            # æ·»åŠ å›å¤
            if pd.notna(row.get('responses')):
                current_responses.append(row['responses'])
            
            # å¤„ç†å›¾ç‰‡
            if pd.notna(row.get('images')):
                img_path = row['images']
                current_images.append(img_path)
                logger.info(f"Found image: {img_path} for current post")
  
        
        # ä¿å­˜æœ€åä¸€ä¸ªé—®ç­”å¯¹
        if current_post is not None and len(current_responses) >= 3:
            qa_pair = {
                'question': current_post,
                'answers': current_responses,
                'images': current_images,
                'metadata': {
                    'type': 'social_qa',
                    'source': 'facebook',
                    'images': ','.join(current_images) if current_images else '',
                    'answer_count': len(current_responses),
                    'link': current_link if current_link else ''
                }
            }
            if pd.notna(row.get('image_description')):
                    qa_pair['metadata']['image_desc'] = row['image_description']

            qa_pairs.append(qa_pair)
            if current_images:
                images.extend(current_images)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
        for i, qa in enumerate(qa_pairs):
            logger.info(f"\nQA Pair {i+1}:")
            logger.info(f"Question: {qa['question'][:100]}...")
            logger.info(f"Number of answers: {len(qa['answers'])}")
            logger.info(f"Images: {qa['images']}")
            logger.info(f"Link: {qa.get('link', 'No link')}")
        
        return qa_pairs, images

    
    def chunk_text(self,paragraph: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
        """
        å°‡çµ¦å®šæ®µè½ï¼Œä»¥ chunk_size å­—ç¬¦ç‚ºä¸Šé™é€²è¡Œåˆ‡åˆ†ï¼Œä¸¦ä¸”åœ¨ chunk ä¹‹é–“ä¿ç•™ overlap å€‹å­—çš„é‡ç–Šï¼Œ
        ä»¥å…ä¸Šä¸‹æ–‡æ–·è£‚ã€‚
        å‚™è¨»: 
        - é€™è£¡ä»¥ã€Œå­—ç¬¦ã€ç‚ºå–®ä½ï¼Œé©åˆä¸­æ–‡ï¼›è‹±æ–‡ä¹Ÿå¯ç”¨ï¼Œä½†è‹¥æƒ³ç²¾ç¢ºå°è‹±æ–‡ tokens å¯æ”¹æ›´å…ˆé€²æ–¹æ³•ã€‚
        """
        chunks = []
        start = 0
        length = len(paragraph)

        # å»æ‰å‰å¾Œå¤šé¤˜ç©ºç™½
        paragraph = paragraph.strip()

        while start < length:
            end = start + chunk_size
            # å– substring
            chunk = paragraph[start:end]
            chunks.append(chunk)
            # ç§»å‹•æŒ‡æ¨™(ä¸‹ä¸€å€‹ chunk)
            # overlap é é˜²æ–·å¥å¤±å»ä¸Šä¸‹æ–‡
            start += (chunk_size - overlap)

        return chunks
  

    def process_pdf(self, pdf_path: str,row_type: str) -> List[Dict]:
        logger.info(f"Processing PDF: {pdf_path}")
        professional_qa_pairs = []
        pdf_name = Path(pdf_path).name  # è·å–æ–‡ä»¶å
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                is_formula = self.detect_domain(pdf_name) == "ä¸­é†«æ–¹åŠ‘"
                is_acu = self.detect_domain(pdf_name) == "é‡ç¸å­¸"


                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    print(f"Page {page_num+1} raw text:", repr(text))
                    if is_formula:
                        paragraphs = self.split_formula_blocks(text)
                    elif is_acu:
                        paragraphs = self.split_acu_blocks(text)
                    else:
                        paragraphs = text.split('\n\n')
                    
                    
                    # è™•ç†æ¯å€‹æ®µè½
                    for para in paragraphs:
                        # logger.info(f"Para type: {type(para)}")

                        para_chunks = self.chunk_text(para)
                        # logger.info(f"Got {len(para_chunks)} chunks from chunk_text()")

                        for c in para_chunks:
                            qa_pair = {
                                'question': c[:50] + "...",  
                                'answers': [c],
                                'metadata': {
                                    'type': row_type,
                                    'source_file': pdf_name,  # æ·»åŠ æ–‡ä»¶å
                                    'page': str(page_num + 1),
                                    'content_length': str(len(c))
                                } #'domain':self.detect_domain(pdf_name),
                            }
                            professional_qa_pairs.append(qa_pair)
                
                logger.info(f"Extracted {len(professional_qa_pairs)} paragraphs from {pdf_name}")
                return professional_qa_pairs
                
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_name}: {str(e)}")
            return []
        
    def detect_domain(self, pdf_name: str) -> str:
        lower = pdf_name.lower()

        if "é‡ç¸" in pdf_name or "acupuncture" in lower:
            return "é‡ç¸å­¸"
        if "herbal" in lower or "herbology" in lower or "æ–¹åŠ‘" in pdf_name:
            return "ä¸­é†«æ–¹åŠ‘"
        return "å…¶ä»–"
    
    def split_formula_blocks(self,text: str) -> list[str]:
        """
        ç”¨æ­£å‰‡æŠ“å‡ºã€â— å…­å‘³åœ°é»ƒä¸¸ã€æˆ–ã€Liu Wei Di Huang Wanã€é–‹é ­ï¼Œ
        æ¯é‡ä¸‹ä¸€å€‹æ–¹åå°±çµæŸä¸Šä¸€å¡Š
        """
        pattern = re.compile(r"(?:â—|\s)([\u4e00-\u9fffA-Za-z\- ]{3,40}(?:æ¹¯|ä¸¸|é£²|æ•£|è†))")
        blocks = []
        cur_block = []
        for line in text.splitlines():
            if pattern.search(line):
                # é‡åˆ°ä¸‹ä¸€å¸–è—¥ â†’ å…ˆæ”¶å‰ä¸€å¸–
                if cur_block:
                    blocks.append("\n".join(cur_block).strip())
                    cur_block = []
            cur_block.append(line)
        if cur_block:
            blocks.append("\n".join(cur_block).strip())
        return [b for b in blocks if len(b) > 60]    

    def split_acu_blocks(self,text: str) -> list[str]:
        # ç¯„ä¾‹ä»£ç¢¼ï¼šLIâ€‘11ã€HT-7ã€SI 3
        pattern = re.compile(r"\b([A-Z]{1,2}[ -â€‘]\d{1,3})\b")
        blocks, cur = [], []
        for line in text.splitlines():
            if pattern.search(line):
                if cur: blocks.append("\n".join(cur).strip()); cur = []
            cur.append(line)
        if cur: blocks.append("\n".join(cur).strip())
        return [b for b in blocks if len(b) > 40]

    def process_all(self, csv_path: str, pdf_paths: List[str]):
        """ç¶œåˆè™•ç†ç¤¾ç¾¤ CSV + PDFs"""
        try:
            social_qa_pairs, images = [], []  
            # 1. å¤„ç†ç¤¾ç¾¤æ•°æ®
            if csv_path: 
                social_qa_pairs, images = self.extract_social_posts(csv_path)
                logger.info(f"\nProcessed social data:")
                logger.info(f"- Social QA pairs: {len(social_qa_pairs)}")
                logger.info(f"- Images found: {len(images)}")
            else:
                logger.info("Skip social CSV, onlyè™•ç† PDFs")
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
            valid_images = []
            for img in images:
                img_path = Path(self.embedding_processor.image_dir) / img
                if img_path.exists():
                    valid_images.append(img)
            
            # 2. å¤„ç†æ‰€æœ‰ PDF
            all_professional_pairs = []
            for pdf_path in pdf_paths:
                pdf_name = pdf_path.name.lower()
                for t, keys in TYPE_MAP.items():
                    if any(k in pdf_name for k in keys):
                        row_type = t; break
                else:
                    row_type = "professional"
                pdf_qa_pairs = self.process_pdf(pdf_path, row_type=row_type)
                #pdf_qa_pairs = self.process_pdf(pdf_path)
                all_professional_pairs.extend(pdf_qa_pairs)
                logger.info(f"\nProcessed {Path(pdf_path).name}:")
                logger.info(f"- Extracted paragraphs: {len(pdf_qa_pairs)}")
            
            # 3. åˆå¹¶ => all_qa_pairs
            all_qa_pairs = social_qa_pairs + all_professional_pairs
            
            # 4. æº–å‚™ texts + metadatas => ä½ å°±èƒ½ä¸€æ¬¡æˆ–å¤šæ¬¡å‘¼å« add_vectors
            questions = []
            answers = []
            question_metas = []
            answer_metas = []
            
            # å¤„ç†æ‰€æœ‰é—®ç­”å¯¹
            for qa_pair in all_qa_pairs:
                # question 
                questions.append(qa_pair['question'])
                question_metas.append(qa_pair['metadata'])
                
                # answers
                for ans_text in qa_pair['answers']:
                    answers.append(ans_text)
                    am = qa_pair['metadata'].copy()
                    am['parent_question'] = qa_pair['question']
                    answer_metas.append(am)

            # ------------- é€™è£¡æ‰é–‹å§‹çµ„ professional texts / metas -------------
            prof_texts = [qa["answers"][0] for qa in all_professional_pairs]
            prof_metas = [qa["metadata"]   for qa in all_professional_pairs]

            
            # è¾“å‡ºå¤„ç†ç»“æœ
            logger.info(f"\nFinal processing summary:")
            logger.info(f"- Total questions: {len(questions)}")
            logger.info(f"- Total answers: {len(answers)}")
            logger.info(f"- Valid images: {len(valid_images)}")
            logger.info(f"- Social content: {len(social_qa_pairs)} QA pairs")
            logger.info(f"- Professional content: {len(all_professional_pairs)} paragraphs")
            


            # --------- ğŸ”§ æŠŠ 3 çµ„ metadata éƒ½ä¿è­‰æ˜¯ dict (æ”¾åœ¨æ­¤è™•) ---------
            question_metas = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in question_metas]
            prof_metas     = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in prof_metas]
            # è‹¥è¦ç”¨ answer_metas ä¹Ÿä¸€ä½µè™•ç†
            answer_metas   = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in answer_metas]


            self.embedding_processor.add_vectors(texts=prof_texts,
                                            metadatas=prof_metas)
            
            # (A) å…ˆåŠ æ‰€æœ‰ question
            self.embedding_processor.add_vectors(
                texts = questions,
                metadatas = question_metas
            )

            if valid_images:
                meta_for_imgs = []
                for img_name in valid_images:
                    meta_for_imgs.append({
                        "type":"image",
                        "source":"facebook",
                        "filename": img_name
                    })

                self.embedding_processor.add_vectors(
                    images=valid_images,
                    metadatas=meta_for_imgs
                )

            logger.info("All data added to clip_collection.")
            return len(questions), len(valid_images) #return questions, question_metas, all_professional_pairs, valid_images
                
        except Exception as e:
            logger.error(f"Error processing documents: {str(e)}")
            raise


# #### QA System

# In[ ]:


from deep_translator import GoogleTranslator
class QASystem:
    def __init__(self, embedding_processor: 'EmbeddingProcessor',
                 model_name: str = 'llama3.2-vision'):
        self.embedding_processor = embedding_processor
        self.model_name = model_name
        logger.info(f"Initialized QA System with Ollama model: {model_name}")

    def _classify_collection_results(self, raw_result) -> Dict:
        """
        å°‡ clip_collection çš„æª¢ç´¢çµæœ (metadatas/documents...) 
        è½‰æ›æˆ { 'social': {...}, 'professional': {...}, 'images': {...} } 
        çš„çµæ§‹ï¼Œä¾¿æ–¼å¾ŒçºŒ gather_references / format_contextã€‚
        """
        # é è¨­ç©ºçµæ§‹
        structured = {
            "social": {
                "metadata": [],
                "link": [],
                "content": [],
                "documents":[]
            },
            "professional": {
                "metadata": [],
                "content": [],
                "documents":[]
            },
            "images": {
                "metadata": [],
                "paths": [],
                "relevance":[]
            },
            
        }

        # raw_result["metadatas"] æ˜¯å€‹ 2D list => [ [meta0, meta1, ...] ]
        if raw_result.get("metadatas"):
            meta_list = raw_result["metadatas"][0]  # å› ç‚ºåªæœ‰1å€‹ query
            dist_list = raw_result["distances"][0] if raw_result.get("distances") else []
            doc_list = raw_result["documents"][0]
            ids_list  = raw_result["ids"][0] if raw_result.get("ids") else []
            # documents_list = raw_result["documents"][0]

            for i, meta in enumerate(meta_list):
                dist = dist_list[i] if i < len(dist_list) else None
                doc_id = ids_list[i] if i < len(ids_list) else ""
                doc_text = doc_list[i] if i < len(doc_list) else ""

                src_type = meta.get("type","")

                if src_type == "social":
                    structured["social"]["metadata"].append(meta)
                    structured["social"]["documents"].append(doc_text)
                elif src_type in ("acupoint", "herb", "ccd", "professional"):
                    structured["professional"]["metadata"].append(meta)
                    structured["professional"]["documents"].append(doc_text)
                elif src_type ("image","images","herb_img"): #== "image":
                    structured["images"]["metadata"].append(meta)
                    # æ”¾ path
                    path = meta.get("path","")
                    structured["images"]["paths"].append(path)
                    structured["images"]["relevance"].append(dist)

                else:
                    # å°‡æœªçŸ¥ type å…¨ä¸Ÿ professionalï¼Œæˆ–ä¾éœ€æ±‚æ”¹ social
                    meta.setdefault("type", "professional")
                    structured["professional"]["metadata"].append(meta)
                    structured["professional"]["documents"].append(doc_text)


        return structured


    def determine_question_type(self,query: str) -> str:
        """
        å›å‚³: "multiple_choice" | "true_false" | "qa"
        æ”¯æ´ä¸­è‹±æ–‡ & å„ç¨®æ¨™é»
        """
        q = query.strip().lower()

        # --- Multipleâ€‘choice --------------------------------------------------
        # 1) è¡Œé¦–æˆ–æ›è¡Œå¾Œå‡ºç¾  Aï½D / å…¨å½¢ï¼¡ï½ï¼¤ / ã€Œç­”ã€ï¼Œ
        #    å¾Œé¢æ¥ã€€. ï¼ : ï¼š ã€)
        mc_pattern = re.compile(r'(?:^|\n)\s*(?:[a-dï½-ï½„]|ç­”)[:\.ï¼:ï¼šã€\)]', re.I)
        # 2) or å¥å­å¸¶ "which of the following"
        mc_keywords_en = ["which of the following", "which one of the following",
                        "which option", "choose one of"]

        if mc_pattern.search(query) or any(kw in q for kw in mc_keywords_en):
            return "multiple_choice"

        # --- True / False -----------------------------------------------------
        tf_keywords_zh = ["æ˜¯å¦", "æ˜¯å—", "å°å—", "æ­£ç¢ºå—"]
        tf_keywords_en = ['true or false', 'is it', 'is this', 'is that', 
             'is it possible', 'correct or not']

        if any(k in q for k in tf_keywords_zh + tf_keywords_en):
            return "true_false"

        # --- Default ----------------------------------------------------------
        return "qa"

    
    def gather_references(self, search_results: Dict) -> str:
        """
        å¾ search_results ä¸­æ“·å– PDF æª”å/ç¤¾ç¾¤é€£çµï¼Œä¸¦çµ„æˆä¸€å€‹å­—ä¸²
        """
        if not isinstance(search_results, dict):
            logger.error("search_results æ ¼å¼éŒ¯èª¤: %s", type(search_results))
            return ""

        references = []

        # è™•ç† social
        for meta in search_results["social"].get("metadata", []):
            if meta.get("type") == "social_qa" and "link" in meta:
                references.append(f"(ç¶“é©—) {meta['link']}")

        # è™•ç† professional
        for meta in search_results["professional"].get("metadata", []):
            if meta.get("type") in ["pdf", "professional"]:
                pdf_name = meta.get("source_file", "unknown.pdf")
                references.append(f"(æ–‡ç») {pdf_name}")

        # å»é‡
        unique_refs = list(set(references))
        return "\n".join(unique_refs)


    def build_user_prompt(
        self,
        query: str,
        context: str,
        references_str: str = ""
        ) -> str:
        # ä¸å«ä»»ä½•æ ¼å¼è¦ç¯„ï¼åªçµ¦é¡Œç›®èˆ‡è³‡æ–™
        return (
            f"""
            ã€åƒè€ƒè³‡æ–™ã€‘
            {context}\n
            ã€è³‡æ–™ä¾†æºã€‘
            {references_str}\n    
            ã€å•é¡Œã€‘
            {query}\n
            """
            # "åƒè€ƒè³‡æ–™ï¼š\n" + context +
            # "\nä¾†æºï¼š\n" + references_str +
            # "å•é¡Œï¼š"ï¼‹query
        )


    def merge_adjacent(self, metas, docs, k_keep: int = 5) -> str:
        """
        å°‡åŒæª”åŒé ä¸” _id é€£è™Ÿçš„ç‰‡æ®µåˆä½µï¼Œå›å‚³å‰ k_keep æ®µæ–‡å­—ã€‚
        åƒæ•¸
        ----
        metas : list[dict]    # raw_result["metadatas"][0]
        docs  : list[str]     # raw_result["documents"][0]
        """
        ID_NUM_RE = re.compile(r"_(\d+)$")   # å°¾ç¢¼å–æ•¸å­—ï¼štext_123 â†’ 123
        merged, buf = [], ""
        last_src, last_idx = ("", ""), -999

        for md, doc in zip(metas, docs):
            src_key = (md.get("source_file", ""), md.get("page", ""))

            # å– _id å°¾ç¢¼ï¼›è‹¥ä¸å­˜åœ¨å‰‡è¨­ -1
            _id = md.get("_id", "")
            m = ID_NUM_RE.search(_id)
            cur_idx = int(m.group(1)) if m else -1

            # åŒæª”åŒé ä¸”é€£è™Ÿ â†’ è¦–ç‚ºç›¸é„°
            if src_key == last_src and cur_idx == last_idx + 1:
                buf += doc
            else:
                if buf:
                    merged.append(buf)
                buf = doc
            last_src, last_idx = src_key, cur_idx

        if buf:
            merged.append(buf)

        return "\n\n".join(merged[:k_keep])


    def generate_response(self, query: str,question_type: Optional[str] = None) -> Tuple[str, List[str]]:
        try:
            raw_result = self.embedding_processor.similarity_search(
                query,
                k=25)  
            print(raw_result["metadatas"])
            
            if not raw_result["documents"] or len(raw_result["documents"][0]) == 0:
                logger.warning("No hits for query â†’ æ”¹ç”¨ k=50 å†è©¦ä¸€æ¬¡")
                raw_result = self.embedding_processor.similarity_search(query, k=50)

            if not raw_result["documents"] or len(raw_result["documents"][0]) == 0:
                return "[NoRef] ç„¡è¶³å¤ è­‰æ“šåˆ¤æ–·", [],[]
            
            # ç”¨å¾Œè™•ç†
            search_results = self._classify_collection_results(raw_result)
            logger.info("SEARCH RESULT(structured): %s",search_results)

            context = self.merge_adjacent(raw_result["metadatas"][0],
                              raw_result["documents"][0])[:1500]

            context = context[:1500]          # æœ€å¤š 1500 å­—

            references_str = self.gather_references(search_results)
            # linkæ‡‰è©²ç”¨å‚³åƒæ•¸çš„æœƒæˆåŠŸ å¯èƒ½ç”¨context.linkä¹‹é¡çš„æŠ“é¡Œç›®çš„reference

            # --- â‘  é¡Œå‹ --------------------------------------------------------
            q_type = question_type or self.determine_question_type(query)

            user_prompt = self.build_user_prompt(
                query=query,
                context=context[:1500],
                references_str=references_str
            )
            # ---------- â‘¡ æ ¹æ“šé¡Œå‹å‹•æ…‹çµ„ system æŒ‡ä»¤ ----------
            if q_type == "multiple_choice":
                format_rules = (
                    "é€™æ˜¯ä¸€é¡Œé¸æ“‡é¡Œï¼Œå›ç­”æ ¼å¼å¦‚ä¸‹ï¼š\n"
                    "å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\n"
                    "ç”¨ 2-3 å¥è©±èªªæ˜ç†ç”±ã€‚\n"
                    "æœ€å¾Œå†çµ¦å‡ºç­”æ¡ˆï¼Œåªèƒ½å›ç­” A/B/C/D (è«‹å‹¿å¸¶ä»»ä½•æ¨™é»ã€æ–‡å­—ã€ä¹Ÿä¸è¦åªå›ç­”é¸é …å…§å®¹)\n"
                    "è‹¥åŒæ™‚å‡ºç¾å¤šå€‹é¸é …ï¼Œè«‹åªé¸ä¸€å€‹æœ€é©åˆçš„\n"
                )
            elif q_type == "true_false":
                format_rules = (
                    "é€™æ˜¯ä¸€é¡Œæ˜¯éé¡Œï¼Œè«‹æŒ‰ç…§ä¸‹åˆ—æ ¼å¼å›ç­”ï¼š\n"
                    "å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\n"
                    "æœ€å¾Œå†çµ¦å‡ºç­”æ¡ˆï¼Œåªèƒ½å¯«ã€ŒTrueã€æˆ–ã€ŒFalseã€\n"
                )
            else:   # qa
                format_rules = (
                    "è«‹ä¾ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n"
                    "é‡å°å•é¡Œæä¾›å…·é«”ç­”æ¡ˆä¸¦ç”¨2-3å¥è©±èªªæ˜ \n"
                )

            #"æ‚¨æ˜¯ä¸€åå°ˆæ¥­ç¸é†«ï¼Œ1.æ“…é•·çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ç¶œåˆç—‡ï¼ˆCCDï¼‰çš„è¨ºæ–·å’Œè­·ç† 2.æ“æœ‰è±å¯Œçš„å¯µç‰©ä¸­é†«çŸ¥è­˜ 3.å¸¸è¦‹å•é¡Œè¨ºæ–·åŠæ”¹å–„å»ºè­°\n" "è«‹åœ¨ç­”æ¡ˆæœ€å¾Œé¡¯ç¤ºä½ åƒè€ƒçš„ä¾†æºé€£çµæˆ–è«–æ–‡åç¨±ï¼Œå¦‚æœä¾†æºä¸­åŒ…å«ã€Œ(ç¶“é©—) some_linkã€ï¼Œè«‹åœ¨å›ç­”ä¸­ä»¥ [Experience: some_link] å½¢å¼æ¨™ç¤ºï¼›è‹¥åŒ…å«ã€Œ(æ–‡ç») some.pdfã€ï¼Œå°± [reference: some.pdf]\n""å¦‚æª¢ç´¢çµæœä»ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹ä»¥[NoRef]æ¨™ç¤ºä¸¦æ ¹æ“šä½ çš„å¸¸è­˜å›ç­”ã€‚\n"
            system_prompt = (
                """ä½ æ˜¯è³‡æ·±ç¸é†«ï¼Œæ“…é•·çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ç¶œåˆç—‡ï¼ˆCCDï¼‰çš„è¨ºæ–·å’Œè­·ç†ä¸¦æ“æœ‰è±å¯Œçš„å¯µç‰©ä¸­é†«çŸ¥è­˜ï¼Œå¿…é ˆéµå®ˆä»¥ä¸‹è¦å‰‡å›ç­”å•é¡Œï¼š
                    1. åªèƒ½æ ¹æ“šã€æª¢ç´¢çµæœã€‘å…§å®¹ä½œç­”ï¼›è‹¥è³‡è¨Šä¸è¶³ï¼Œè«‹å›ç­”ã€Œè³‡æ–™ä¸è¶³ã€ï¼Œä¸¦ä¾ç…§ä½ çš„å¸¸è­˜å›ç­”ã€‚
                    2. è‹¥éœ€è£œå……ä¸€èˆ¬è‡¨åºŠå¸¸è­˜ï¼Œè«‹å°‡è©²å¥æ”¾åœ¨æ®µè½æœ€å¾Œä¸¦æ¨™è¨»ï¼»å¸¸è­˜ï¼½ã€‚
                    3. æ¯ä¸€å¥çµå°¾å¿…é ˆæ¨™è¨»å¼•ç”¨ä¾†æºç·¨è™Ÿï¼Œå¦‚ï¼»1ï¼½æˆ–ï¼»1,3ï¼½ã€‚
                    4. ä¸¦åœ¨æœ€å¾Œé¢æ•´ç†åˆ—å‡ºæ¯å€‹ç·¨è™Ÿçš„source_fileï¼Œå¦‚[1] ...pdf æˆ– [2] Chinese Veterinary Materia Medica """
                + format_rules
            )
        

            # è™•ç†åœ–ç‰‡
            image_paths = []
            # 2) æŠŠåœ–ç‰‡æ’ˆå‡º
            # -- (a) å…ˆæƒ retrieved_contexts --
            for meta in search_results.get("images", {}).get("metadata", []):
                if meta.get("type") in ("image", "herb_img") and meta.get("path"):
                    full_path = self.embedding_processor.image_dir / meta["path"]
                    if full_path.exists():
                        image_paths.append(str(full_path.resolve()))

            # -- (b) ä¿ç•™èˆŠçš„ social.images è¦å‰‡ (è‹¥é‚„éœ€è¦) --
            for md in search_results.get("social", {}).get("metadata", []):
                if md.get("images"):
                    for img_name in md["images"].split(","):
                        full_path = self.embedding_processor.image_dir / img_name.strip()
                        if full_path.exists():
                            image_paths.append(str(full_path.resolve()))
            # 3) OLlama åªå…è¨±ä¸€å¼µ, ä½ å¯å– image_paths[:1] => message["images"] = ...
            if image_paths:
                print("We found images: ", image_paths)
            else:
                logger.info("No images to display")
            
            

            message = [
                {"role": "system", "content": system_prompt},
                {"role": "user",   "content": user_prompt},
                # {'images': image_paths[:1]}
            ]
            # print("=======sys prompt =======",system_prompt)
            # print("=======user prompt =======",user_prompt)
            # ç”Ÿæˆå“åº”
            response = ollama.chat(
                model=self.model_name,
                messages=message
            )


            # å–å¾—æª¢ç´¢æ®µè½ï¼ˆæ–‡å­—å³å¯ï¼‰
            retrieved_contexts = search_results["professional"]["documents"] + \
                                search_results["social"]["documents"]

            # æŠŠä¸‰æ¨£éƒ½å›å‚³ ----------------------------------â–¼ æ–°å¢
            response_text = response["message"]["content"]

            return response_text, retrieved_contexts, image_paths #response['message']['content'], image_paths

        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            # return f"å‡ºç¾å•é¡Œï¼Œæª¢æŸ¥ollamaé€£ç·šæˆ–æ˜¯generate_response", []
            raise

    def format_context(self, search_results: Dict) -> str:
        """Format context from search results"""
        try:
            context = ""

            # 1) è™•ç†ç¤¾ç¾¤è¨è«–
            social_metas = search_results["social"].get("metadata", [])
            social_links = search_results["social"].get("link", [])
            social_docs = search_results["social"].get("documents", [])
            social_content = search_results["social"].get("content", [])

            if social_metas or social_links or social_docs:
                context += "\n[ç¤¾ç¾¤è¨è«–]\n"
                # é€™è£¡ç¤ºç¯„æŠŠ linkã€documents éƒ½è¼¸å‡º
                for i, meta in enumerate(social_metas):
                    link_str = meta.get("link", "")
                    doc_text = social_docs[i] if i < len(social_docs) else ""
                    context += f"ã€Linkã€‘{link_str}\n" if link_str else ""
                    # doc_text å°±æ˜¯æª¢ç´¢å›ä¾†çš„ chunk
                    context += f"ã€è¨è«–ç‰‡æ®µã€‘{doc_text}\n\n"

            # 2) è™•ç†å°ˆæ¥­æ–‡ç»
            prof_metas = search_results["professional"].get("metadata", [])
            prof_docs = search_results["professional"].get("documents", [])

            if prof_metas or prof_docs:
                context += "\n[å°ˆæ¥­æ–‡ç»]\n"
                for j, meta in enumerate(prof_metas):
                    source_file = meta.get("source_file", "")
                    doc_text = prof_docs[j] if j < len(prof_docs) else ""
                    # å¦‚æœæ‚¨æœ‰å¦å¤–å­˜æ”¾é ç¢¼ page = meta.get("page"), ä¹Ÿå¯åˆ—å‡º
                    page_num = meta.get("page", "")
                    context += f"ã€æ–‡ä»¶ç‰‡æ®µã€‘{doc_text}\n"
                    if source_file:
                        context += f"(æª”æ¡ˆ: {source_file}"
                        context += f", é : {page_num})" if page_num else ")"
                    context += "\n\n"

            # åµéŒ¯ç”¨ (å¯ä¿ç•™ä¹Ÿå¯ç§»é™¤)
            print("social metadata:", social_metas)
            print("social links:", social_links)
            print("professional metadata:", prof_metas)

            return context if context.strip() else "åƒè€ƒè³‡æ–™ç„¡æ³•å–å¾—"

        except Exception as e:
            logger.error(f"Error formatting context: {str(e)}")
            return "Unable to retrieve reference materials"


    def display_response(self, query: str,question_type: Optional[str] = None):
            """Display response with text and images"""
            try:
                logger.info("Starting to generate response...")
                try:
                    response_text, _ , image_paths = self.generate_response(query,question_type)
                except Exception as e:
                    response_text = f"[ERROR] {e}"
                    image_paths = []
                
                print("Question:", query)
                print("\nSystem Response:")
                print(response_text)
                print("\n" + "-"*50 + "\n")

                if image_paths:
                    print("\nRelated Image:")
                    img_path = image_paths[0]  # We now only have one image
                    try:
                        img = PILImage.open(img_path)
                        display(IPyImage(filename=img_path))
                    except Exception as e:
                        logger.error(f"Error displaying image {img_path}: {str(e)}")
                else:
                    logger.info("No images to display")


                return response_text, image_paths # add for response 0406
                    
            except Exception as e:
                logger.error(f"Error in display_response: {str(e)}", exc_info=True)  
                return "", [] 


# ### é¡Œç›®æ¸¬è©¦

# ##### å–®ä¸€é¡Œç›®æ¸¬è©¦

# In[ ]:


# æ¸¬è©¦æŸ¥è©¢
test_queries = [
# "CCD æ˜¯å¦èˆ‡ç¥ç¶“ç™¼ç‚ç›¸é—œï¼Ÿæœ‰ç„¡ç‰¹å®šç´°èƒå› å­ï¼ˆcytokinesï¼‰æˆ–ç™¼ç‚è·¯å¾‘ï¼ˆä¾‹å¦‚NLRP3 inflammasomeï¼‰åƒèˆ‡ï¼Ÿ",
# "CCD æ˜¯å¦èˆ‡è…¸é“å¾®ç”Ÿç‰©ç¾¤è®ŠåŒ–æœ‰é—œï¼Ÿæ˜¯å¦æœ‰ç‰¹å®šç´°èŒç¾¤è½æœƒå½±éŸ¿å¤§è…¦å¥åº·ï¼Ÿ",
# " å¤±æ™ºçŠ¬çš„æ¾æœé«”æ˜¯å¦é€€åŒ–",
# " æœ‰åˆ»æ¿å½¢ç‚ºçš„çŠ¬éš»æ˜¯å¦æœƒå¢åŠ CCDé¢¨éšªï¼Ÿ",
# " å¤±æ™ºçŠ¬åˆ†æ³Œè¤ªé»‘æ¿€ç´ çš„èƒ½åŠ›æ˜¯å¦é€€åŒ–ï¼Ÿ",
# " çš®è³ªé¡å›ºé†‡cortisolæˆ–æ‡‰æ¿€è·çˆ¾è’™stress hormonesæ˜¯å¦å¯ä½œç‚º CCD çš„æ½›åœ¨è¨ºæ–·æŒ‡æ¨™ï¼Ÿ",
# " å¦‚ä½•å€åˆ†æ­£å¸¸è€åŒ–èˆ‡CCDçš„æ—©æœŸå¾µå…†ï¼Ÿ ",
# " B ç¾¤ç¶­ç”Ÿç´ æ˜¯å¦èƒ½é™ä½ CCD é€²å±•é¢¨éšªï¼Ÿ",
# " é£Ÿç”¨GABAæ˜¯å¦å°æ–¼é é˜²CCDæœ‰æ•ˆï¼Ÿ",
# " è­¦çŠ¬ã€æ•‘é›£çŠ¬ç­‰å·¥ä½œçŠ¬åœ¨ç½¹æ‚£CCDçš„æ©Ÿç‡æ¯”è¼ƒå®¶åº­é™ªä¼´çŠ¬",
# " ç›®å‰æ˜¯å¦æœ‰å½±åƒå­¸æª¢æ¸¬å¯ä»¥æº–ç¢ºå€åˆ† CCD èˆ‡å…¶ä»–ç¥ç¶“é€€è¡Œæ€§ç–¾ç—…ï¼Ÿ",
# " å¦‚æœCCDé€²å±•åˆ°æœ€å¾Œéšæ®µï¼Œå“ªäº›ç—‡ç‹€æœ€éœ€è¦é—œæ³¨ï¼Ÿå¦‚ä½•å¹³è¡¡ç‹—ç‹—çš„ç”Ÿæ´»è³ªé‡èˆ‡ç–¼ç—›ç®¡ç†ï¼Œä¸¦ä¸”æ±ºå®šç‹—ç‹—æœªä¾†çš„æ–¹å‘",

# "æ ¹æ“šè³‡æ–™ä¸­å°çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ï¼ˆCCDï¼‰ç¥ç¶“ç™¼ç‚æ©Ÿåˆ¶çš„æ¢è¨ï¼ŒNLRP3ç‚ç—‡å°é«”åœ¨åˆ†å­å±¤é¢ä¸Šå¦‚ä½•åƒèˆ‡CCDé€²ç¨‹ï¼Ÿè©²éç¨‹æ¶‰åŠå“ªäº›é—œéµç´°èƒå› å­èˆ‡èª¿æ§æ©Ÿåˆ¶ï¼Ÿ",
# "è³‡æ–™æåˆ°è…¸é“å¾®ç”Ÿç‰©ç¾¤èˆ‡CCDä¹‹é–“å¯èƒ½å­˜åœ¨è¯ç¹«ï¼Œè«‹å•æ–‡ä¸­å¦‚ä½•é—¡è¿°è…¸é“èŒç¾¤å¤±è¡¡å½±éŸ¿ç¥ç¶“å‚³å°èˆ‡å…ç–«åæ‡‰çš„åˆ†å­æ©Ÿåˆ¶ï¼Ÿå“ªäº›ç‰¹å®šç´°èŒç¾¤è½çš„è®ŠåŒ–è¢«èªç‚ºèˆ‡CCDé€²å±•ç›¸é—œï¼Ÿ",
# "åœ¨æ¢è¨CCDçš„è¨ºæ–·ç­–ç•¥ä¸­ï¼Œè©²è³‡æ–™å°æ–¼åˆ©ç”¨å½±åƒå­¸æŠ€è¡“ï¼ˆå¦‚MRIèˆ‡CTï¼‰å€åˆ†CCDèˆ‡å…¶ä»–ç¥ç¶“é€€è¡Œæ€§ç–¾ç—…çš„æ‡‰ç”¨æå‡ºäº†å“ªäº›è¦‹è§£ï¼Ÿé€™äº›æŠ€è¡“çš„å„ªå‹¢èˆ‡å±€é™æ€§åˆ†åˆ¥æ˜¯ä»€éº¼ï¼Ÿ",
# "è³‡æ–™ä¸­å°å¤±æ™ºçŠ¬æ¾æœé«”é€€åŒ–èˆ‡è¤ªé»‘æ¿€ç´ åˆ†æ³Œæ¸›å°‘ä¹‹é–“çš„é—œè¯æœ‰è©³ç´°è«–è¿°ï¼Œè«‹å•è©²ç ”ç©¶å¦‚ä½•æè¿°é€™ä¸€ç”Ÿç†è®ŠåŒ–çš„åˆ†å­æ©Ÿåˆ¶ä»¥åŠå…¶å°çŠ¬éš»ç¡çœ -è¦ºé†’é€±æœŸçš„å½±éŸ¿ï¼Ÿ",
# "é‡å°CCDçš„æ²»ç™‚ç­–ç•¥ï¼Œè³‡æ–™ä¸­æå‡ºäº†å“ªäº›åŸºæ–¼åˆ†å­æ©Ÿåˆ¶çš„æ²»ç™‚æ–¹æ³•ï¼Ÿè«‹åˆ†æé€™äº›æ–¹æ³•åœ¨è‡¨åºŠæ‡‰ç”¨ä¸Šçš„ç¾ç‹€ã€æ½›åœ¨å„ªå‹¢åŠæœªä¾†ç ”ç©¶ä¸­äºŸå¾…è§£æ±ºçš„æŒ‘æˆ°ã€‚",

# "å“ªç¨®çŠ¬å®¹æ˜“å¤±æ™ºï¼Ÿ",
# "å¤§ä¸­å°å‹ç‹—çš„å¤±æ™ºç…§é¡§æ–¹å¼æœ‰ä»€éº¼ä¸åŒï¼Ÿ"
# "æˆ‘çš„ç‹—ç‹—æœ‰å¤±æ™ºç—‡ï¼Œæ™šä¸Šç¸½æ˜¯ç¹åœˆåœˆè€Œä¸”å«å€‹ä¸åœï¼Œæœ‰ä»€éº¼æ–¹æ³•èƒ½å¹«åŠ©ç‰ å®‰éœä¸‹ä¾†ç¡è¦ºå—ï¼Ÿæœ‰äººæ¨è–¦éè¤ªé»‘æ¿€ç´ ï¼Œé€™çœŸçš„æœ‰æ•ˆå—ï¼Ÿ",
# "æˆ‘çš„è€ç‹—æœ‰èªçŸ¥éšœç¤™ï¼Œç¶“å¸¸å¡åœ¨è§’è½æˆ–å®¶å…·é–“ä¸çŸ¥é“å¦‚ä½•è„«å›°ï¼Œæœ‰ä»€éº¼ç’°å¢ƒå®‰æ’å’Œå±…å®¶ç…§è­·æªæ–½å¯ä»¥å¹«åŠ©ç‰ æ›´èˆ’é©åœ°ç”Ÿæ´»ï¼Ÿå…¶ä»–é£¼ä¸»éƒ½æ˜¯æ€éº¼è™•ç†é€™ç¨®æƒ…æ³çš„ï¼Ÿæœ‰ç›¸é—œç…§ç‰‡å—ï¼Ÿ",
# "çµ¦æˆ‘ä¸€äº›ç…§è­·ç’°å¢ƒçš„åœ–ç‰‡",
# "é‡å°å¹´é•·çŠ¬éš»å¯èƒ½å‡ºç¾çš„ç¥ç¶“ç—…ç†è®ŠåŒ–ï¼Œå“ªäº›é—œéµæŒ‡æ¨™å¸¸è¢«ç”¨ä¾†å°æ¯”é˜¿èŒ²æµ·é»˜é¡å‹çš„é€€åŒ–ç—‡ç‹€ï¼Œä¸¦ä¸”èˆ‡è‡¨åºŠè§€å¯Ÿåˆ°çš„è¡Œç‚ºè¡°é€€æœ‰ä½•é—œè¯ï¼Ÿ",
# "é™¤äº†è—¥ç‰©ä»‹å…¥ä¹‹å¤–ï¼Œå¹³æ™‚é£¼é¤Šç®¡ç†èˆ‡ç’°å¢ƒèª¿æ•´æ–¹é¢æœ‰å“ªäº›å…·é«”ä½œæ³•ï¼Œèƒ½åŒæ™‚æœ‰åŠ©æ–¼å¤±æ™ºçŠ¬èˆ‡å¤±æ™ºè²“ç¶­æŒè¼ƒä½³çš„ç”Ÿæ´»å“è³ªï¼Œä¸¦ç‚ºä½•å¤šç¨®æ–¹å¼ä¸¦ç”¨çš„ç…§è­·ç­–ç•¥å¾€å¾€æ›´èƒ½å»¶ç·©èªçŸ¥é€€åŒ–ï¼Ÿ",
# "è‹¥ä»¥è€çŠ¬ä½œç‚ºæ¨¡æ“¬äººé¡è€åŒ–èˆ‡å¤±æ™ºçš„å¯¦é©—æ¨¡å‹ï¼Œé€²è¡ŒèªçŸ¥å¢ç›Šæˆ–æ²»ç™‚æ€§è—¥ç‰©çš„è©•ä¼°æ™‚ï¼Œæœ€å¸¸æ¡ç”¨å“ªäº›è©•é‡æ–¹æ³•ä¾†ç¢ºèªè—¥ç‰©å°è¡Œç‚ºå’Œç¥ç¶“åŠŸèƒ½çš„å½±éŸ¿ï¼Œä¸¦ä¸”åœ¨å“ªäº›ç¥ç¶“å‚³å°è·¯å¾‘ä¸Šé€šå¸¸æœƒçœ‹åˆ°è¼ƒæ˜é¡¯çš„æŒ‡æ¨™æ€§è®ŠåŒ–ï¼Ÿ",
# "In older dogs, which key indicators are commonly used to compare with Alzheimer-type degeneration, and how do these indicators relate to clinically observed behavioral decline?",
# "Beyond pharmacological intervention, which specific management and environmental adjustments help senior dogs and cats with cognitive impairment maintain a higher quality of life, and why does combining multiple caregiving strategies often slow cognitive decline more effectively?",
# "When using senior dogs as a model for human aging and dementia to evaluate cognitive-enhancing or therapeutic drugs, what assessment methods are most commonly employed to gauge the drugâ€™s effects on behavior and neurological function, and in which neurotransmission pathways are the most prominent changes typically observed?"
"åœ¨è©•ä¼°çŠ¬éš» CCD çš„è‡¨åºŠç—‡ç‹€æ™‚ï¼Œä¸‹åˆ—å“ªä¸€é …è¡Œç‚ºé¢å‘æœ€å¸¸è¢«åˆ—ç‚ºä¸»è¦è§€å¯ŸæŒ‡æ¨™ä¹‹ä¸€? A. æ¯›è‰²æ˜¯å¦è®Šç™½ B. é£²æ°´é‡çš„å¢åŠ  C. å®šå‘èƒ½åŠ› (Orientation) èˆ‡ç©ºé–“è¾¨è­˜åº¦ D. å¿ƒè·³èˆ‡å‘¼å¸é€Ÿç‡"

                    ]

for query in test_queries:
    qa_system.display_response(query)


# Embedding processor

# In[332]:


from pathlib import Path
TEST_MODE = False                           # â† åˆ‡æ›é–‹é—œ
COLLECTION_NAME = "clip_collection_0504"   

# 1) åˆå§‹åŒ– embedding_processorï¼Œå‚³å…¥æ–°çš„ collection_name
embedding_processor = EmbeddingProcessor(
    image_size=(224, 224) ,
    collection_name=COLLECTION_NAME,    # â˜…è‹¥ __init__ æ²’é€™åƒæ•¸ï¼Œæ”¹ä¸‹æ–¹è¨»è§£æ–¹æ³•
    reset=False
)


# In[ ]:


manifest = pd.read_excel("herb_image_manifest.xlsx")

for row in manifest.itertuples(index=False):
    qa_system.embedding_processor.add_vectors(
        images=[row.filename],
        metadatas=[{
            "type":      "images",
            "herb":      row.herb_name,
            "caption":   row.caption,
            "path":      row.filename,      # â˜… ç¢ºä¿æœ‰ path
            "source_file":"herb_images"
        }]
    )


# In[337]:


col      = qa_system.embedding_processor.clip_collection
manifest = pd.read_excel("herb_image_manifest.xlsx")

for row in manifest.itertuples(index=False):
    path      = row.filename
    herb_name = row.herb_name
    caption   = str(row.caption) if pd.notna(row.caption) else ""

    # --- ç”¨ get() åˆ¤æ–·æ˜¯å¦å·²æœ‰è©²åœ– ---
    exists = bool(col.get(where={"path": path})["ids"])

    if exists:
        # åº«è£¡å·²æœ‰åœ–ç‰‡å‘é‡ â†’ åªè£œ captionï¼ˆ768-dim textï¼‰
        qa_system.embedding_processor.add_vectors(
            texts=[caption],
            metadatas=[{
                "type":      "caption",
                "ref_image": path,
                "herb":      herb_name,
                "path":      path
            }]
        )
    else:
        # åœ–ç‰‡ + caption ä¸€èµ·åŠ ï¼ˆå„ 1 ç­†å‘é‡ï¼Œéƒ½æ˜¯ 768-dimï¼‰
        qa_system.embedding_processor.add_vectors(
            images   =[path],
            metadatas=[{
                "type":     "image",
                "herb":     herb_name,
                "caption":  caption,
                "path":     path
            }]
        )

print("âœ… herb åœ–ç‰‡èˆ‡ caption å®ŒæˆåŒæ­¥ï¼ˆä»ç‚º 768 ç¶­å‘é‡ç©ºé–“ï¼‰")


# In[335]:


raw = embedding_processor.similarity_search("What does GanCao look like?", k=30)
print(raw)  # è¼¸å‡ºå®Œæ•´çš„å›å‚³è³‡æ–™
print(raw["metadatas"])  # ç¢ºä¿å…¶ä¸­æœ‰"image"æˆ–ä½ è¨­å®šçš„type


# In[336]:


resp, imgs = qa_system.display_response("What does GanCao look like?", "qa")


# ##### é‡å»ºDB

# In[ ]:


# 2) åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨
data_processor = DataProcessor(embedding_processor)

# 3) æŒ‡å®šæ¸¬è©¦æˆ–æ­£å¼è³‡æ–™å¤¾
rag_data_dir = Path("RAG_data_test" if TEST_MODE else "RAG_data")
pdf_paths = list(rag_data_dir.glob("*.pdf"))


print("æ‰¾åˆ°ä»¥ä¸‹ PDFï¼š")
for p in pdf_paths: print(" -", p.name)

# 4) è™•ç†è³‡æ–™ ï¼ˆCSV ä½ å¯ä»¥å‚³ None ä»£è¡¨ä¸è™•ç†ç¤¾ç¾¤è³‡æ–™ï¼‰
_ = data_processor.process_all(
    csv_path="post_response_filtered.xlsx",           # åªæ¸¬ PDFï¼Œå¯å…ˆä¸ç®¡ç¤¾ç¾¤
    pdf_paths=pdf_paths
)


# ##### Initialized QA System

# In[333]:


# å»ºç«‹ QA ç³»çµ±ï¼Œæ²¿ç”¨åŒä¸€å€‹ embedding_processor
qa_system = QASystem(
    embedding_processor=embedding_processor,
    model_name='llama3.2-vision'
)


# ##### ç¿»è­¯

# In[ ]:


def translate_zh_to_en(chinese_text: str) -> str:
    try:
        # æŒ‡å®šåŸæ–‡èªè¨€ç‚º 'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œç›®æ¨™èªè¨€ç‚º 'en'ï¼ˆè‹±æ–‡ï¼‰
        translator = GoogleTranslator(source='zh-TW', target='en')
        result = translator.translate(chinese_text)
        return result
    except Exception as e:
        print(f"ç¿»è­¯éŒ¯èª¤ï¼š{e} - å°æ‡‰ä¸­æ–‡å•é¡Œï¼š{chinese_text}")
        return chinese_text  # è‹¥ç¿»è­¯å¤±æ•—ï¼Œè¿”å›åŸæ–‡


# ##### åˆ¤æ–·æ­£ç¢ºç­”æ¡ˆ

# In[ ]:


import re

def parse_llm_answer(resp: str, q_type: str) -> str:
    """
    è§£æ LLM å›ç­”æ–‡å­—ï¼Œå›å‚³æœ€çµ‚ç­”æ¡ˆï¼š
      â€¢ multiple_choice â†’ 'A'|'B'|'C'|'D'|'UNK'
      â€¢ true_false      â†’ 'TRUE'|'FALSE'|'UNK'
    """
    txt = resp.lower()
    txt = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼›ï¼š\s]+', ' ', txt)        # å…ˆçµ±ä¸€ç©ºç™½

    if q_type == "multiple_choice":
        # æ‰¾æ‰€æœ‰ã€Œç¨ç«‹ã€çš„ a-d (å«å¤§å°å¯«)ï¼Œä¸å« 'é¸é …a' é€™ç¨®çµ„å­—
        matches = re.findall(r'\b([abcd])\b', txt, flags=re.I)
        return matches[-1].upper() if matches else "UNK"

    elif q_type == "true_false":
        # æ‰¾æ‰€æœ‰ true/false / å°/éŒ¯ / æ˜¯/å¦
        tf_matches = re.findall(
            r'\b(true|false|æ­£ç¢º|éŒ¯èª¤|å°|éŒ¯|æ˜¯|å¦)\b', txt)
        if not tf_matches:
            return "UNK"
        last = tf_matches[-1]
        return "TRUE" if last in ("true", "æ­£ç¢º", "å°", "æ˜¯") else "FALSE"

    else:   # å…¶é¤˜é¡Œå‹åŸæ–‡è¿”å›
        return resp


# æ¸¬è©¦

# In[274]:


# 1. è®€æª” + é¡Œå‹ç¯©é¸
df = pd.read_excel("test_questions_en.xlsx")#test_questions #test_questions_withANS
# test_df = df[df["type"].isin(["multiple_choice", "true_false"])].copy()
test_df = df[df["type"].isin(["qa"])].copy()

# 2. â˜… å»ºç«‹æ¬„ä½ï¼ˆä¸€å®šè¦åœ¨å¾Œé¢çš„ç¯©é¸å‰å…ˆåŠ ï¼‰
test_df["llm_response"] = ""
test_df["predicted"]    = ""
test_df["is_correct"]   = 0

# 3. å†ä¾ domain ç¯©å­é›†åˆ
# test_df = test_df[test_df["domain"] == "ä¸­é†«"].copy()
test_df=test_df.head(5)


# In[ ]:


dataset = [] # for ragas

# 4. è¿´åœˆè¨ˆåˆ†
for idx, row in test_df.iterrows():
    q  = row["question_en"]
    q_type = row["type"]
    gt = str(row["answers"]).strip()
    ref_ctx   = [ str(row["RAG"]) ] 

    resp, _ = qa_system.display_response(q, q_type)

    if not resp.strip():
        print(f"[WARN] id={row['id']}  LLM å›å‚³ç©ºç™½")


    resp, ctxs, _ = qa_system.generate_response(q, q_type)
    pred = parse_llm_answer(resp, q_type)

    test_df.at[idx, "llm_response"] = resp
    test_df.at[idx, "predicted"]    = pred
    test_df.at[idx, "is_correct"]   = int(pred.upper() == gt.upper())
    
    ctxs = [str(c) for c in ctxs]

    dataset.append({
        "user_input":           str(q),           # question
        "response":             str(resp),        # llm response
        "retrieved_contexts":   ctxs,             # llmæª¢ç´¢åˆ°çš„è³‡æ–™
        "reference_contexts":   ref_ctx,          # å‡ºé¡Œæ®µè½
        "reference":            gt                # answers
    })


# In[ ]:


# 5. è¨ˆç®— Accuracy
overall_acc = test_df["is_correct"].mean()

print("\n=== æ¯å€‹ domain çš„ Accuracy ===")
domain_stats = (
    test_df.groupby("domain")["is_correct"]
           .agg(["count", "sum"])
           .reset_index()
           .rename(columns={"sum": "correct"})
)
domain_stats["accuracy"] = domain_stats["correct"] / domain_stats["count"]
print(domain_stats.to_string(index=False, 
      formatters={"accuracy": "{:.2%}".format}))

print(f"\nOVERALL Accuracy = {overall_acc:.2%}")


# In[ ]:


# å…ˆæŒ‘å‡ºç­”éŒ¯çš„è³‡æ–™åˆ—
wrong_df = (
    test_df.loc[test_df["is_correct"] == 0,
                ["id", "question", "answers", "predicted"]]
            .sort_values("id")          # ä¾é¡Œè™Ÿæ’åºæ–¹ä¾¿æŸ¥çœ‹
)

print("=== ç­”éŒ¯é¡Œç›®ä¸€è¦½ ===")
print(wrong_df.to_string(index=False))


# #### æª¢æŸ¥

# In[ ]:


coll = qa_system.embedding_processor.clip_collection
# æŠ½ 200 ç­†çœ‹ metadata å‡ºç¾å“ªäº› type
sample = coll.get(limit=1000, include=["metadatas"])
types  = [m.get("type") for m in sample["metadatas"]]
print(set(types))


# In[ ]:


import numpy as np, pandas as pd, re

coll = qa_system.embedding_processor.clip_collection

# ----- 1. similarity_search() -----
raw = qa_system.embedding_processor.similarity_search(
"some Canine behavior", k=20)

# ----- 2. å®‰å…¨æ”¤å¹³å·¥å…· -----
def safe_flat(val):
    if val is None:
        return []
    if isinstance(val, list):
        flat = []
        for v in val:
            flat.extend(v if isinstance(v, list) else [v])
        return flat
    return [val]

flat_ids        = safe_flat(raw.get("ids"))
flat_metas      = safe_flat(raw.get("metadatas"))
flat_docs       = safe_flat(raw.get("documents"))
flat_embeds     = safe_flat(raw.get("embeddings"))

# è‹¥ embeddings æ²’å›å‚³ï¼Œç¢ºä¿é•·åº¦å°é½Š
if not flat_embeds:
    flat_embeds = [None] * len(flat_ids)

# ----- 3. åªçœ‹å‰ 5 ç­† -----
rows = []
for _id, meta, doc, emb in zip(flat_ids[:5], flat_metas, flat_docs, flat_embeds):
    if emb is None:                              # æ²’å¸¶å›å‘é‡å°±é¡å¤– get
        emb = coll.get(ids=[_id], include=["embeddings"])["embeddings"][0]
    norm = np.linalg.norm(emb)

    if isinstance(meta, dict):                   # æœ‰äº›ç‰ˆæœ¬ meta å¯èƒ½ç©º
        src = meta.get("source_file", "?")
        pg  = meta.get("page", "?")
    else:
        src = pg = "?"

    rows.append({
        "id":      _id,
        "norm":    round(norm, 4),
        "source":  src,
        "page":    pg,
        "preview": str(doc).replace("\n", " ")[:60] + "â€¦"
    })

print("ğŸ” LI-11 å‘é‡ L2-norm æª¢æŸ¥")
display(pd.DataFrame(rows))


# In[ ]:


# pip install deep_translator openpyxl pandas
from deep_translator import GoogleTranslator
import pandas as pd, hashlib, sqlite3, time

# --- 1.  å»ºå¿«å– DB    -----------------------
conn = sqlite3.connect("trans_cache.sqlite")
conn.execute("""CREATE TABLE IF NOT EXISTS cache
                (h TEXT PRIMARY KEY, en TEXT)""")
translator = GoogleTranslator(source='zh-TW', target='en')

def zh2en(text):
    if not isinstance(text, str):
        return str(text)
    h = hashlib.md5(text.encode()).hexdigest()
    row = conn.execute("SELECT en FROM cache WHERE h=?", (h,)).fetchone()
    if row:           # å‘½ä¸­å¿«å–
        return row[0]
    # å‘¼å« Google ç¿»è­¯
    try:
        en = translator.translate(text)
    except Exception:
        print("[WARN] Google ç¿»è­¯å¤±æ•—ï¼Œç¡ 2 ç§’é‡è©¦â€¦")
        time.sleep(2)
        try:
            en = translator.translate(text)
        except Exception as e:
            print("ä»å¤±æ•—ï¼Œå›å‚³åŸæ–‡ >", e)
            en = text
    conn.execute("INSERT OR REPLACE INTO cache VALUES (?,?)", (h, en))
    conn.commit()
    return en

# --- 2.  è®€é¡Œåº«ï¼ŒåŠ ç¿»è­¯ ---------------------
df = pd.read_excel("test_questions_withANS.xlsx")

df["question_en"]   = df["question"].apply(zh2en)
df["reference_en"]  = df["reference"].apply(zh2en)  if "reference" in df.columns else ""
df["answers_en"]    = df["answers"].apply(zh2en)    if "answers"   in df.columns else ""

# --- 3.  å­˜æª” --------------------------------
out_path = "test_questions_en.xlsx"
df.to_excel(out_path, index=False)
print("âœ… ç¿»è­¯å®Œæˆï¼Œå·²å­˜", out_path)


# #### RAGAS

# https://docs.ragas.io/en/stable/

# In[ ]:


from dotenv import load_dotenv
env_path = Path("key") / ".env"
load_dotenv(dotenv_path=env_path, override=False)

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("æ‰¾ä¸åˆ° OPENAI_API_KEY")


# è·‘è©•åˆ†

# In[ ]:


from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI  # langchain>=0.1
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0)
evaluator_llm = LangchainLLMWrapper(llm)


# In[ ]:


from ragas import EvaluationDataset
evaluation_dataset = EvaluationDataset.from_list(dataset)


# In[ ]:


from ragas import evaluate
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness,ContextPrecision

result = evaluate(
    dataset=evaluation_dataset,
    metrics=[
        LLMContextRecall(),
        ContextPrecision(),
        # Faithfulness(),      #only QA å¿ å¯¦åº¦
        # FactualCorrectness(), #only QA æ­£ç¢ºæ€§
    ],
    llm=evaluator_llm
)


# In[ ]:


result

