#!/usr/bin/env python
# coding: utf-8

# #### è½‰py

# In[262]:


get_ipython().system('jupyter nbconvert --to script rag_system_llama.ipynb')


# ### Code

# ##### import

# In[3]:


import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import pandas as pd
import numpy as np
import re

# Display and Image handling
from IPython.display import display
from PIL import Image as PILImage  # ä½¿ç”¨ PILImage ä½œä¸º PIL.Image çš„åˆ«å
from IPython.display import Image as IPyImage  # ä½¿ç”¨ IPyImage ä½œä¸º IPython çš„ Image

# Vector DB
import chromadb


# LLM
import ollama

# PDFå¤„ç†
import PyPDF2

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ£€æŸ¥å¹¶åˆ›å»ºå¿…è¦çš„ç›®å½•
Path('chroma_db').mkdir(exist_ok=True)
Path('image').mkdir(exist_ok=True)


# In[4]:


# æ”¾åœ¨æª”æ¡ˆæœ€ä¸Šæ–¹ (import ä¹‹å¾Œ)
TYPE_MAP = {
    "acupoint"    : ["é‡ç¸", "acupuncture"],
    "herb"        : ["herbology", "herbal", "æ–¹åŠ‘"],
    "ccd"         : ["ccd", "èªçŸ¥", "cognition"],
    "social"      : [],                   # csv ç›´æ¥æŒ‡å®š
    "professional": [],
    "image":[]                                       # å…¶ä»–æœªåˆ†é¡
}


# ##### voice to text
# 

# In[ ]:


# whisper /Users/zirong/Desktop/test.mp4 --language Chinese --model tiny
import whisper
def transcribe_file(file_path, model_size="base"):
    model = whisper.load_model(model_size)
    result = model.transcribe(file_path)
    return result["text"]

# def main():
#     audio_file = "no_upload/test_mp3/01.mp3"  # ä¿®æ”¹ç‚ºä½ çš„éŸ³æª”è·¯å¾‘
#     transcription = transcribe_file(audio_file)
#     print("Transcription:", transcription)

# if __name__ == "__main__":
#     main()


# #### ImageProcessor

# In[5]:


from typing import Union  # æ·»åŠ  Union å¯¼å…¥
from pathlib import Path

class ImageProcessor:
    def __init__(self, image_dir: str = "image"):
        self.image_dir = Path(image_dir)
        self.image_dir.mkdir(exist_ok=True)
        
    def process_and_save(
        self,
        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |
        target_size: Tuple[int, int],
        prefix: str = "resized_",
        quality: int = 95
    ) -> Optional[Path]:
        """ç»Ÿä¸€çš„å›¾ç‰‡å¤„ç†æ–¹æ³•ï¼Œå¤„ç†å¹¶ä¿å­˜å›¾ç‰‡"""
        try:
            # ç¡®ä¿ image_path æ˜¯ Path å¯¹è±¡
            image_path = Path(image_path)
            if not str(image_path).startswith(str(self.image_dir)):
                image_path = self.image_dir / image_path
                
            # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
            if not image_path.exists():
                logger.error(f"Image not found: {image_path}")
                return None
                
            # è¯»å–å¹¶å¤„ç†å›¾ç‰‡
            image = PILImage.open(image_path)
            
            # è½¬æ¢ä¸º RGB æ¨¡å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')
                
            # è®¡ç®—ç­‰æ¯”ä¾‹ç¼©æ”¾çš„å¤§å°
            width, height = image.size
            ratio = min(target_size[0]/width, target_size[1]/height)
            new_size = (int(width * ratio), int(height * ratio))
            
            # ç¼©æ”¾å›¾ç‰‡
            image = image.resize(new_size, PILImage.Resampling.LANCZOS)
            
            # åˆ›å»ºæ–°çš„ç™½è‰²èƒŒæ™¯å›¾ç‰‡
            new_image = PILImage.new('RGB', target_size, (255, 255, 255))
            
            # è®¡ç®—å±…ä¸­ä½ç½®
            x = (target_size[0] - new_size[0]) // 2
            y = (target_size[1] - new_size[1]) // 2
            
            # è´´ä¸Šç¼©æ”¾åçš„å›¾ç‰‡
            new_image.paste(image, (x, y))
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_path = self.image_dir / f"{image_path.name}" #output_path = self.image_dir / f"{prefix}{image_path.name}"
            # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
            new_image.save(output_path, quality=quality)
            logger.info(f"Saved processed image to: {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error processing image {image_path}: {str(e)}")
            return None
            
    def load_for_display(self, 
                        image_path: Union[str, Path],  # ä½¿ç”¨ Union æ›¿ä»£ |
                        display_size: Tuple[int, int]) -> Optional[PILImage.Image]:
        """è½½å…¥å›¾ç‰‡ç”¨äºæ˜¾ç¤º"""
        try:
            processed_path = self.process_and_save(image_path, display_size, prefix="display_")
            if processed_path:
                return PILImage.open(processed_path)
            return None
        except Exception as e:
            logger.error(f"Error loading image for display {image_path}: {str(e)}")
            return None


# #### EmbeddingProcessor

# In[29]:


get_ipython().run_line_magic('matplotlib', 'inline')
from transformers import AutoProcessor, AutoModel
import torch
import sentencepiece as spm 
import uuid

class EmbeddingProcessor:

    MAX_TOKEN = 56          # 56 + BOS + EOS = 58 < 64
    OVERLAP   = 16
    DEFAULT_COLLECTION = "ccd_docs_siglip"

    # åˆå§‹åŒ– embedding processor
    def __init__(self, 
                persist_directory: str = "chroma_db",
                image_dir: str = "image",
                image_size: tuple = (224, 224),
                collection_name:str = DEFAULT_COLLECTION,
                reset: bool = False 
                ):
        """
        åˆå§‹åŒ–: å»ºç«‹collection
        """
        # ---------- è·¯å¾‘ & åŸºæœ¬è¨­å®š ----------
        self.image_dir = Path(image_dir)
        self.image_size = image_size
        self.collection_name = collection_name
        self.image_processor = ImageProcessor(image_dir)

        # ---------- 1) å»ºç«‹ Chroma client ----------
        logger.info(f"Initializing Chroma with directory: {persist_directory}")
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)

        # ---------- 2) reset (= åˆªæ‰èˆŠåº«) ----------
        if reset:
            try:
                self.chroma_client.delete_collection(self.collection_name)
                logger.info(f"Deleted collection: {self.collection_name}")
            except (chromadb.errors.NotFoundError, ValueError):
                logger.info("No old collection to delete")

        
        # ---------- 3) åˆå§‹åŒ– SigLIP ----------
        SIGLIP_NAME = "google/siglip-base-patch16-224"
        self.processor = AutoProcessor.from_pretrained(SIGLIP_NAME)
        self.siglip    = AutoModel.from_pretrained(SIGLIP_NAME)

        # å–è¼¸å‡ºå‘é‡é•·åº¦ (base = 768)
        with torch.no_grad():
            dummy = torch.zeros((1, 3, 224, 224))
            self.clip_dim = self.siglip.get_image_features(dummy).shape[1]

        # ---------- 4) å–å¾—æˆ–å»ºç«‹ collection ----------
        self.clip_collection = self.chroma_client.get_or_create_collection(
            name     = self.collection_name,
            metadata = {"dimension": self.clip_dim}
        )
        logger.info(
            f"Using collection '{self.collection_name}' "
            f"(dimension={self.clip_dim}, reset={reset})"
        )

    def to_2d(self,x):
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()
        elif isinstance(x, list):
            x = np.asarray(x, dtype=np.float32)
        if x.ndim == 1:        # (512,) â†’ (1,512)
            x = x[None, :]
        elif x.ndim != 2:
            raise ValueError(f"embedding ndim should be 1 or 2, got {x.shape}")
        return x.tolist()      # List[List[float]]

    def chunk_text_by_token(
            self,
            text: str,
            max_tokens: Optional[int] = None,
            overlap: Optional[int] = None
        ) -> List[str]:
        CH_SENT_SPLIT = re.compile(r'([ã€‚ï¼ï¼Ÿï¼›\n])')
        """å¥è™Ÿå„ªå…ˆæ–·å¥ï¼›ä»»ä½•å­å¥æœ€çµ‚éƒ½ â‰¤ 56 token"""
        max_tokens = max_tokens or self.MAX_TOKEN      # 56
        overlap    = overlap    or self.OVERLAP        # 16

        # --- 1) ä»¥ä¸­æ–‡æ¨™é»åˆ‡æˆå­å¥ ---
        sentences, buf, parts = [], "", CH_SENT_SPLIT.split(text)
        for frag in parts:
            if CH_SENT_SPLIT.match(frag):
                buf += frag          # æŠŠæ¨™é»åŠ å›ä¾†
                sentences.append(buf.strip())
                buf = ""
            else:
                buf += frag
        if buf: sentences.append(buf.strip())

        # --- 2) ä»»ä½• >56 token çš„å¥å­å†æ»‘çª—åˆ‡ ---
        chunks = []
        for s in sentences:
            ids = self.processor.tokenizer(s).input_ids
            if len(ids) <= max_tokens:
                chunks.append(s)
            else:
                step = max_tokens - overlap
                for i in range(0, len(ids), step):
                    seg_ids = ids[i:i + max_tokens]
                    seg = self.processor.tokenizer.decode(seg_ids,
                                                        skip_special_tokens=True)
                    chunks.append(seg)
        return chunks


    def encode_text_to_vec(self, text: str) -> Optional[np.ndarray]:
        """
        ç”¨ CLIP çš„ text encoder å°‡æ–‡å­—è½‰ç‚º512ç¶­å‘é‡
        """
        try:
            chunks = self.chunk_text_by_token(text)
            if not chunks:
                logger.error("No valid chunks generated for the text.")
                return None
            all_vecs = []
            for ch in chunks:
                inp = self.processor(text=[ch], return_tensors="pt").to(self.siglip.device)
                with torch.no_grad():
                    vec = self.siglip.get_text_features(**inp)
                all_vecs.append(vec)
            # é€™è£¡å¯å–å¹³å‡æˆ–ç›´æ¥å›å‚³å¤šæ¢å‘é‡
            embs = torch.stack(all_vecs).mean(dim=0)
            emb = embs / embs.norm(dim=-1, keepdim=True)
            return emb.squeeze(0).cpu().tolist()   
        except Exception as e:
            logger.error(f"Error in encode_text_to_vec: {e}")
            return None
        
    def add_qa_pairs(self,
                questions: List[str],
                answers: List[str],
                question_metadatas: List[Dict],
                answer_metadatas: List[Dict],
                images: Optional[List[str]] = None):
        """æ·»åŠ é—®ç­”å¯¹åˆ°ä¸åŒçš„é›†åˆ"""
        try:
            # æ·»åŠ é—®é¢˜
            if questions and question_metadatas:
                logger.info(f"Adding {len(questions)} questions")
                self.question_collection.add(
                    documents=questions,
                    metadatas=question_metadatas,
                    ids=[f"q_{i}" for i in range(len(questions))]
                )
            
            # æ·»åŠ å›ç­”
            if answers and answer_metadatas:
                logger.info(f"Adding {len(answers)} answers")
                self.answer_collection.add(
                    documents=answers,
                    metadatas=answer_metadatas,
                    ids=[f"a_{i}" for i in range(len(answers))]
                )
            
            # å¤„ç†å›¾ç‰‡
            if images:
                logger.info(f"Processing {len(images)} images")
                all_ids=[]
                all_embeddings=[]
                all_metadatas = []

                
                for i, (img_path,question_text) in enumerate(zip(images, questions)):
                    img_emb = self.process_image(str(self.image_dir / img_path))
                    txt_emb = self.encode_text_to_vec(question_text)

                    if img_emb is not None:
                        all_embeddings.append(img_emb.tolist())
                        all_metadatas.append({
                            "type": "image", 
                            "path": img_path,
                            "associated_question": question_text
                        })
                        all_ids.append(f"img_{uuid.uuid4().hex}")
                    if txt_emb is not None:
                        all_embeddings.append(txt_emb.tolist())  
                        all_metadatas.append({
                            "type": "clip_text", 
                            "text": question_text,
                            "related_image": img_path
                        })
                        all_ids.append(f"txt_{uuid.uuid4().hex}") # all_ids.append(f"txt_{i}")

                if len(all_embeddings)>0:
                    logger.info(f"Adding {len(all_embeddings)} total embeddings to collection")
                    self.image_collection.add(
                        embeddings=all_embeddings,
                        metadatas=all_metadatas,
                        ids=all_ids
                    )
            
        except Exception as e:
            logger.error(f"Error adding QA pairs: {str(e)}")
            raise

    def encode_image_to_vec(self, image_path: str) -> Optional[np.ndarray]:

        try:
                # å…ˆåšåŸºç¤è™•ç†,ç¸®æ”¾æˆ–å¦å­˜
                processed_path = self.image_processor.process_and_save(
                    image_path, self.image_size
                )
                if not processed_path:
                    return None

                image = PILImage.open(processed_path)
                inputs = self.processor(images=[image], return_tensors="pt").to(self.siglip.device)
                with torch.no_grad():
                    embs = self.siglip.get_image_features(**inputs)
                return (embs / embs.norm(dim=-1, keepdim=True)).cpu().numpy()
        except Exception as e:
                print(f"Error in encode_image_to_vec: {str(e)}")
                return None

    def add_vectors(
        self,
        texts: Optional[List[str]] = None,
        metadatas: Optional[List[Dict]] = None,
        images: Optional[List[str]] = None,
        ):
        """
        çµ±ä¸€æŠŠæ–‡å­— / åœ–ç‰‡å¯«é€² clip_collection
        """
        texts     = texts or []
        images    = images or []
        metadatas = metadatas or []

        all_embs, all_metas, docs, all_ids = [], [], [], []
        idx = 0

        # -------------------- æ–‡å­— --------------------
        for i, txt in enumerate(texts):
            emb = self.encode_text_to_vec(txt)
            if emb is None:
                continue

            # â‘  å– metadata ä¸”ä¿è­‰æ˜¯ dict
            src_meta = metadatas[i] if i < len(metadatas) else {}
            if not isinstance(src_meta, dict):
                src_meta = {"note": str(src_meta)}

            # â‘¡ domain â†’ type æ˜ å°„ï¼ˆåªåšä¸€æ¬¡ï¼‰
            domain = src_meta.pop("domain", "").lower()
            if domain in {"é‡ç¸å­¸", "acupuncture"}:
                src_meta["type"] = "acupoint"
            elif domain in {"herb","herbology"}:
                src_meta["type"] = "herb"
            elif domain in {"ccd","canine"}:
                src_meta["type"] = "ccd"

            for vec in self.to_2d(emb):
                md = {
                    "type": src_meta.get("type", "professional"),
                    "content": txt,
                    **src_meta,            # å…¶é¤˜æ¬„ä½ä¿ç•™
                }
                all_embs.append(vec)
                all_metas.append(md)
                docs.append(txt)
                all_ids.append(str(uuid.uuid4())) #(f"text_{idx}")
                idx += 1

        # -------------------- åœ–ç‰‡ --------------------
        for j, img_name in enumerate(images):
            full_path = str(self.image_dir / img_name)
            emb = self.encode_image_to_vec(full_path)
            if emb is None:
                continue

            src_meta = metadatas[j] if j < len(metadatas) else {}
            if not isinstance(src_meta, dict):
                src_meta = {"note": str(src_meta)}
            src_meta.pop("type", None) 

            # --- 2-1 åœ–ç‰‡å‘é‡ ---
            img_meta = {
                "type": "image",
                "path": img_name,
                **src_meta,
            }
            all_embs.append(self.to_2d(emb)[0])
            all_metas.append(img_meta)
            docs.append("")                         # åœ–ç‰‡æ²’æœ‰ document
            img_id = f"img_{uuid.uuid4().hex}"
            all_ids.append(img_id)

            # --- 2-2 caption å‘é‡ï¼ˆè‹¥æœ‰ï¼‰---
            cap = src_meta.get("caption") or src_meta.get("image_description")
            if cap:
                cap_emb = self.encode_text_to_vec(cap)
                if cap_emb is not None:
                    all_embs.append(self.to_2d(cap_emb)[0])
                    all_metas.append({
                        "type": "caption",          # æ–¹ä¾¿å‰ç«¯è¾¨è­˜
                        "ref_image": img_name,      # æ—¥å¾Œå¯èšåˆ
                        "content": cap,
                        **src_meta,
                    })
                    docs.append(cap)
                    all_ids.append(f"{img_id}_cap")

            # md = {
            #     "type": "image",
            #     "path": img_name,
            #     **src_meta,
            # }
            # for vec in self.to_2d(emb):
            #     all_embs.append(vec)
            #     all_metas.append(md)
            #     docs.append("")          # å ä½
            #     all_ids.append(f"img_{uuid.uuid4().hex}")#(f"img_{idx}")
            #     idx += 1

        # -------------------- å¯«å…¥ Chroma --------------------
        if all_embs:
            self.clip_collection.add(
                embeddings = all_embs,
                metadatas  = all_metas,
                documents  = docs,
                ids        = all_ids,
            )
            logger.info(f"Added {len(all_embs)} items to '{self.collection_name}'")


    def detect_weight(self,q: str):
        ACU_REGEX = r"[A-Za-z]{2,3}-\d{1,2}"
        ql = q.lower()
        if re.search(ACU_REGEX, ql) or "ç©´ä½" in ql:
            # é¡Œç›®åœ¨å•é‡ç¸ç©´ä½ â†’ é™ herb / ccd æ¬Šé‡
            return {"herb": 0.3, "ccd": 0.3}
        elif re.search(r"(æŸ´èƒ¡|é»ƒèŠ©|æ¸…ç†±|ç”˜è‰|ç•¶æ­¸)", ql):
            return {"acupoint": 0.3, "ccd": 0.3}
        elif re.search(r"(èªçŸ¥|cognitive|nlrp3|å¤±æ™ºçŠ¬|ccd)", ql):
            # é¡Œç›®åœ¨å• CCD â†’ é™ herb / acupoint æ¬Šé‡
            return {"herb": 0.3, "acupoint": 0.3}
        else:
            return {}          # ä¸èª¿æ¬Šé‡
        
    def similarity_search(self, query: str, k=25) -> Dict:
        """
        å°queryåšCLIP text embeddingå¾Œ,åœ¨clip_collectionè£¡æ‰¾æœ€ç›¸ä¼¼çš„kç­†
        """
        try:
            emb = self.encode_text_to_vec(query)
            if emb is None:
                return {"metadatas":[],"documents":[],"distances":[]}
        
            results = self.clip_collection.query(
                    query_embeddings=[emb],
                    n_results=k,
                    include=["documents","metadatas","distances","embeddings"] #include=["distances", "metadatas", "documents"]
            ) 
            # ---------- â–Œå‹•æ…‹é™æ¬Š + re-rank ----------------
            # q = query.lower()
            # if re.search(r"(st|cv|gv|bl|pc)-\d{1,2}|ç©´ä½", q):
            #     weight = {"herb": 0.3, "ccd": 0.3}     # acupoint = 1.0
            # elif any(w in q for w in ["æŸ´èƒ¡", "é»ƒèŠ©", "æ¸…ç†±"]):
            #     weight = {"acupoint": 0.3, "ccd": 0.3}
            # elif any(w in q for w in ["èªçŸ¥", "nlrp3", "ç™¼ç‚"]):
            #     weight = {"herb": 0.3, "acupoint": 0.3}
            # else:
            #     weight = {}
            weight = self.detect_weight(query)
            metas = results["metadatas"][0]
            dists = results["distances"][0]
            docs  = results["documents"][0]

            scored = []
            for i, (m, d) in enumerate(zip(metas, dists)):
                w = weight.get(m.get("type", ""), 1.0)
                scored.append((d * w, i))          # è·é›¢æ„ˆå°æ„ˆå¥½
            scored.sort(key=lambda x: x[0])

            idxs = [i for _, i in scored][:k]       # å–å‰ k

            #-----------ä¸é™æ¬Šç”¨çš„-----------------
            # metas = results["metadatas"][0]
            # idxs  = list(range(len(metas)))[:k]
            #-------------------------------------
            
            for key in ["metadatas", "distances", "documents"]:
                results[key][0] = [results[key][0][i] for i in idxs]

            return results
            
        except Exception as e:
            print(f"Error in search: {str(e)}")
            return {"metadatas":[],"documents":[],"distances":[]}

    # ------------------------------------------------------------
    #  æ–°å¢ï¼šsimilarity_search_with_images()
    # ------------------------------------------------------------
    def similarity_search_with_images(
        self,
        query: str,
        k_mix: int = 40,
        img_threshold: float = 0.45,
        max_images: int = 1,
    ):
        """
        å…ˆå–å‰ k_mix åæ··åˆçµæœï¼Œå†æŠŠè·é›¢ < img_threshold çš„
        image / caption è£œåˆ°çµæœä¸­ï¼ˆæœ€å¤š max_images å¼µï¼‰ã€‚
        å›å‚³å€¼æ ¼å¼èˆ‡ similarity_search ç›¸åŒã€‚
        """
        emb = self.encode_text_to_vec(query)
        if emb is None:
            return {"metadatas": [], "documents": [], "distances": []}

        # â‘  æ··åˆå‰ k_mix
        raw = self.clip_collection.query(
            query_embeddings=[emb],
            n_results=k_mix,
            include=["metadatas", "documents", "distances"],
        )

        # â‘¡ æŠŠåœ–ç‰‡å€™é¸æŠ“å‡ºä¾†ï¼ˆè·é›¢å°æ–¼é–€æª»ï¼‰
        metas    = raw["metadatas"][0]
        dists    = raw["distances"][0]
        docs     = raw["documents"][0]
        good_idx = [
            i for i, (m, d) in enumerate(zip(metas, dists))
            if m.get("type") in ("image", "images", "caption") and d < img_threshold
        ]

        # â‘¢ è‹¥ä¸è¶³ max_imagesï¼Œå†å°ˆæœåœ–ç‰‡è£œè¶³
        if len(good_idx) < max_images:
            need   = max_images - len(good_idx)
            imgraw = self.clip_collection.query(
                query_embeddings=[emb],
                n_results=need,
                where={"type": {"$in": ["image", "images", "caption"]}},
                include=["metadatas", "documents", "distances"],
            )
            for key in ["metadatas", "documents", "distances"]:
                raw[key][0].extend(imgraw[key][0])

        return raw

    


# #### DataProcessor

# In[9]:


class DataProcessor:
    def __init__(self, embedding_processor: 'EmbeddingProcessor'):
        self.embedding_processor = embedding_processor
        
    def extract_social_posts(self, csv_path: str) -> Tuple[List[Dict], List[str]]:
        """å¤„ç† CSV å¹¶æå–é—®ç­”å¯¹å’Œå›¾ç‰‡"""
        logger.info(f"Processing CSV: {csv_path}")
        qa_pairs = []
        images = []
        
        df = pd.read_excel(csv_path)
        current_post = None
        current_responses = []
        current_images = []
        current_link = None
        
        for _, row in df.iterrows():
            # å¤„ç†æ–°çš„å¸–å­
            if pd.notna(row['post']):
                # ä¿å­˜å‰ä¸€ä¸ªé—®ç­”å¯¹
                if current_post is not None:
                    qa_pair = {
                        'question': current_post,
                        'answers': current_responses.copy(),
                        'images': current_images.copy(),
                        'metadata': {
                            'type': 'social',
                            'source': 'facebook',
                            'images': ','.join(current_images) if current_images else '',
                            'answer_count': len(current_responses),
                            'link': current_link if current_link else ''
                        }
                    }
                    if pd.notna(row.get('image_description')):
                        qa_pair['metadata']['image_desc'] = row['image_description']

                    qa_pairs.append(qa_pair)
                    if current_images:
                        images.extend(current_images)
                
                # åˆå§‹åŒ–æ–°çš„é—®ç­”å¯¹
                current_post = row['post']
                current_responses = []
                current_images = []
                current_link = row.get('link', '')
            
            # æ·»åŠ å›å¤
            if pd.notna(row.get('responses')):
                current_responses.append(row['responses'])
            
            # å¤„ç†å›¾ç‰‡
            if pd.notna(row.get('images')):
                img_path = row['images']
                current_images.append(img_path)
                logger.info(f"Found image: {img_path} for current post")
  
        
        # ä¿å­˜æœ€åä¸€ä¸ªé—®ç­”å¯¹
        if current_post is not None and len(current_responses) >= 3:
            qa_pair = {
                'question': current_post,
                'answers': current_responses,
                'images': current_images,
                'metadata': {
                    'type': 'social_qa',
                    'source': 'facebook',
                    'images': ','.join(current_images) if current_images else '',
                    'answer_count': len(current_responses),
                    'link': current_link if current_link else ''
                }
            }
            if pd.notna(row.get('image_description')):
                    qa_pair['metadata']['image_desc'] = row['image_description']

            qa_pairs.append(qa_pair)
            if current_images:
                images.extend(current_images)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
        for i, qa in enumerate(qa_pairs):
            logger.info(f"\nQA Pair {i+1}:")
            logger.info(f"Question: {qa['question'][:100]}...")
            logger.info(f"Number of answers: {len(qa['answers'])}")
            logger.info(f"Images: {qa['images']}")
            logger.info(f"Link: {qa.get('link', 'No link')}")
        
        return qa_pairs, images

    
    def chunk_text(self,paragraph: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
        """
        å°‡çµ¦å®šæ®µè½ï¼Œä»¥ chunk_size å­—ç¬¦ç‚ºä¸Šé™é€²è¡Œåˆ‡åˆ†ï¼Œä¸¦ä¸”åœ¨ chunk ä¹‹é–“ä¿ç•™ overlap å€‹å­—çš„é‡ç–Šï¼Œ
        ä»¥å…ä¸Šä¸‹æ–‡æ–·è£‚ã€‚
        å‚™è¨»: 
        - é€™è£¡ä»¥ã€Œå­—ç¬¦ã€ç‚ºå–®ä½ï¼Œé©åˆä¸­æ–‡ï¼›è‹±æ–‡ä¹Ÿå¯ç”¨ï¼Œä½†è‹¥æƒ³ç²¾ç¢ºå°è‹±æ–‡ tokens å¯æ”¹æ›´å…ˆé€²æ–¹æ³•ã€‚
        """
        chunks = []
        start = 0
        length = len(paragraph)

        # å»æ‰å‰å¾Œå¤šé¤˜ç©ºç™½
        paragraph = paragraph.strip()

        while start < length:
            end = start + chunk_size
            # å– substring
            chunk = paragraph[start:end]
            chunks.append(chunk)
            # ç§»å‹•æŒ‡æ¨™(ä¸‹ä¸€å€‹ chunk)
            # overlap é é˜²æ–·å¥å¤±å»ä¸Šä¸‹æ–‡
            start += (chunk_size - overlap)

        return chunks
  

    def process_pdf(self, pdf_path: str,row_type: str) -> List[Dict]:
        logger.info(f"Processing PDF: {pdf_path}")
        professional_qa_pairs = []
        pdf_name = Path(pdf_path).name  # è·å–æ–‡ä»¶å
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                is_formula = self.detect_domain(pdf_name) == "ä¸­é†«æ–¹åŠ‘"
                is_acu = self.detect_domain(pdf_name) == "é‡ç¸å­¸"


                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    print(f"Page {page_num+1} raw text:", repr(text))
                    if is_formula:
                        paragraphs = self.split_formula_blocks(text)
                    elif is_acu:
                        paragraphs = self.split_acu_blocks(text)
                    else:
                        paragraphs = text.split('\n\n')
                    
                    
                    # è™•ç†æ¯å€‹æ®µè½
                    for para in paragraphs:
                        # logger.info(f"Para type: {type(para)}")

                        para_chunks = self.chunk_text(para)
                        # logger.info(f"Got {len(para_chunks)} chunks from chunk_text()")

                        for c in para_chunks:
                            qa_pair = {
                                'question': c[:50] + "...",  
                                'answers': [c],
                                'metadata': {
                                    'type': row_type,
                                    'source_file': pdf_name,  # æ·»åŠ æ–‡ä»¶å
                                    'page': str(page_num + 1),
                                    'content_length': str(len(c))
                                } #'domain':self.detect_domain(pdf_name),
                            }
                            professional_qa_pairs.append(qa_pair)
                
                logger.info(f"Extracted {len(professional_qa_pairs)} paragraphs from {pdf_name}")
                return professional_qa_pairs
                
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_name}: {str(e)}")
            return []
        
    def detect_domain(self, pdf_name: str) -> str:
        lower = pdf_name.lower()

        if "é‡ç¸" in pdf_name or "acupuncture" in lower:
            return "é‡ç¸å­¸"
        if "herbal" in lower or "herbology" in lower or "æ–¹åŠ‘" in pdf_name:
            return "ä¸­é†«æ–¹åŠ‘"
        return "å…¶ä»–"
    
    def split_formula_blocks(self,text: str) -> list[str]:
        """
        ç”¨æ­£å‰‡æŠ“å‡ºã€â— å…­å‘³åœ°é»ƒä¸¸ã€æˆ–ã€Liu Wei Di Huang Wanã€é–‹é ­ï¼Œ
        æ¯é‡ä¸‹ä¸€å€‹æ–¹åå°±çµæŸä¸Šä¸€å¡Š
        """
        pattern = re.compile(r"(?:â—|\s)([\u4e00-\u9fffA-Za-z\- ]{3,40}(?:æ¹¯|ä¸¸|é£²|æ•£|è†))")
        blocks = []
        cur_block = []
        for line in text.splitlines():
            if pattern.search(line):
                # é‡åˆ°ä¸‹ä¸€å¸–è—¥ â†’ å…ˆæ”¶å‰ä¸€å¸–
                if cur_block:
                    blocks.append("\n".join(cur_block).strip())
                    cur_block = []
            cur_block.append(line)
        if cur_block:
            blocks.append("\n".join(cur_block).strip())
        return [b for b in blocks if len(b) > 60]    

    def split_acu_blocks(self,text: str) -> list[str]:
        # ç¯„ä¾‹ä»£ç¢¼ï¼šLIâ€‘11ã€HT-7ã€SI 3
        pattern = re.compile(r"\b([A-Z]{1,2}[ -â€‘]\d{1,3})\b")
        blocks, cur = [], []
        for line in text.splitlines():
            if pattern.search(line):
                if cur: blocks.append("\n".join(cur).strip()); cur = []
            cur.append(line)
        if cur: blocks.append("\n".join(cur).strip())
        return [b for b in blocks if len(b) > 40]

    def process_all(self, csv_path: str, pdf_paths: List[str]):
        """ç¶œåˆè™•ç†ç¤¾ç¾¤ CSV + PDFs"""
        try:
            social_qa_pairs, images = [], []  
            # 1. å¤„ç†ç¤¾ç¾¤æ•°æ®
            if csv_path: 
                social_qa_pairs, images = self.extract_social_posts(csv_path)
                logger.info(f"\nProcessed social data:")
                logger.info(f"- Social QA pairs: {len(social_qa_pairs)}")
                logger.info(f"- Images found: {len(images)}")
            else:
                logger.info("Skip social CSV, onlyè™•ç† PDFs")
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
            valid_images = []
            for img in images:
                img_path = Path(self.embedding_processor.image_dir) / img
                if img_path.exists():
                    valid_images.append(img)
            
            # 2. å¤„ç†æ‰€æœ‰ PDF
            all_professional_pairs = []
            for pdf_path in pdf_paths:
                pdf_name = pdf_path.name.lower()
                for t, keys in TYPE_MAP.items():
                    if any(k in pdf_name for k in keys):
                        row_type = t; break
                else:
                    row_type = "professional"
                pdf_qa_pairs = self.process_pdf(pdf_path, row_type=row_type)
                #pdf_qa_pairs = self.process_pdf(pdf_path)
                all_professional_pairs.extend(pdf_qa_pairs)
                logger.info(f"\nProcessed {Path(pdf_path).name}:")
                logger.info(f"- Extracted paragraphs: {len(pdf_qa_pairs)}")
            
            # 3. åˆå¹¶ => all_qa_pairs
            all_qa_pairs = social_qa_pairs + all_professional_pairs
            
            # 4. æº–å‚™ texts + metadatas => ä½ å°±èƒ½ä¸€æ¬¡æˆ–å¤šæ¬¡å‘¼å« add_vectors
            questions = []
            answers = []
            question_metas = []
            answer_metas = []
            
            # å¤„ç†æ‰€æœ‰é—®ç­”å¯¹
            for qa_pair in all_qa_pairs:
                # question 
                questions.append(qa_pair['question'])
                question_metas.append(qa_pair['metadata'])
                
                # answers
                for ans_text in qa_pair['answers']:
                    answers.append(ans_text)
                    am = qa_pair['metadata'].copy()
                    am['parent_question'] = qa_pair['question']
                    answer_metas.append(am)

            # ------------- é€™è£¡æ‰é–‹å§‹çµ„ professional texts / metas -------------
            prof_texts = [qa["answers"][0] for qa in all_professional_pairs]
            prof_metas = [qa["metadata"]   for qa in all_professional_pairs]

            
            # è¾“å‡ºå¤„ç†ç»“æœ
            logger.info(f"\nFinal processing summary:")
            logger.info(f"- Total questions: {len(questions)}")
            logger.info(f"- Total answers: {len(answers)}")
            logger.info(f"- Valid images: {len(valid_images)}")
            logger.info(f"- Social content: {len(social_qa_pairs)} QA pairs")
            logger.info(f"- Professional content: {len(all_professional_pairs)} paragraphs")
            


            # --------- ğŸ”§ æŠŠ 3 çµ„ metadata éƒ½ä¿è­‰æ˜¯ dict (æ”¾åœ¨æ­¤è™•) ---------
            question_metas = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in question_metas]
            prof_metas     = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in prof_metas]
            # è‹¥è¦ç”¨ answer_metas ä¹Ÿä¸€ä½µè™•ç†
            answer_metas   = [m if isinstance(m, dict) else {"note": str(m)}
                            for m in answer_metas]


            self.embedding_processor.add_vectors(texts=prof_texts,
                                            metadatas=prof_metas)
            
            # (A) å…ˆåŠ æ‰€æœ‰ question
            self.embedding_processor.add_vectors(
                texts = questions,
                metadatas = question_metas
            )

            if valid_images:
                meta_for_imgs = []
                for img_name in valid_images:
                    meta_for_imgs.append({
                        "type":"image",
                        "source":"facebook",
                        "filename": img_name
                    })

                self.embedding_processor.add_vectors(
                    images=valid_images,
                    metadatas=meta_for_imgs
                )

            logger.info("All data added to clip_collection.")
            return len(questions), len(valid_images) #return questions, question_metas, all_professional_pairs, valid_images
                
        except Exception as e:
            logger.error(f"Error processing documents: {str(e)}")
            raise


# #### QA System

# In[10]:


from deep_translator import GoogleTranslator
class QASystem:
    def __init__(self, embedding_processor: 'EmbeddingProcessor',
                 model_name: str = 'llama3.2-vision'):
        self.embedding_processor = embedding_processor
        self.model_name = model_name
        logger.info(f"Initialized QA System with Ollama model: {model_name}")

    def _classify_collection_results(self, raw_result) -> Dict:

        structured = {
            "social": {
                "metadata": [],
                "link": [],
                "content": [],
                "documents":[]
            },
            "professional": {
                "metadata": [],
                "content": [],
                "documents":[]
            },
            "images": {
                "metadata": [],
                "paths": [],
                "relevance":[],
                "description":[]
            },
            
        }

        # raw_result["metadatas"] æ˜¯å€‹ 2D list => [ [meta0, meta1, ...] ]
        if raw_result.get("metadatas"):
            meta_list = raw_result["metadatas"][0]  # å› ç‚ºåªæœ‰1å€‹ query
            dist_list = raw_result["distances"][0] if raw_result.get("distances") else []
            doc_list = raw_result["documents"][0]
            ids_list  = raw_result["ids"][0] if raw_result.get("ids") else []
            # documents_list = raw_result["documents"][0]

            for i, meta in enumerate(meta_list):
                dist = dist_list[i] if i < len(dist_list) else None
                doc_id = ids_list[i] if i < len(ids_list) else ""
                doc_text = doc_list[i] if i < len(doc_list) else ""

                src_type = meta.get("type","")

                if src_type == "social":
                    structured["social"]["metadata"].append(meta)
                    structured["social"]["documents"].append(doc_text)
                elif src_type in ("acupoint", "herb", "ccd", "professional"):
                    structured["professional"]["metadata"].append(meta)
                    structured["professional"]["documents"].append(doc_text)
                elif src_type in ("image", "images", "caption"):
                    structured["images"]["metadata"].append(meta)
                    path = meta.get("path") or meta.get("ref_image", "") or ""
                    structured["images"]["paths"].append(path)
                    structured["images"]["relevance"].append(dist)
                    doc_text = meta.get("content", "")
                    structured["images"]["description"].append(doc_text)

                else:
                    # å°‡æœªçŸ¥ type å…¨ä¸Ÿ professionalï¼Œæˆ–ä¾éœ€æ±‚æ”¹ social
                    meta.setdefault("type", "professional")
                    structured["professional"]["metadata"].append(meta)
                    structured["professional"]["documents"].append(doc_text)

        return structured


    def determine_question_type(self,query: str) -> str:
        """
        å›å‚³: "multiple_choice" | "true_false" | "qa"
        æ”¯æ´ä¸­è‹±æ–‡ & å„ç¨®æ¨™é»
        """
        q = query.strip().lower()

        # --- Multipleâ€‘choice --------------------------------------------------
        # 1) è¡Œé¦–æˆ–æ›è¡Œå¾Œå‡ºç¾  Aï½D / å…¨å½¢ï¼¡ï½ï¼¤ / ã€Œç­”ã€ï¼Œ
        #    å¾Œé¢æ¥ã€€. ï¼ : ï¼š ã€)
        mc_pattern = re.compile(r'(?:^|\n)\s*(?:[a-dï½-ï½„]|ç­”)[:\.ï¼:ï¼šã€\)]', re.I)
        # 2) or å¥å­å¸¶ "which of the following"
        mc_keywords_en = ["which of the following", "which one of the following",
                        "which option", "choose one of"]

        if mc_pattern.search(query) or any(kw in q for kw in mc_keywords_en):
            return "multiple_choice"

        # --- True / False -----------------------------------------------------
        tf_keywords_zh = ["æ˜¯å¦", "æ˜¯å—", "å°å—", "æ­£ç¢ºå—"]
        tf_keywords_en = ['true or false', 'is it', 'is this', 'is that', 
             'is it possible', 'correct or not']

        if any(k in q for k in tf_keywords_zh + tf_keywords_en):
            return "true_false"

        # --- Default ----------------------------------------------------------
        return "qa"

    
    def gather_references(self, search_results: Dict) -> str:
        """
        å¾ search_results ä¸­æ“·å– PDF æª”å/ç¤¾ç¾¤é€£çµï¼Œä¸¦çµ„æˆä¸€å€‹å­—ä¸²
        """
        if not isinstance(search_results, dict):
            logger.error("search_results æ ¼å¼éŒ¯èª¤: %s", type(search_results))
            return ""

        references = []

        # è™•ç† social
        for meta in search_results["social"].get("metadata", []):
            if meta.get("type") == "social_qa" and "link" in meta:
                references.append(f"(ç¶“é©—) {meta['link']}")

        # è™•ç† professional
        for meta in search_results["professional"].get("metadata", []):
            if meta.get("type") in ["pdf", "professional"]:
                pdf_name = meta.get("source_file", "unknown.pdf")
                references.append(f"(æ–‡ç») {pdf_name}")

        # å»é‡
        unique_refs = list(set(references))
        return "\n".join(unique_refs)


    def build_user_prompt(
        self,
        query: str,
        context: str,
        references_str: str = ""
        ) -> str:
        # ä¸å«ä»»ä½•æ ¼å¼è¦ç¯„ï¼åªçµ¦é¡Œç›®èˆ‡è³‡æ–™
        return (
            f"""
            ã€åƒè€ƒè³‡æ–™ã€‘
            {context}\n
            ã€è³‡æ–™ä¾†æºã€‘
            {references_str}\n    
            ã€å•é¡Œã€‘
            {query}\n
            """
            # "åƒè€ƒè³‡æ–™ï¼š\n" + context +
            # "\nä¾†æºï¼š\n" + references_str +
            # "å•é¡Œï¼š"ï¼‹query
        )


    def merge_adjacent(self, metas, docs, k_keep: int = 5) -> str:
        """
        å°‡åŒæª”åŒé ä¸” _id é€£è™Ÿçš„ç‰‡æ®µåˆä½µï¼Œå›å‚³å‰ k_keep æ®µæ–‡å­—ã€‚
        åƒæ•¸
        ----
        metas : list[dict]    # raw_result["metadatas"][0]
        docs  : list[str]     # raw_result["documents"][0]
        """
        ID_NUM_RE = re.compile(r"_(\d+)$")   # å°¾ç¢¼å–æ•¸å­—ï¼štext_123 â†’ 123
        merged, buf = [], ""
        last_src, last_idx = ("", ""), -999

        for md, doc in zip(metas, docs):
            src_key = (md.get("source_file", ""), md.get("page", ""))

            # å– _id å°¾ç¢¼ï¼›è‹¥ä¸å­˜åœ¨å‰‡è¨­ -1
            _id = md.get("_id", "")
            m = ID_NUM_RE.search(_id)
            cur_idx = int(m.group(1)) if m else -1

            # åŒæª”åŒé ä¸”é€£è™Ÿ â†’ è¦–ç‚ºç›¸é„°
            if src_key == last_src and cur_idx == last_idx + 1:
                buf += doc
            else:
                if buf:
                    merged.append(buf)
                buf = doc
            last_src, last_idx = src_key, cur_idx

        if buf:
            merged.append(buf)

        return "\n\n".join(merged[:k_keep])


    def generate_response(self, query: str,question_type: Optional[str] = None) -> Tuple[str, List[str]]:
        try:
            raw_result = self.embedding_processor.similarity_search(
                query,
                k=25) 
            # raw_result = self.embedding_processor.similarity_search_with_images(
            #      query, k_mix=40, img_threshold=0.45, max_images=1)
            
            if not raw_result["documents"] or len(raw_result["documents"][0]) == 0:
                logger.warning("No hits for query â†’ æ”¹ç”¨ k=50 å†è©¦ä¸€æ¬¡")
                raw_result = self.embedding_processor.similarity_search(query, k=50)

            if not raw_result["documents"] or len(raw_result["documents"][0]) == 0:
                return "[NoRef] ç„¡è¶³å¤ è­‰æ“šåˆ¤æ–·", [],[]
            
            # ç”¨å¾Œè™•ç†
            search_results = self._classify_collection_results(raw_result)
            logger.info("SEARCH RESULT(structured): %s",search_results)

            context = self.merge_adjacent(raw_result["metadatas"][0],
                              raw_result["documents"][0])[:1500]

            context = context[:1500]          # æœ€å¤š 1500 å­—

            references_str = self.gather_references(search_results)
            # linkæ‡‰è©²ç”¨å‚³åƒæ•¸çš„æœƒæˆåŠŸ å¯èƒ½ç”¨context.linkä¹‹é¡çš„æŠ“é¡Œç›®çš„reference

            # --- â‘  é¡Œå‹ --------------------------------------------------------
            q_type = question_type or self.determine_question_type(query)
            # å–å‰ 2 å¼µåœ–çš„ caption
#             caption_snips = [
#                 m.get("content", "")[:120]
#                 for m in search_results["images"]["metadata"][:1]
#                 if m.get("content")
#             ]
#             caption_block = "\n".join(caption_snips)

#             user_prompt = self.build_user_prompt(
#                 query=query,
#                 context=caption_block + "\n" + context[:1500],
#                 references_str=references_str
# )

            user_prompt = self.build_user_prompt(
                query=query,
                context=context[:1500],
                references_str=references_str
            )

            # ---------- â‘¡ æ ¹æ“šé¡Œå‹å‹•æ…‹çµ„ system æŒ‡ä»¤ ----------
            if q_type == "multiple_choice":
                format_rules = (
                    "é€™æ˜¯ä¸€é¡Œé¸æ“‡é¡Œï¼Œå›ç­”æ ¼å¼å¦‚ä¸‹ï¼š\n"
                    "å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\n"
                    "ç”¨ 2-3 å¥è©±èªªæ˜ç†ç”±ã€‚\n"
                    "æœ€å¾Œå†çµ¦å‡ºç­”æ¡ˆï¼Œåªèƒ½å›ç­” A/B/C/D (è«‹å‹¿å¸¶ä»»ä½•æ¨™é»ã€æ–‡å­—ã€ä¹Ÿä¸è¦åªå›ç­”é¸é …å…§å®¹)\n"
                    "è‹¥åŒæ™‚å‡ºç¾å¤šå€‹é¸é …ï¼Œè«‹åªé¸ä¸€å€‹æœ€é©åˆçš„\n"
                )
            elif q_type == "true_false":
                format_rules = (
                    "é€™æ˜¯ä¸€é¡Œæ˜¯éé¡Œï¼Œè«‹æŒ‰ç…§ä¸‹åˆ—æ ¼å¼å›ç­”ï¼š\n"
                    "å…ˆæ ¹æ“šé¡Œç›®æ•´ç†åƒè€ƒè³‡è¨Šã€ä½ çš„ç†è§£èˆ‡å¸¸è­˜\n"
                    "æœ€å¾Œå†çµ¦å‡ºç­”æ¡ˆï¼Œåªèƒ½å¯«ã€ŒTrueã€æˆ–ã€ŒFalseã€\n"
                )
            else:   # qa
                format_rules = (
                    "è«‹ä¾ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n"
                    "é‡å°å•é¡Œæä¾›å…·é«”ç­”æ¡ˆä¸¦è©³ç´°èªªæ˜ \n"
                )

            #"æ‚¨æ˜¯ä¸€åå°ˆæ¥­ç¸é†«ï¼Œ1.æ“…é•·çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ç¶œåˆç—‡ï¼ˆCCDï¼‰çš„è¨ºæ–·å’Œè­·ç† 2.æ“æœ‰è±å¯Œçš„å¯µç‰©ä¸­é†«çŸ¥è­˜ 3.å¸¸è¦‹å•é¡Œè¨ºæ–·åŠæ”¹å–„å»ºè­°\n" "è«‹åœ¨ç­”æ¡ˆæœ€å¾Œé¡¯ç¤ºä½ åƒè€ƒçš„ä¾†æºé€£çµæˆ–è«–æ–‡åç¨±ï¼Œå¦‚æœä¾†æºä¸­åŒ…å«ã€Œ(ç¶“é©—) some_linkã€ï¼Œè«‹åœ¨å›ç­”ä¸­ä»¥ [Experience: some_link] å½¢å¼æ¨™ç¤ºï¼›è‹¥åŒ…å«ã€Œ(æ–‡ç») some.pdfã€ï¼Œå°± [reference: some.pdf]\n""å¦‚æª¢ç´¢çµæœä»ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹ä»¥[NoRef]æ¨™ç¤ºä¸¦æ ¹æ“šä½ çš„å¸¸è­˜å›ç­”ã€‚\n"
            system_prompt = (
                """ä½ æ˜¯è³‡æ·±ç¸é†«ï¼Œæ“…é•·çŠ¬èªçŸ¥åŠŸèƒ½éšœç¤™ç¶œåˆç—‡ï¼ˆCCDï¼‰çš„è¨ºæ–·å’Œè­·ç†ä¸¦æ“æœ‰è±å¯Œçš„å¯µç‰©ä¸­é†«çŸ¥è­˜ï¼Œå¿…é ˆéµå®ˆä»¥ä¸‹è¦å‰‡å›ç­”å•é¡Œï¼š
                    1. å…ˆç†è§£å•é¡Œï¼Œåˆ¤æ–·è§£é¡Œæ‰€éœ€è³‡è¨Šï¼Œä¸¦æ ¹æ“šã€æª¢ç´¢çµæœã€‘å…§å®¹æ‰¾å°‹ç›¸é—œè³‡æ–™ï¼Œè‹¥æœ‰ç›¸é—œè«‹æ¡ç”¨ä¸¦ä»¥æª¢ç´¢çµæœç‚ºæº–
                    2. è‹¥è³‡è¨Šä¸ç›¸é—œï¼Œå°±ä¸è¦ç†æœƒæª¢ç´¢å…§å®¹ï¼Œä¾ç…§ä½ çš„å¸¸è­˜å›ç­”ã€‚
                    3. è‹¥æª¢ç´¢çµæœä¸­å«æœ‰åœ–ç‰‡ï¼Œåœ¨å›ç­”ä¸­è‡ªç„¶çš„å¸¶å…¥åœ–ç‰‡èªªæ˜
                    4. è‹¥éœ€è£œå……ä¸€èˆ¬è‡¨åºŠå¸¸è­˜ï¼Œè«‹å°‡è©²å¥æ”¾åœ¨æ®µè½æœ€å¾Œä¸¦æ¨™è¨»ï¼»å¸¸è­˜ï¼½ã€‚
                    5. æ¯ä¸€å¥çµå°¾å¿…é ˆæ¨™è¨»å¼•ç”¨ä¾†æºç·¨è™Ÿï¼Œå¦‚ï¼»1ï¼½æˆ–ï¼»1,3ï¼½ã€‚
                    6. ä¸¦åœ¨æœ€å¾Œé¢æ•´ç†åˆ—å‡ºæ¯å€‹ç·¨è™Ÿçš„source_fileï¼Œå¦‚[1] ...pdf æˆ– [2] Chinese Veterinary Materia Medica """
                + format_rules
            )
        

            # è™•ç†åœ–ç‰‡
            image_paths = []
            for md in search_results.get("social", {}).get("metadata", []):
                if md.get("images"):
                    for img_name in md["images"].split(","):
                        full_path = self.embedding_processor.image_dir / img_name.strip()
                        if full_path.exists():
                            image_paths.append(str(full_path.resolve()))
            for mm in search_results.get("images", {}).get("metadata", []):#["images"]["metadata"]:
                p = mm.get("path") or mm.get("ref_image")
                if p and (self.embedding_processor.image_dir / p).exists():
                    image_paths.append(str((self.embedding_processor.image_dir / p).resolve()))
            # 3) OLlama åªå…è¨±ä¸€å¼µ, ä½ å¯å– image_paths[:1] => message["images"] = ...
            if image_paths:
                print("We found images: ", image_paths)
            else:
                logger.info("No images to display")
            


            message = [
                {"role": "system", "content": system_prompt},
                {"role": "user",   "content": user_prompt},
                # {'images': image_paths[:1]}
            ]
            # print("=======sys prompt =======",system_prompt)
            # print("=======user prompt =======",user_prompt)
            # ç”Ÿæˆå“åº”
            response = ollama.chat(
                model=self.model_name,
                messages=message
            )


            # å–å¾—æª¢ç´¢æ®µè½ï¼ˆæ–‡å­—å³å¯ï¼‰
            retrieved_contexts = search_results["professional"]["documents"] + \
                                search_results["social"]["documents"]

            # æŠŠä¸‰æ¨£éƒ½å›å‚³ ----------------------------------â–¼ æ–°å¢
            response_text = response["message"]["content"]

            return response_text, retrieved_contexts, image_paths #response['message']['content'], image_paths

        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            # return f"å‡ºç¾å•é¡Œï¼Œæª¢æŸ¥ollamaé€£ç·šæˆ–æ˜¯generate_response", []
            raise

    def format_context(self, search_results: Dict) -> str:
        """Format context from search results"""
        try:
            context = ""

            # 1) è™•ç†ç¤¾ç¾¤è¨è«–
            social_metas = search_results["social"].get("metadata", [])
            social_links = search_results["social"].get("link", [])
            social_docs = search_results["social"].get("documents", [])
            social_content = search_results["social"].get("content", [])

            if social_metas or social_links or social_docs:
                context += "\n[ç¤¾ç¾¤è¨è«–]\n"
                # é€™è£¡ç¤ºç¯„æŠŠ linkã€documents éƒ½è¼¸å‡º
                for i, meta in enumerate(social_metas):
                    link_str = meta.get("link", "")
                    doc_text = social_docs[i] if i < len(social_docs) else ""
                    context += f"ã€Linkã€‘{link_str}\n" if link_str else ""
                    # doc_text å°±æ˜¯æª¢ç´¢å›ä¾†çš„ chunk
                    context += f"ã€è¨è«–ç‰‡æ®µã€‘{doc_text}\n\n"

            # 2) è™•ç†å°ˆæ¥­æ–‡ç»
            prof_metas = search_results["professional"].get("metadata", [])
            prof_docs = search_results["professional"].get("documents", [])

            if prof_metas or prof_docs:
                context += "\n[å°ˆæ¥­æ–‡ç»]\n"
                for j, meta in enumerate(prof_metas):
                    source_file = meta.get("source_file", "")
                    doc_text = prof_docs[j] if j < len(prof_docs) else ""
                    # å¦‚æœæ‚¨æœ‰å¦å¤–å­˜æ”¾é ç¢¼ page = meta.get("page"), ä¹Ÿå¯åˆ—å‡º
                    page_num = meta.get("page", "")
                    context += f"ã€æ–‡ä»¶ç‰‡æ®µã€‘{doc_text}\n"
                    if source_file:
                        context += f"(æª”æ¡ˆ: {source_file}"
                        context += f", é : {page_num})" if page_num else ")"
                    context += "\n\n"

            # åµéŒ¯ç”¨ (å¯ä¿ç•™ä¹Ÿå¯ç§»é™¤)
            print("social metadata:", social_metas)
            print("social links:", social_links)
            print("professional metadata:", prof_metas)

            return context if context.strip() else "åƒè€ƒè³‡æ–™ç„¡æ³•å–å¾—"

        except Exception as e:
            logger.error(f"Error formatting context: {str(e)}")
            return "Unable to retrieve reference materials"


    def display_response(self, query: str,question_type: Optional[str] = None):
            """Display response with text and images"""
            try:
                logger.info("Starting to generate response...")
                try:
                    response_text, _ , image_paths = self.generate_response(query,question_type)
                except Exception as e:
                    response_text = f"[ERROR] {e}"
                    image_paths = []
                
                print("Question:", query)
                print("\nSystem Response:")
                print(response_text)
                print("\n" + "-"*50 + "\n")

                if image_paths:
                    print("\nRelated Image:")
                    img_path = image_paths[0]  # We now only have one image
                    try:
                        img = PILImage.open(img_path)
                        display(IPyImage(filename=img_path))
                    except Exception as e:
                        logger.error(f"Error displaying image {img_path}: {str(e)}")
                else:
                    logger.info("No images to display")


                return response_text, image_paths # add for response 0406
                    
            except Exception as e:
                logger.error(f"Error in display_response: {str(e)}", exc_info=True)  
                return "", [] 


# ### é¡Œç›®æ¸¬è©¦

# Embedding processor

# In[11]:


from pathlib import Path
TEST_MODE = False                           # â† åˆ‡æ›é–‹é—œ
COLLECTION_NAME = "clip_collection_0504"

# 1) åˆå§‹åŒ– embedding_processorï¼Œå‚³å…¥æ–°çš„ collection_name
embedding_processor = EmbeddingProcessor(
    image_size=(224, 224) ,
    collection_name=COLLECTION_NAME,    # â˜…è‹¥ __init__ æ²’é€™åƒæ•¸ï¼Œæ”¹ä¸‹æ–¹è¨»è§£æ–¹æ³•
    reset=False
)


# ##### é‡å»ºDB

# In[205]:


# 2) åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨
data_processor = DataProcessor(embedding_processor)

# 3) æŒ‡å®šæ¸¬è©¦æˆ–æ­£å¼è³‡æ–™å¤¾
rag_data_dir = Path("RAG_data_test" if TEST_MODE else "RAG_data")
pdf_paths = list(rag_data_dir.glob("*.pdf"))


print("æ‰¾åˆ°ä»¥ä¸‹ PDFï¼š")
for p in pdf_paths: print(" -", p.name)

# 4) è™•ç†è³‡æ–™ ï¼ˆCSV ä½ å¯ä»¥å‚³ None ä»£è¡¨ä¸è™•ç†ç¤¾ç¾¤è³‡æ–™ï¼‰
_ = data_processor.process_all(
    csv_path="post_response_filtered.xlsx",           # åªæ¸¬ PDFï¼Œå¯å…ˆä¸ç®¡ç¤¾ç¾¤
    pdf_paths=pdf_paths
)


# ##### Initialized QA System

# In[12]:


# å»ºç«‹ QA ç³»çµ±ï¼Œæ²¿ç”¨åŒä¸€å€‹ embedding_processor
qa_system = QASystem(
    embedding_processor=embedding_processor,
    model_name='llama3.2-vision'
)


# In[237]:


qa_system.display_response("What does ç”˜è‰GanCao look like?")


# In[261]:


qa_system.display_response("the benefits of MCT Oil")


# In[235]:


emb = embedding_processor.encode_text_to_vec("What does RenShen look like?")
hits = embedding_processor.clip_collection.query(
            query_embeddings=[emb],
            n_results=10,
            where={"type": {"$in": ["image", "images","caption"]}},   # â˜…åªè¦åœ–åƒé¡
            include=["metadatas", "documents", "distances"])
print(hits["metadatas"][0])


# In[227]:


raw = qa_system.embedding_processor.similarity_search("ç”˜è‰", k=25)
print(raw["metadatas"][0][0])     # æ‡‰è©²çœ‹åˆ° {'type': 'caption', 'ref_image': 'GanCao.png', ...}


# ##### åˆ¤æ–·æ­£ç¢ºç­”æ¡ˆ

# In[270]:


import re

def parse_llm_answer(resp: str, q_type: str) -> str:
    """
    è§£æ LLM å›ç­”æ–‡å­—ï¼Œå›å‚³æœ€çµ‚ç­”æ¡ˆï¼š
      â€¢ multiple_choice â†’ 'A'|'B'|'C'|'D'|'UNK'
      â€¢ true_false      â†’ 'TRUE'|'FALSE'|'UNK'
    """
    txt = resp.lower()
    txt = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼›ï¼š\s]+', ' ', txt)        # å…ˆçµ±ä¸€ç©ºç™½

    if q_type == "multiple_choice":
        # æ‰¾æ‰€æœ‰ã€Œç¨ç«‹ã€çš„ a-d (å«å¤§å°å¯«)ï¼Œä¸å« 'é¸é …a' é€™ç¨®çµ„å­—
        matches = re.findall(r'\b([abcd])\b', txt, flags=re.I)
        return matches[-1].upper() if matches else "UNK"

    elif q_type == "true_false":
        # æ‰¾æ‰€æœ‰ true/false / å°/éŒ¯ / æ˜¯/å¦
        tf_matches = re.findall(
            r'\b(true|false|æ­£ç¢º|éŒ¯èª¤|å°|éŒ¯|æ˜¯|å¦)\b', txt)
        if not tf_matches:
            return "UNK"
        last = tf_matches[-1]
        return "TRUE" if last in ("true", "æ­£ç¢º", "å°", "æ˜¯") else "FALSE"

    else:   # å…¶é¤˜é¡Œå‹åŸæ–‡è¿”å›
        return resp


# æ¸¬è©¦

# In[351]:


# 1. è®€æª” + é¡Œå‹ç¯©é¸
df = pd.read_excel("test_questions_en_fixed.xlsx")#test_questions_en #test_questions_withANS
test_df = df[df["type"].isin(["multiple_choice", "true_false"])].copy()
# test_df = df[df["type"].isin(["qa"])].copy()

# 2. â˜… å»ºç«‹æ¬„ä½ï¼ˆä¸€å®šè¦åœ¨å¾Œé¢çš„ç¯©é¸å‰å…ˆåŠ ï¼‰
test_df["llm_response"] = ""
test_df["predicted"]    = ""
test_df["is_correct"]   = 0

# 3. å†ä¾ domain ç¯©å­é›†åˆ
test_df = test_df[test_df["domain"] == "ä¸­é†«"].copy()
test_df=test_df.head(10)


# In[ ]:


test_df


# In[352]:


dataset = [] # for ragas

# 4. è¿´åœˆè¨ˆåˆ†
for idx, row in test_df.iterrows():
    q_row  = row["query_for_embed"]#["question_en"]
    q = expand_query(q_row)
    q_type = row["type"]
    gt = str(row["answers"]).strip()
    ref_ctx   = [ str(row["RAG"]) ] 

    resp, _ = qa_system.display_response(q, q_type)

    if not resp.strip():
        print(f"[WARN] id={row['id']}  LLM å›å‚³ç©ºç™½")


    resp, ctxs, _ = qa_system.generate_response(q, q_type)
    pred = parse_llm_answer(resp, q_type)

    test_df.at[idx, "llm_response"] = resp
    test_df.at[idx, "predicted"]    = pred
    test_df.at[idx, "is_correct"]   = int(pred.upper() == gt.upper())
    
    ctxs = [str(c) for c in ctxs]

    dataset.append({
        "user_input":           str(q),           # question
        "response":             str(resp),        # llm response
        "retrieved_contexts":   ctxs,             # llmæª¢ç´¢åˆ°çš„è³‡æ–™
        "reference_contexts":   ref_ctx,          # å‡ºé¡Œæ®µè½
        "reference":            gt                # answers
    })


# In[353]:


# 5. è¨ˆç®— Accuracy
overall_acc = test_df["is_correct"].mean()

print("\n=== æ¯å€‹ domain çš„ Accuracy ===")
domain_stats = (
    test_df.groupby("domain")["is_correct"]
           .agg(["count", "sum"])
           .reset_index()
           .rename(columns={"sum": "correct"})
)
domain_stats["accuracy"] = domain_stats["correct"] / domain_stats["count"]
print(domain_stats.to_string(index=False, 
      formatters={"accuracy": "{:.2%}".format}))

print(f"\nOVERALL Accuracy = {overall_acc:.2%}")


# In[354]:


# å…ˆæŒ‘å‡ºç­”éŒ¯çš„è³‡æ–™åˆ—
wrong_df = (
    test_df.loc[test_df["is_correct"] == 0,
                ["id", "question", "answers", "predicted"]]
            .sort_values("id")          # ä¾é¡Œè™Ÿæ’åºæ–¹ä¾¿æŸ¥çœ‹
)

print("=== ç­”éŒ¯é¡Œç›®ä¸€è¦½ ===")
print(wrong_df.to_string(index=False))


# In[337]:


# ==== å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾ ====
from pathlib import Path
OUT_DIR = Path("results")
OUT_DIR.mkdir(exist_ok=True)

# ==== å­˜æª” ====
csv_path = OUT_DIR / "RAG_results.csv"
test_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
print(f"âœ… RAG çµæœå·²å­˜åˆ° {csv_path}")


# #### æª¢æŸ¥

# In[368]:


# æ‹‰å¤§ k çœ‹æ’å
hits = qa_system.embedding_processor.similarity_search("Wan Ying San", k=200)
for i, txt in enumerate(hits, 1):
    if "Wan Ying San" in txt:
        print("rank =", i)
        print(txt[:120], "...\n")
        break


# In[27]:


from rank_bm25 import BM25Okapi
from chromadb import PersistentClient
client = PersistentClient(path="./chroma_db")
col    = client.get_collection("clip_collection_0504")

all_texts = col.get(include=["documents"])["documents"]
# â”€â”€ å»º BM25 ç´¢å¼•ï¼ˆä¸€æ¬¡å³å¯ï¼‰ â”€â”€

bm25      = BM25Okapi([t.lower().split() for t in all_texts])
embedder = qa_system.embedding_processor.encode_text_to_vec

def hybrid_search(query, k_dense=100, k_final=8):
    q_vec   = embedder(query)
    dense   = qa_system.embedding_processor.similarity_search(query, k=k_dense)
    lexical = bm25.get_top_n(query.lower().split(), all_texts, n=k_dense)

    scores = {}
    for t in dense:   scores[t] = scores.get(t, 0) + 1          # dense +1
    for t in lexical: scores[t] = scores.get(t, 0) + 1.5        # BM25 +1.5

    top = sorted(scores, key=scores.get, reverse=True)[:k_final]
    return top


# In[28]:


top_docs = hybrid_search("Wan Ying San", k_dense=100)
print(top_docs[0])        # æ‡‰è©²å°±èƒ½çœ‹åˆ° Always Responsive å¥


# In[339]:


# fix_questions.py  â”€â”€ ç›´æ¥ python fix_questions.py å³å¯
import re, json, pandas as pd

# === 0. è®€æª” ===
df = pd.read_excel("test_questions_en.xlsx")

# === 1. ä¿®å¾© OCR æ–·å­—ï¼ˆe.g. "Defi ciency" -> "Deficiency") ===
def fix_split_words(text:str) -> str:
    # è‹±æ–‡å­—æ¯ä¸­é–“åªè¦æ˜¯å–®ä¸€ç©ºç™½ä¸”å…©å´çš†å°å¯«å°±è¦–ç‚ºæ–·å­—
    return re.sub(r'([a-z])\s+([a-z])', r'\1\2', str(text), flags=re.I)
df["question_en"] = df["question_en"].apply(fix_split_words)

# === 2. å¥—ç”¨äººå·¥ç¿»è­¯æ›´æ­£ ===
manual_patch = {
    14: """In the composition of a Chinese herbal formula, which of the following is NOT a function of the â€œAdjuvant (Zuo)â€ herb?
A) Address the minor cause of a disease or a secondary Pattern
B) Suppress the toxicity or overly harsh action of the King/Minister herbs
C) Assist or enhance the action of the King herb
D) Balance the overall energy of the whole prescription""",
    15: '"Ge Jie San" is an important classical formula for treating chronic cough in horses caused by Lung Yin and Kidney Qi deficiency.',
    16: """In the clinical study of the modified â€œDi Tan Tangâ€ for treating hyperlipidemia, was the lipid-lowering effect of the treatment group significantly different from the control group?
A) Not significant (P > 0.05)
B) Significant (P < 0.01)
C) Significance level not provided
D) This clinical study is not mentioned in the sources"""
}
df["question_en"] = df.apply(lambda r: manual_patch.get(r["id"], r["question_en"]), axis=1)

# === 3. å»ºç«‹æœ€å° alias å­—å…¸ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰ ===
alias_dict = {
    "LI-11": ["LI-11", "Large Intestine 11", "Quchi", "æ›²æ± "],
    "HT-7" : ["HT-7", "Heart 7", "Shenmen", "ç¥é–€"],
    "Bai He": ["Bai He", "Lily bulb", "ç™¾åˆ"],
    "Di Gu Pi": ["Di Gu Pi", "Lycium Root Bark", "åœ°éª¨çš®", "æ¸ææ ¹çš®"],
    # â€¦â€¦è‡ªè¡ŒåŠ ç¢¼â€¦â€¦
}

# === 4. åœ¨é€é€² embed å‰è‡ªå‹•æŠŠ alias è²¼åˆ° query ===
def expand_query(q:str) -> str:
    q_low = q.lower()
    for alts in alias_dict.values():
        if any(a.lower() in q_low for a in alts):
            q += " " + " ".join(alts)   # ç­‰æ–¼ OR æŸ¥è©¢
    return q

df["query_for_embed"] = df["question_en"].apply(expand_query)

# === 5. è¼¸å‡ºä¿®æ­£ç‰ˆ ===
out_path = "test_questions_en_fixed.xlsx"
df.to_excel(out_path, index=False)
print(f"âœ…  å·²å„²å­˜ï¼š{out_path}")


# In[344]:


q_test = "Where is LI-11 located?"
print(expand_query(q_test))


# In[ ]:


caps = ep.clip_collection.get(
    where   = {"type":"caption"},
    include = ["metadatas"]
)
print("caption å‘é‡æ•¸ :", len(caps["metadatas"]))
print("å‰ 3 ç­†ç¤ºä¾‹    :", [m.get("ref_image") for m in caps["metadatas"][:3]])


# åŠ å…¥è‰è—¥åœ–ç‰‡

# In[226]:


# === åŒ¯å…¥è…³æœ¬ =========================================
from pathlib import Path
import pandas as pd, uuid, os

ep      = qa_system.embedding_processor
df      = pd.read_excel("herb_image_manifest.xlsx")

img_paths, cap_texts, all_metas = [], [], []

for _, row in df.iterrows():
    # 1. åœ–ç‰‡ -------------------------------------------------
    img_file = Path(row["filename"]).name          # åªç•™æª”å

    img_paths.append(str(img_file))
    all_metas.append({
        "id"      : str(uuid.uuid4()),             # å”¯ä¸€ id
        "type"    : "image",
        "category": "herb",
        "path"    : img_file                       # æ²’æœ‰ image/
    })

    # 2. Caption ---------------------------------------------
    cap_texts.append(f"{row['herb_name']} : {row['caption']}")
    all_metas.append({
        "id"        : str(uuid.uuid4()),
        "type"      : "caption",
        "category"  : "herb",
        "ref_image" : img_file
    })

print(f"åŒ¯å…¥ {len(img_paths)} å¼µåœ–ã€{len(cap_texts)} å‰‡ captionâ€¦")
ep.add_vectors(images=img_paths, texts=cap_texts, metadatas=all_metas)
print("âœ… å®Œæˆ")


# ç¤¾ç¾¤åœ–ç‰‡

# In[ ]:


# === åŒ¯å…¥ç¤¾ç¾¤åœ–ç‰‡ï¼ˆæœ‰ image_description æ‰åŒ¯å…¥ï¼‰ =============
from pathlib import Path
import pandas as pd, uuid, requests, shutil, os

ep        = qa_system.embedding_processor
IMG_DIR   = Path("image")                 # èˆ‡è‰è—¥åœ–å…±ç”¨åŒä¸€è³‡æ–™å¤¾
IMG_DIR.mkdir(exist_ok=True)

df = pd.read_excel("post_response_filtered.xlsx")

img_paths, img_metas = [], []
cap_texts, cap_metas = [], []

def download_to_image(url: str) -> str:
    """Download url to image/ and return filename; raise if not jpg/png."""
    fname = url.split("/")[-1].split("?")[0]
    if not fname.lower().endswith((".jpg", ".png")):
        raise ValueError("é jpg / png æª”ï¼Œè·³é")
    local = IMG_DIR / fname
    if not local.exists():
        r = requests.get(url, timeout=10, stream=True)
        r.raise_for_status()
        with local.open("wb") as f:
            shutil.copyfileobj(r.raw, f)
    return fname               # åªå›æª”å

for _, row in df.iterrows():
    # --------------- 1. å…ˆæª¢æŸ¥ caption --------------------------
    cap = str(row.get("image_descrption", "")).strip()
    if cap == "" or cap.lower() == "nan":
        continue  # å¿…é ˆæœ‰ image_description æ‰åŒ¯å…¥

    # --------------- 2. è§£æ images æ¬„ --------------------------
    raw = str(row["images"]).strip()
    if raw == "" or raw.lower() == "nan":
        continue

    try:
        if raw.startswith("http"):
            img_file = download_to_image(raw)            # â†’ ä¸‹è¼‰
        else:
            img_file = Path(raw).name
            if not img_file.lower().endswith((".jpg", ".png")):
                continue
            if not (IMG_DIR / img_file).exists():
                print(f"âš ï¸ æ‰¾ä¸åˆ°æœ¬æ©Ÿæª”ï¼š{IMG_DIR/img_file}")
                continue
    except Exception as e:
        print(f"âŒ è·³é {raw}ï¼ŒåŸå› ï¼š{e}")
        continue

    # --------------- 3. åœ–ç‰‡å‘é‡ meta --------------------------
    img_paths.append(str(img_file))            # çœŸå¯¦è·¯å¾‘
    img_metas.append({
        "id":       str(uuid.uuid4()),
        "type":     "image",
        "category": "social_img",
        "path":     img_file,                            # åªå­˜æª”å
        "caption":  cap,
        "post_id":  row.get("post_id", "")
    })

    # --------------- 4. caption æ–‡å­—å‘é‡ -----------------------
    cap_texts.append(cap)
    cap_metas.append({
        "id":        str(uuid.uuid4()),
        "type":      "caption",
        "category":  "social_img",
        "ref_image": img_file,
        "post_id":   row.get("post_id", "")
    })

print(f"ğŸ“¸  å³å°‡åŒ¯å…¥ï¼š{len(img_paths)} å¼µåœ–ã€{len(cap_texts)} å‰‡ caption")

# --------------- 5. å¯«å…¥ Chroma ------------------------------
if img_paths:
    ep.add_vectors(images=img_paths, metadatas=img_metas)
if cap_texts:
    ep.add_vectors(texts=cap_texts,  metadatas=cap_metas)

print("âœ… ç¤¾ç¾¤åœ–ç‰‡èˆ‡ caption å·²å®ŒæˆåŒ¯å…¥")


# In[233]:


ep = qa_system.embedding_processor
df = pd.read_excel("herb_image_manifest.xlsx")

img_paths, img_metas = [], []
cap_texts, cap_metas = [], []

for _, row in df.iterrows():
    img_file = Path(row["filename"]).name

    # 1) åœ–ç‰‡ --------------------------
    img_paths.append(img_file)
    img_metas.append({
        "id":       str(uuid.uuid4()),
        "type":     "image",
        "category": "herb",
        "herb":     row["herb_name"],
        "path":     img_file,
        "caption":  row["caption"]
    })

    # 2) caption æ–‡å­— -------------------
    cap_texts.append(f"{row['herb_name']} : {row['caption']}")
    cap_metas.append({
        "id":        str(uuid.uuid4()),
        "type":      "caption",
        "category":  "herb",
        "herb":      row["herb_name"],
        "ref_image": img_file
    })

print(f"åŒ¯å…¥ {len(img_paths)} å¼µåœ–ï¼Œ{len(cap_texts)} å‰‡ caption")

# âœ¦ å…ˆåŒ¯åœ–ç‰‡
ep.add_vectors(images=img_paths, metadatas=img_metas)

# âœ¦ å†åŒ¯ caption æ–‡å­—
ep.add_vectors(texts=cap_texts,  metadatas=cap_metas)

print("âœ… å®Œæˆï¼Œåœ–ç‰‡ & caption å·²æ­£ç¢ºå°é½Š")


# æª¢æŸ¥è³‡æ–™åº«è£¡çš„type

# In[287]:


docs = clip.get(limit=50000, include=["metadatas"])
print(set(m.get("type") for m in docs["metadatas"]))
# é æœŸè¼¸å‡ºï¼š{'image', 'caption', 'acupoint'}


# åœ–ç‰‡æœå°‹æ¸¬è©¦

# In[ ]:


# â‘  ç›´æ¥æ‰‹å‹• query clip_collection
vec = ep.encode_text_to_vec("gancao")
raw = clip.query(query_embeddings=[vec], n_results=100, include=["distances","metadatas"])
print(len(raw["metadatas"][0]))
for md, dist in zip(raw["metadatas"][0][:100], raw["distances"][0][:100]):
    print(dist, md.get("type"), md.get("path") or md.get("ref_image"))


# åˆªé™¤ç‰¹å®šè³‡æ–™

# In[243]:


import chromadb

COLLECTION = "clip_collection_0504"
client = chromadb.PersistentClient(path="chroma_db")
coll   = client.get_collection(COLLECTION)

# where æ”¯æ´ $inï¼Œç›´æ¥æŠŠä¸‰ç¨® type ä¸€å£æ°£åˆªå…‰
coll.delete(where={"category": {"$in": ["social_img"]}})
print("âœ… å·²æ¸…é™¤æ‰€æœ‰åœ–ç‰‡ï¼caption å‘é‡")


# åˆªé™¤DB

# In[220]:


client = chromadb.PersistentClient(path="chroma_db")
try:
    client.delete_collection("clip_collection_0509")
    print("å·²åˆª")
except (chromadb.errors.NotFoundError, ValueError):
    pass


# #### RAGAS

# https://docs.ragas.io/en/stable/

# In[ ]:


from dotenv import load_dotenv
import os

env_path = Path("key") / ".env"
load_dotenv(dotenv_path=env_path, override=False)

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("æ‰¾ä¸åˆ° OPENAI_API_KEY")


# è·‘è©•åˆ†

# In[ ]:


from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI  # langchain>=0.1
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0)
evaluator_llm = LangchainLLMWrapper(llm)


# In[ ]:


from ragas import EvaluationDataset
evaluation_dataset = EvaluationDataset.from_list(dataset)


# In[ ]:


from ragas import evaluate
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness,ContextPrecision

result = evaluate(
    dataset=evaluation_dataset,
    metrics=[
        LLMContextRecall(),
        ContextPrecision(),
        # Faithfulness(),      #only QA å¿ å¯¦åº¦
        # FactualCorrectness(), #only QA æ­£ç¢ºæ€§
    ],
    llm=evaluator_llm
)


# In[ ]:


result


# çµæœå­˜æª”

# In[ ]:


# ==== é€™æ®µåŠ åœ¨ 5. è¨ˆç®— Accuracy å‰å¾Œçš†å¯ ====
from pathlib import Path, PurePath
OUT_DIR = Path("results")
OUT_DIR.mkdir(exist_ok=True)

# åˆ¤æ–·ç›®å‰è·‘çš„æ˜¯å“ªä¸€ç¨®æ¨¡å¼
# ä½ å¯ä»¥ç”¨ flag æˆ–ç°¡å–®ç”¨æª”åæ‰‹å‹•åˆ†æµ
RUN_TAG = "rag_text"   # æˆ– "rag_mm"ã€"rag_mc_tf" â€¦è‡ªå·±å®šç¾©

# 1) ä¸»çµæœ CSV
csv_path = OUT_DIR / f"{RUN_TAG}_results.csv"
test_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# 2) RAGAS JSONï¼ˆè‹¥æœ‰è·‘ evaluateï¼‰
if 'result' in globals():          # ç¢ºå®š evaluate() å·²åŸ·è¡Œ
    import json
    json_path = OUT_DIR / f"{RUN_TAG}_ragas.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

print(f"âœ… {RUN_TAG} çµæœå·²å­˜åˆ° {csv_path}")
if 'result' in globals():
    print(f"âœ… RAGAS åˆ†æ•¸å·²å­˜åˆ° {json_path}")

