{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Vector DB\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Embedding Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# llama\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# PDF處理\n",
    "import PyPDF2\n",
    "\n",
    "import ollama\n",
    "# from PIL import Image\n",
    "# from IPython.display import display, Image as IPyImage\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 檢查並創建必要的目錄\n",
    "Path('chroma_db').mkdir(exist_ok=True)\n",
    "Path('image').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:55:20) \n",
      "[Clang 16.0.6 ]\n",
      "PyTorch version: 2.5.1\n",
      "Transformers version: 4.47.0\n",
      "Accelerate version: 0.26.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voice to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper /Users/zirong/Desktop/test.mp4 --language Chinese --model tiny\n",
    "import whisper\n",
    "def transcribe_file(file_path, model_size=\"base\"):\n",
    "    model = whisper.load_model(model_size)\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# def main():\n",
    "#     audio_file = \"no_upload/test_mp3/01.mp3\"  # 修改為你的音檔路徑\n",
    "#     transcription = transcribe_file(audio_file)\n",
    "#     print(\"Transcription:\", transcription)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 圖片處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union  # 添加 Union 导入\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, image_dir: str = \"image\"):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def process_and_save(\n",
    "        self,\n",
    "        image_path: Union[str, Path],  # 使用 Union 替代 |\n",
    "        target_size: Tuple[int, int],\n",
    "        prefix: str = \"resized_\",\n",
    "        quality: int = 95\n",
    "    ) -> Optional[Path]:\n",
    "        \"\"\"统一的图片处理方法，处理并保存图片\"\"\"\n",
    "        try:\n",
    "            # 确保 image_path 是 Path 对象\n",
    "            image_path = Path(image_path)\n",
    "            if not str(image_path).startswith(str(self.image_dir)):\n",
    "                image_path = self.image_dir / image_path\n",
    "                \n",
    "            # 检查图片是否存在\n",
    "            if not image_path.exists():\n",
    "                logger.error(f\"Image not found: {image_path}\")\n",
    "                return None\n",
    "                \n",
    "            # 读取并处理图片\n",
    "            image = PILImage.open(image_path)\n",
    "            \n",
    "            # 转换为 RGB 模式\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "                \n",
    "            # 计算等比例缩放的大小\n",
    "            width, height = image.size\n",
    "            ratio = min(target_size[0]/width, target_size[1]/height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            \n",
    "            # 缩放图片\n",
    "            image = image.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            \n",
    "            # 创建新的白色背景图片\n",
    "            new_image = PILImage.new('RGB', target_size, (255, 255, 255))\n",
    "            \n",
    "            # 计算居中位置\n",
    "            x = (target_size[0] - new_size[0]) // 2\n",
    "            y = (target_size[1] - new_size[1]) // 2\n",
    "            \n",
    "            # 贴上缩放后的图片\n",
    "            new_image.paste(image, (x, y))\n",
    "            \n",
    "            # 生成输出路径\n",
    "            output_path = self.image_dir / f\"{image_path.name}\" #output_path = self.image_dir / f\"{prefix}{image_path.name}\"\n",
    "            # 保存处理后的图片\n",
    "            new_image.save(output_path, quality=quality)\n",
    "            logger.info(f\"Saved processed image to: {output_path}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    def load_for_display(self, \n",
    "                        image_path: Union[str, Path],  # 使用 Union 替代 |\n",
    "                        display_size: Tuple[int, int]) -> Optional[PILImage.Image]:\n",
    "        \"\"\"载入图片用于显示\"\"\"\n",
    "        try:\n",
    "            processed_path = self.process_and_save(image_path, display_size, prefix=\"display_\")\n",
    "            if processed_path:\n",
    "                return PILImage.open(processed_path)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image for display {image_path}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 處理模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from IPython.display import display, Image\n",
    "class HybridEmbeddingProcessor:\n",
    "    # 初始化 embedding processor\n",
    "    def __init__(self, \n",
    "                persist_directory: str = \"chroma_db\",\n",
    "                image_dir: str = \"image\",\n",
    "                image_size: tuple = (224, 224)):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_size = image_size\n",
    "        self.image_processor = ImageProcessor(image_dir)\n",
    "        \n",
    "        # 初始化模型\n",
    "        self.text_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # 設置 Chroma，使用已下載的 SentenceTransformer\n",
    "        logger.info(f\"Initializing Chroma with directory: {persist_directory}\")\n",
    "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # 創建或獲取集合，使用 SentenceTransformer embedding function\n",
    "        sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name='all-mpnet-base-v2',\n",
    "            device=\"cpu\"  # 明確指定設備\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # 先嘗試刪除現有的集合（如果存在）\n",
    "            try:\n",
    "                self.chroma_client.delete_collection(\"text_documents\")\n",
    "                self.chroma_client.delete_collection(\"image_documents\")\n",
    "                logger.info(\"Deleted existing collections\")\n",
    "            except:\n",
    "                logger.info(\"No existing collections to delete\")\n",
    "            \n",
    "            # 創建新的集合\n",
    "            self.text_collection = self.chroma_client.create_collection(\n",
    "                name=\"text_documents\",\n",
    "                embedding_function=sentence_transformer_ef\n",
    "            )\n",
    "            \n",
    "            self.image_collection = self.chroma_client.create_collection(\n",
    "                name=\"image_documents\"\n",
    "            )\n",
    "            logger.info(\"Created new collections\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing collections: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        logger.info(\"Initialized hybrid embedding system\")\n",
    "\n",
    "    def process_image(self, image_path: str) -> Optional[np.ndarray]:\n",
    "            \"\"\"處理圖片並生成 embedding\"\"\"\n",
    "            try:\n",
    "                logger.info(f\"Processing image: {image_path}\")\n",
    "                processed_path = self.image_processor.process_and_save(\n",
    "                    image_path=image_path,\n",
    "                    target_size=self.image_size\n",
    "                )\n",
    "                \n",
    "                if processed_path is None:\n",
    "                    logger.error(f\"Failed to process image: {image_path}\")\n",
    "                    return None\n",
    "                    \n",
    "                image = PILImage.open(processed_path)\n",
    "                inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "                image_features = self.clip_model.get_image_features(**inputs)\n",
    "                \n",
    "                logger.info(f\"Successfully generated embedding for image: {image_path}\")\n",
    "                return image_features.detach().numpy()[0]\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "    def add_documents(self, \n",
    "                     texts: List[str], \n",
    "                     metadatas: List[Dict],\n",
    "                     images: Optional[List[str]] = None):\n",
    "        \"\"\"添加文件到不同的集合\"\"\"\n",
    "        try:\n",
    "            # 添加文本文件\n",
    "            if texts and metadatas:\n",
    "                logger.info(f\"Adding {len(texts)} text documents\")\n",
    "                self.text_collection.add(\n",
    "                    documents=texts,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=[f\"text_{i}\" for i in range(len(texts))]\n",
    "                )\n",
    "                logger.info(\"Successfully added text documents\")\n",
    "            \n",
    "            # 處理並添加圖片\n",
    "            if images:\n",
    "                logger.info(f\"Processing {len(images)} images\")\n",
    "                image_embeddings = []\n",
    "                valid_images = []\n",
    "                valid_metadatas = []\n",
    "                \n",
    "                for i, img_path in enumerate(images):\n",
    "                    logger.info(f\"Processing image {i+1}/{len(images)}: {img_path}\")\n",
    "                    embedding = self.process_image(str(self.image_dir / img_path))\n",
    "                    if embedding is not None:\n",
    "                        image_embeddings.append(embedding.tolist())\n",
    "                        valid_images.append(img_path)\n",
    "                        valid_metadatas.append({\n",
    "                            \"type\": \"image\", \n",
    "                            \"path\": img_path,\n",
    "                            \"associated_text\": texts[i] if i < len(texts) else \"\"\n",
    "                        })\n",
    "                \n",
    "                if valid_images:\n",
    "                    logger.info(f\"Adding {len(valid_images)} valid images to collection\")\n",
    "                    self.image_collection.add(\n",
    "                        embeddings=image_embeddings,\n",
    "                        metadatas=valid_metadatas,\n",
    "                        ids=[f\"img_{i}\" for i in range(len(valid_images))]\n",
    "                    )\n",
    "                    logger.info(\"Successfully added images to collection\")\n",
    "                else:\n",
    "                    logger.warning(\"No valid images to add to collection\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def process_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"使用 SentenceTransformer 處理文本\"\"\"\n",
    "        try:\n",
    "            embedding = self.text_model.encode(text)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def search(self, query: str, k: int = 3) -> Dict:\n",
    "        \"\"\"改進的搜索方法\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting search operation...\")\n",
    "            \n",
    "            # 檢查集合是否存在\n",
    "            if not self.text_collection or not self.image_collection:\n",
    "                logger.error(\"Collections not initialized properly\")\n",
    "                return {\n",
    "                    \"texts\": {\"documents\": [[]], \"metadatas\": [[]], \"distances\": []},\n",
    "                    \"images\": {\"metadatas\": [[]], \"documents\": [], \"distances\": []}\n",
    "                }\n",
    "\n",
    "            # 文本搜索\n",
    "            text_results = self.text_collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=k\n",
    "            )\n",
    "            \n",
    "            # 圖片搜索\n",
    "            try:\n",
    "                inputs = self.clip_processor(\n",
    "                    text=[query], \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=77\n",
    "                )\n",
    "                text_features = self.clip_model.get_text_features(**inputs)\n",
    "                query_embedding = text_features.detach().numpy()[0]\n",
    "                query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "                \n",
    "                image_results = self.image_collection.query(\n",
    "                    query_embeddings=[query_embedding.tolist()],\n",
    "                    n_results=k\n",
    "                )\n",
    "                logger.info(f\"Found {len(image_results['metadatas'][0]) if image_results['metadatas'] else 0} images\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in image search: {str(e)}\")\n",
    "                image_results = {\n",
    "                    \"ids\": [], \n",
    "                    \"documents\": [], \n",
    "                    \"metadatas\": [[]], \n",
    "                    \"distances\": []\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"texts\": text_results,\n",
    "                \"images\": image_results\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search error: {str(e)}\")\n",
    "            return {\n",
    "                \"texts\": {\"documents\": [[]], \"metadatas\": [[]], \"distances\": []},\n",
    "                \"images\": {\"metadatas\": [[]], \"documents\": [], \"distances\": []}\n",
    "            }\n",
    "\n",
    "    def display_search_results(self, query: str, k: int = 3, display_size: tuple = (400, 400)):\n",
    "        \"\"\"顯示搜索結果\"\"\"\n",
    "        results = self.search(query, k)\n",
    "        \n",
    "        print(f\"Query: {query}\\n\")\n",
    "        \n",
    "        # 顯示圖片結果\n",
    "        if results[\"image_relevance\"] >= 0.3 and results[\"images\"][\"metadatas\"]:\n",
    "            print(\"Related Images:\")\n",
    "            try:\n",
    "                for i, meta in enumerate(results[\"images\"][\"metadatas\"][0], 1):\n",
    "                    print(f\"\\nImage {i}:\")\n",
    "                    \n",
    "                    # 使用統一的圖片處理方法載入圖片\n",
    "                    img = self.image_processor.load_for_display(\n",
    "                        image_path=meta['path'],\n",
    "                        display_size=display_size\n",
    "                    )\n",
    "                    \n",
    "                    if img is not None:\n",
    "                        # 顯示圖片\n",
    "                        display.display(display.Image(data=img.tobytes()))\n",
    "                        \n",
    "                        if 'associated_text' in meta:\n",
    "                            print(f\"Context: {meta['associated_text'][:200]}...\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error displaying images: {str(e)}\")\n",
    "                print(\"Error displaying images\")\n",
    "        else:\n",
    "            if results[\"image_relevance\"] < 0.3:\n",
    "                print(\"No images needed for this query\")\n",
    "            else:\n",
    "                print(\"No relevant images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料處理模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, embedding_processor: HybridEmbeddingProcessor):\n",
    "        self.embedding_processor = embedding_processor\n",
    "        \n",
    "    def process_csv_with_images(self, csv_path: str) -> Tuple[List[str], List[Dict], List[str]]:\n",
    "        \"\"\"處理 CSV 並提取圖片，確保 metadata 值都是基本類型\"\"\"\n",
    "        logger.info(f\"Processing CSV: {csv_path}\")\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        images = []\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        current_post = None\n",
    "        current_responses = []\n",
    "        current_images = []\n",
    "        current_metadata = None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # 開始新的貼文\n",
    "            if pd.notna(row['post']):\n",
    "                if current_post is not None:\n",
    "                    # 保存前一個貼文\n",
    "                    text = f\"{current_post} {' '.join(current_responses)}\"\n",
    "                    texts.append(text)\n",
    "                    if current_images:\n",
    "                        current_metadata[\"images\"] = \",\".join(current_images)\n",
    "                        images.extend(current_images)\n",
    "                    # 將回應列表轉換為字符串，使用 | 作為分隔符\n",
    "                    current_metadata[\"responses\"] = \"|\".join(current_responses)\n",
    "                    metadatas.append(current_metadata)\n",
    "                \n",
    "                # 初始化新貼文\n",
    "                current_post = row['post']\n",
    "                current_responses = []\n",
    "                current_images = []\n",
    "                current_metadata = {\n",
    "                    \"type\": \"social_post\",\n",
    "                    \"source\": \"facebook\",\n",
    "                    \"is_post\": True,\n",
    "                    \"original_post\": current_post\n",
    "                }\n",
    "                \n",
    "                # 添加連結（如果有）\n",
    "                if pd.notna(row['link']):\n",
    "                    current_metadata[\"link\"] = row['link']\n",
    "            \n",
    "            # 添加回覆\n",
    "            if pd.notna(row['responses']):\n",
    "                current_responses.append(row['responses'])\n",
    "                \n",
    "            # 處理圖片\n",
    "            if pd.notna(row.get('images')):\n",
    "                img_path = row['images']\n",
    "                current_images.append(img_path)\n",
    "                logger.info(f\"Found image: {img_path} for current post\")\n",
    "        \n",
    "        # 保存最後一個貼文\n",
    "        if current_post is not None:\n",
    "            text = f\"{current_post} {' '.join(current_responses)}\"\n",
    "            texts.append(text)\n",
    "            if current_images:\n",
    "                current_metadata[\"images\"] = \",\".join(current_images)\n",
    "                images.extend(current_images)\n",
    "            # 將回應列表轉換為字符串\n",
    "            current_metadata[\"responses\"] = \"|\".join(current_responses)\n",
    "            metadatas.append(current_metadata)\n",
    "        \n",
    "        # 顯示處理結果的詳細資訊\n",
    "        for i, (text, meta) in enumerate(zip(texts, metadatas)):\n",
    "            logger.info(f\"\\nPost {i+1}:\")\n",
    "            logger.info(f\"Content: {text[:100]}...\")\n",
    "            logger.info(f\"Images: {meta.get('images', 'No images')}\")\n",
    "            # 使用 split('|') 代替 split('\\n')\n",
    "            logger.info(f\"Responses: {len(meta.get('responses', '').split('|'))} replies\")\n",
    "            logger.info(f\"Link: {meta.get('link', 'No link')}\")\n",
    "        \n",
    "        return texts, metadatas, images\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"處理 PDF 文件，確保 metadata 值都是基本類型\"\"\"\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    text = page.extract_text()\n",
    "                    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "                    \n",
    "                    for para in paragraphs:\n",
    "                        documents.append({\n",
    "                            'content': para,\n",
    "                            'metadata': {\n",
    "                                'source': pdf_path,\n",
    "                                'page': str(page_num + 1),  # 轉換為字符串\n",
    "                                'type': 'pdf',\n",
    "                                'content_length': str(len(para))  # 添加額外信息，也確保是字符串\n",
    "                            }\n",
    "                        })\n",
    "            \n",
    "            logger.info(f\"Extracted {len(documents)} paragraphs from PDF\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_all(self, csv_path: str, pdf_path: str):\n",
    "        \"\"\"處理所有資料，確保所有 metadata 正確\"\"\"\n",
    "        logger.info(\"Starting to process all documents...\")\n",
    "        \n",
    "        try:\n",
    "            # 處理 CSV 和圖片\n",
    "            texts, metadatas, images = self.process_csv_with_images(csv_path)\n",
    "            logger.info(f\"\\nFound {len(images)} images:\")\n",
    "            for img in images:\n",
    "                logger.info(f\"- {img}\")\n",
    "            \n",
    "            # 檢查圖片文件是否存在\n",
    "            image_dir = Path(\"image\")\n",
    "            valid_images = []\n",
    "            for img in images:\n",
    "                img_path = image_dir / img\n",
    "                if img_path.exists():\n",
    "                    logger.info(f\"Image file exists: {img_path}\")\n",
    "                    valid_images.append(img)\n",
    "                else:\n",
    "                    logger.error(f\"Image file not found: {img_path}\")\n",
    "            \n",
    "            # 處理 PDF\n",
    "            pdf_docs = self.process_pdf(pdf_path)\n",
    "            texts.extend([doc['content'] for doc in pdf_docs])\n",
    "            metadatas.extend([doc['metadata'] for doc in pdf_docs])\n",
    "            \n",
    "            logger.info(f\"\\nProcessing summary:\")\n",
    "            logger.info(f\"- Total texts: {len(texts)}\")\n",
    "            logger.info(f\"- Total valid images: {len(valid_images)}\")\n",
    "            \n",
    "            # 最終檢查確保所有 metadata 值都是基本類型\n",
    "            for meta in metadatas:\n",
    "                for key, value in list(meta.items()):\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        meta[key] = str(value)\n",
    "            \n",
    "            # 添加到 Chroma\n",
    "            self.embedding_processor.add_documents(\n",
    "                texts=texts,\n",
    "                metadatas=metadatas,\n",
    "                images=valid_images\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully processed all documents\")\n",
    "            return len(texts), len(valid_images)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing documents: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA系統模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "You are a professional veterinarian specializing in:\n",
    "\n",
    "Diagnosis and care of Canine Cognitive Dysfunction Syndrome (CCD)\n",
    "Pet behavior problem diagnosis and improvement suggestions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Please handle user questions according to the following steps:\n",
    "Please refer to the references provided below to answer\n",
    "Step 1: Determine if images are needed in the system response\n",
    "Based on the question content:\n",
    "\n",
    "Environment-related: Space planning, facility placement, equipment usage, etc.\n",
    "Behavior-related: Posture, movement, physical appearance, etc.\n",
    "Care-related: Wheelchairs, activity areas, equipment usage, etc.\n",
    "Product recommendations: Medications, assistive devices\n",
    "\n",
    "Step 2: Provide recommendations\n",
    "Text response:\n",
    "\n",
    "Provide professional advice based on textual information\n",
    "Explain relevant symptoms and precautions\n",
    "If there are relevant images, naturally incorporate image descriptions into the content\n",
    "For example:\n",
    "\"Consider using MCT oil (as shown in Figure 1) for nutritional supplementation\"\n",
    "\"Veterinarians may prescribe cognitive support medications (such as CogniCaps shown in Figure 2)\"\n",
    "\n",
    "If images are needed:\n",
    "\n",
    "Find appropriate photos from reference materials\n",
    "Naturally embed image descriptions within relevant recommendations rather than placing them all at the end\n",
    "Explain how the images relate to the recommendations (e.g., this is the recommended nutritional supplement, this is a commonly prescribed medication, etc.)\n",
    "If images are needed but not available in reference materials, proceed without images\n",
    "\n",
    "Step 3: Response format\n",
    "\n",
    "Symptom/problem analysis\n",
    "Professional advice explanation\n",
    "Recommendations based on community-collected data\n",
    "Necessary medical reminders\n",
    "If reference materials are used, mark the source at the end of the response, e.g., [xxx.pdf] or [Link] for community post references\n",
    "\n",
    "Please respond with empathy and a professional tone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, embedding_processor: 'HybridEmbeddingProcessor',\n",
    "                 model_name: str = 'llama3.2-vision'):\n",
    "        self.embedding_processor = embedding_processor\n",
    "        self.model_name = model_name\n",
    "        logger.info(f\"Initialized QA System with Ollama model: {model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Generate response using Ollama\"\"\"\n",
    "        try:\n",
    "            # Search for relevant content\n",
    "            search_results = self.embedding_processor.search(query)\n",
    "            \n",
    "            # Get context and format prompt\n",
    "            context = self.format_context(search_results)\n",
    "            \n",
    "            # Prepare the message with text and images\n",
    "            message = {\n",
    "                'role': 'user',\n",
    "                'content': f\"\"\"{role}\n",
    "\n",
    "{task}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Reference Materials:\n",
    "{context}\n",
    "\"\"\"\n",
    "            }\n",
    "\n",
    "            # Add images if available\n",
    "            image_paths = []\n",
    "            if (search_results[\"images\"][\"metadatas\"] and \n",
    "                search_results[\"images\"][\"metadatas\"][0]):\n",
    "                images = []\n",
    "                for metadata in search_results[\"images\"][\"metadatas\"][0]:\n",
    "                    img_path = self.embedding_processor.image_dir / metadata[\"path\"]\n",
    "                    if img_path.exists():\n",
    "                        image_paths.append(str(img_path))\n",
    "                        images.append(str(img_path))\n",
    "                if images:\n",
    "                    message['images'] = images\n",
    "\n",
    "            # Generate response using Ollama\n",
    "            response = ollama.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[message]\n",
    "            )\n",
    "\n",
    "            return response['message']['content'], image_paths\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\")\n",
    "            error_msg = \"Sorry, an error occurred while processing your question. \"\n",
    "            if \"vision model only supports a single image\" in str(e):\n",
    "                error_msg += \"There was an issue processing the images.\"\n",
    "            return error_msg, []\n",
    "\n",
    "    def format_context(self, search_results: Dict) -> str:\n",
    "        \"\"\"Format context from search results\"\"\"\n",
    "        try:\n",
    "            context = \"\"\n",
    "            \n",
    "            # Add text results\n",
    "            if (search_results[\"texts\"][\"documents\"] and \n",
    "                search_results[\"texts\"][\"documents\"][0] and \n",
    "                search_results[\"texts\"][\"metadatas\"] and \n",
    "                search_results[\"texts\"][\"metadatas\"][0]):\n",
    "                \n",
    "                for doc, meta in zip(\n",
    "                    search_results[\"texts\"][\"documents\"][0],\n",
    "                    search_results[\"texts\"][\"metadatas\"][0]\n",
    "                ):\n",
    "                    if meta[\"type\"] == \"pdf\":\n",
    "                        context += f\"[Medical Literature]\\n{doc}\\n\\n\"\n",
    "                    else:\n",
    "                        context += f\"[Community Discussion]\\n{doc}\\n\"\n",
    "                        if meta.get(\"link\"):\n",
    "                            context += f\"Source: {meta['link']}\\n\\n\"\n",
    "            \n",
    "            # Add image information if relevant\n",
    "            if (search_results.get(\"image_relevance\", 0) >= 0.3 and \n",
    "                search_results[\"images\"].get(\"metadatas\") and \n",
    "                search_results[\"images\"][\"metadatas\"][0]):\n",
    "                \n",
    "                context += \"\\n[Most Relevant Image]\\n\"\n",
    "                for meta in search_results[\"images\"][\"metadatas\"][0]:\n",
    "                    context += f\"- {meta['path']}\\n\"\n",
    "            \n",
    "            return context\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting context: {str(e)}\")\n",
    "            return \"Unable to retrieve reference materials\"\n",
    "\n",
    "    def display_response(self, query: str):\n",
    "        \"\"\"Display response with text and images\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting to generate response...\")\n",
    "            response_text, image_paths = self.generate_response(query)\n",
    "            \n",
    "            print(\"Question:\", query)\n",
    "            print(\"\\nSystem Response:\")\n",
    "            print(response_text)\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "            if image_paths:\n",
    "                print(\"\\nRelated Image:\")\n",
    "                img_path = image_paths[0]  # We now only have one image\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    display(IPyImage(filename=img_path))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error displaying image {img_path}: {str(e)}\")\n",
    "            else:\n",
    "                logger.info(\"No images to display\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in display_response: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系統初始化和資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:__main__:Initializing Chroma with directory: chroma_db\n",
      "INFO:__main__:Deleted existing collections\n",
      "INFO:__main__:Created new collections\n",
      "INFO:__main__:Initialized hybrid embedding system\n",
      "INFO:__main__:Starting to process all documents...\n",
      "INFO:__main__:Processing CSV: post_response.csv\n",
      "INFO:__main__:Found image: image01.jpg for current post\n",
      "INFO:__main__:Found image: image02.jpg for current post\n",
      "INFO:__main__:Found image: image03.jpg for current post\n",
      "INFO:__main__:Found image: image04.jpg for current post\n",
      "INFO:__main__:Found image: image05.jpg for current post\n",
      "INFO:__main__:Found image: image06.jpg for current post\n",
      "INFO:__main__:Found image: image07.jpg for current post\n",
      "INFO:__main__:Found image: image08.jpg for current post\n",
      "INFO:__main__:\n",
      "Post 1:\n",
      "INFO:__main__:Content: 失智的狗狗，如果晚上四處走，不懂自己睡，是否買輪椅給牠晚上用，讓牠累就直接睡比較好？但我看到長期用手腳的位置會紅呢，應該怎樣比較好？（現在都是抱牠入睡，但凌晨牠自己起身我也不知道呢）請指教。 輪椅不能...\n",
      "INFO:__main__:Images: image01.jpg,image02.jpg,image03.jpg\n",
      "INFO:__main__:Responses: 7 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/share/p/qFf55T4BgHJs6TzY/\n",
      "INFO:__main__:\n",
      "Post 2:\n",
      "INFO:__main__:Content: 吃老狗失智夜晚狂叫，有何妙方？ 請醫生開立輕微鎮靜的藥，不要覺得捨不得，狗狗一直叫沒有休息也是不行的，主人更應該得到好的睡眠才能照顧好寶貝 白天盡量多散步、或是在家裡陪他活動，請醫生開保健品、嚴重的時...\n",
      "INFO:__main__:Images: No images\n",
      "INFO:__main__:Responses: 9 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/groups/403191506497801/permalink/2631558230327773/?rdid=HqDuRASCUk53jABS&share_url=https%3A%2F%2Fwww.facebook.com%2Fshare%2Fp%2FidyouqqRo1EWrH3H%2F\n",
      "INFO:__main__:\n",
      "Post 3:\n",
      "INFO:__main__:Content: 關於犬失智有問題想請益\n",
      "上禮拜開始發現家中小狗（11歲）開始對於指令失去反應，聯想到前陣子他開始偶爾會躲在角落，有時候好像也會不認得我姊姊，懷疑他開始有失智的情形。\n",
      "1.已有預約台大神經科醫師診察，但...\n",
      "INFO:__main__:Images: No images\n",
      "INFO:__main__:Responses: 5 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/share/p/6DkLRFqSQQBN7G3M/\n",
      "INFO:__main__:\n",
      "Post 4:\n",
      "INFO:__main__:Content: 家中狗狗16歲了，\n",
      "去年底開始出現失智的症狀後來也被診斷，\n",
      "最近這2個月和家人都沒有睡好覺，\n",
      "狗狗半夜都會起來走路、繞圈圈、跌倒站不起或是卡住出不來就會尖叫，\n",
      "原本因為擔心狗狗年紀大了，\n",
      "身體無法負荷...\n",
      "INFO:__main__:Images: image04.jpg\n",
      "INFO:__main__:Responses: 6 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/share/p/vAAY4JiM3AfdQ496/\n",
      "INFO:__main__:\n",
      "Post 5:\n",
      "INFO:__main__:Content: 今年3月寶貝由台大醫院神經科吳芝菁醫師正式診斷為失智，當時吳醫師提醒我、市面上的保養品，對每隻狗狗狀態不同，也不見得每個時期都會一樣，所以她開了清單給我(清單是她找的到研究數據的商品)\n",
      "1.蘇活沙美─...\n",
      "INFO:__main__:Images: image05.jpg,image06.jpg,image07.jpg\n",
      "INFO:__main__:Responses: 6 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/share/p/8wwZPppwzKkueWwv/\n",
      "INFO:__main__:\n",
      "Post 6:\n",
      "INFO:__main__:Content: 大家好唷，想請益照顧癱瘓+失智狗狗的經驗談！\n",
      "胖妞今年17歲，很有個性的米克斯女孩，今年開始發現會卡在角落無法出來，漸漸開始繞圈圈、變瘦、後腳沒力、常常跌坐，到現在已經沒辦法靠自己起身了...\n",
      "我想請...\n",
      "INFO:__main__:Images: image08.jpg\n",
      "INFO:__main__:Responses: 12 replies\n",
      "INFO:__main__:Link: https://www.facebook.com/share/p/SBaxQXDk1ps5hktC/\n",
      "INFO:__main__:\n",
      "Found 8 images:\n",
      "INFO:__main__:- image01.jpg\n",
      "INFO:__main__:- image02.jpg\n",
      "INFO:__main__:- image03.jpg\n",
      "INFO:__main__:- image04.jpg\n",
      "INFO:__main__:- image05.jpg\n",
      "INFO:__main__:- image06.jpg\n",
      "INFO:__main__:- image07.jpg\n",
      "INFO:__main__:- image08.jpg\n",
      "INFO:__main__:Image file exists: image/image01.jpg\n",
      "INFO:__main__:Image file exists: image/image02.jpg\n",
      "INFO:__main__:Image file exists: image/image03.jpg\n",
      "INFO:__main__:Image file exists: image/image04.jpg\n",
      "INFO:__main__:Image file exists: image/image05.jpg\n",
      "INFO:__main__:Image file exists: image/image06.jpg\n",
      "INFO:__main__:Image file exists: image/image07.jpg\n",
      "INFO:__main__:Image file exists: image/image08.jpg\n",
      "INFO:__main__:Processing PDF: salvin2010.pdf\n",
      "INFO:__main__:Extracted 5 paragraphs from PDF\n",
      "INFO:__main__:\n",
      "Processing summary:\n",
      "INFO:__main__:- Total texts: 11\n",
      "INFO:__main__:- Total valid images: 8\n",
      "INFO:__main__:Adding 11 text documents\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "INFO:__main__:Successfully added text documents\n",
      "INFO:__main__:Processing 8 images\n",
      "INFO:__main__:Processing image 1/8: image01.jpg\n",
      "INFO:__main__:Processing image: image/image01.jpg\n",
      "INFO:__main__:Saved processed image to: image/image01.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image01.jpg\n",
      "INFO:__main__:Processing image 2/8: image02.jpg\n",
      "INFO:__main__:Processing image: image/image02.jpg\n",
      "INFO:__main__:Saved processed image to: image/image02.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image02.jpg\n",
      "INFO:__main__:Processing image 3/8: image03.jpg\n",
      "INFO:__main__:Processing image: image/image03.jpg\n",
      "INFO:__main__:Saved processed image to: image/image03.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image03.jpg\n",
      "INFO:__main__:Processing image 4/8: image04.jpg\n",
      "INFO:__main__:Processing image: image/image04.jpg\n",
      "INFO:__main__:Saved processed image to: image/image04.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image04.jpg\n",
      "INFO:__main__:Processing image 5/8: image05.jpg\n",
      "INFO:__main__:Processing image: image/image05.jpg\n",
      "INFO:__main__:Saved processed image to: image/image05.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image05.jpg\n",
      "INFO:__main__:Processing image 6/8: image06.jpg\n",
      "INFO:__main__:Processing image: image/image06.jpg\n",
      "INFO:__main__:Saved processed image to: image/image06.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image06.jpg\n",
      "INFO:__main__:Processing image 7/8: image07.jpg\n",
      "INFO:__main__:Processing image: image/image07.jpg\n",
      "INFO:__main__:Saved processed image to: image/image07.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image07.jpg\n",
      "INFO:__main__:Processing image 8/8: image08.jpg\n",
      "INFO:__main__:Processing image: image/image08.jpg\n",
      "INFO:__main__:Saved processed image to: image/image08.jpg\n",
      "INFO:__main__:Successfully generated embedding for image: image/image08.jpg\n",
      "INFO:__main__:Adding 8 valid images to collection\n",
      "INFO:__main__:Successfully added images to collection\n",
      "INFO:__main__:Successfully processed all documents\n"
     ]
    }
   ],
   "source": [
    "# 初始化 embedding processor\n",
    "embedding_processor = HybridEmbeddingProcessor(\n",
    "    image_size=(224, 224)  # 設定圖片處理的目標尺寸\n",
    ")\n",
    "\n",
    "# 初始化數據處理器\n",
    "data_processor = DataProcessor(embedding_processor)\n",
    "\n",
    "# 處理資料\n",
    "num_texts, num_images = data_processor.process_all(\n",
    "    csv_path=\"post_response.csv\",\n",
    "    pdf_path=\"salvin2010.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系統測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized QA System with Ollama model: llama3.2-vision\n"
     ]
    }
   ],
   "source": [
    "qa_system = QASystem(\n",
    "    embedding_processor=embedding_processor,\n",
    "    model_name='llama3.2-vision'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用語音轉文字做輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zirong/miniforge3/envs/llm/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/zirong/miniforge3/envs/llm/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:__main__:Starting to generate response...\n",
      "INFO:__main__:Starting search operation...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "INFO:__main__:Found 3 images\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 500 Internal Server Error\"\n",
      "ERROR:__main__:Error generating response: vision model only supports a single image per message\n",
      "INFO:__main__:No images to display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 我家的狗最近早上總是10歲晚上總是一直起來賺全權這樣是實質症狀嗎?\n",
      "\n",
      "System Response:\n",
      "Sorry, an error occurred while processing your question. There was an issue processing the images.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"no_upload/test_mp3/01.mp3\"  # 修改為你的音檔路徑\n",
    "transcription = transcribe_file(audio_file)\n",
    "qa_system.display_response(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一般輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting to generate response...\n",
      "INFO:__main__:Starting search operation...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.94it/s]\n",
      "INFO:__main__:Found 3 images\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 500 Internal Server Error\"\n",
      "ERROR:__main__:Error generating response: vision model only supports a single image per message\n",
      "INFO:__main__:No images to display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 我家狗晚上會繞圈圈，不知道是不是算失智症狀？我想提前準備好，有適合她活動的佈置嗎？\n",
      "\n",
      "System Response:\n",
      "Sorry, an error occurred while processing your question. There was an issue processing the images.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 測試查詢\n",
    "test_queries = [\n",
    "    \"我家狗晚上會繞圈圈，不知道是不是算失智症狀？我想提前準備好，有適合她活動的佈置嗎？\"\n",
    "    # \"給我關於活動佈置可以參考的圖\"\n",
    "    # 'My dog ​​walks in circles at night. Is this a symptom of dementia? I want to prepare in advance. Is there a suitable layout for her event?'\n",
    "    # \"老狗失智症有什麼症狀？\",\n",
    "    # \"晚上狗狗一直叫該怎麼辦？\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    qa_system.display_response(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
